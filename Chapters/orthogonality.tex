\section{Lecture 35: November 28, 2022}

    \subsection{Inner Product Spaces}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Inner Products and Inner Product Spaces}{innerprod}

            An \(\mathbb{F}\)-valued inner product on a vector space \(V\) is a function 
            \begin{equation*}
                \langle\cdot,\cdot\rangle:V\times V\to \mathbb{F}
            \end{equation*}
            such that
            \begin{enumerate}
                \item \(\forall\vec{v}\in V,\iprod{\vec{v}}{\vec{v}}\geq 0\).
                \item \(\vec{v}=\vec{0}_V\iff\iprod{\vec{v}}{\vec{v}}=0\).
                \item \(\forall\vec{u},\vec{v}\in V,\iprod{\vec{u}}{\vec{v}}=\bar{\iprod{\vec{v}}{\vec{u}}}\).
                \item \(\forall\vec{u},\vec{v},\vec{w}\in V,\iprod{\vec{u}+\vec{v}}{\vec{w}}=\iprod{\vec{u}}{\vec{w}}+\iprod{\vec{v}}{\vec{w}}\).
                \item \(\forall c\in\mathbb{F},\forall\vec{u},\vec{v}\in V,\iprod{c\vec{u}}{\vec{v}}=c\iprod{\vec{u}}{\vec{v}}\).
            \end{enumerate}
            \vphantom
            \\
            \\
            The pair \((V,\langle\cdot,\cdot\rangle)\) is called an inner product space.
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following inner product spaces.
        \begin{enumerate}
            \item The pair \((\mathbb{R}^n, \iprod{\vec{u}}{\vec{v}}=\vec{u}\cdot\vec{v}=u_1v_1+\cdots+u_nv_n)\) is a real inner product space.
            \item The pair \((\mathbb{C}^n, \iprod{\vec{u}}{\vec{v}}=\vec{u}\cdot\vec{v}=u_1\bar{v_1}+\cdots+u_n\bar{v_n})\) is a complex inner product space.
            \item The pair 
                \begin{equation*}
                    \left(V,\iprod{\vec{f}}{\vec{g}}=\int_0^1 f(x)g(x)\dd x\right)
                \end{equation*}
                is an real inner product space, for 
                \begin{equation*}
                    V=\left\{f:[0,1]\to\mathbb{R}:\forall c, \lim_{x\to c}=f(c)\right\}.
                \end{equation*}
            \item The pair
            \begin{equation*}
                \left(\mathcal{P}_n,\iprod{\vec{p}}{\vec{q}}=\int_{-1}^1 p(x)\bar{q(x)}\dd x\right)
            \end{equation*}
            is a real inner product space.
        \end{enumerate}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorems and definitions.
        \begin{theorem}{\Stop\,\,Properties of Inner Products}{innerprodprops}

            Let \(V\) be an inner product space. Suppose \(\vec{u},\vec{v},\vec{w}\in V\) and \(c\in\mathbb{F}\). Then,
            \begin{enumerate}
                \item \(F:V\to \mathbb{F},\vec{v}\mapsto\iprod{\vec{v}}{\vec{u}}\) is a linear transformation.
                \begin{proof}
                    Consider \(\vec{v}_1,\vec{v}_2\in V\). Then,
                    \begin{align*}
                        F(\vec{v}_1+\vec{v}_2)&=\iprod{\vec{v}_1+\vec{v}_2}{\vec{u}} \\
                        &=\iprod{\vec{v}_1}{u}+\iprod{\vec{v}_2}{\vec{u}} \\
                        &=F(\vec{v}_1)+F(\vec{v}_2).
                    \end{align*}
                    For \(c\in\mathbb{F}\), we have
                    \begin{align*}
                        F(c\vec{v}_1)&=\iprod{c\vec{v}_1}{\vec{u}} \\
                        &=c\iprod{\vec{v}_1}{\vec{u}},
                    \end{align*}
                    as desired.
                \end{proof}
                \item \(\iprod{\vec{0}_V}{\vec{v}}=\vec{0}_V=\iprod{\vec{v}}{\vec{0}_V}\).
                \begin{proof}
                    The first equality is derived by the first part since, for all linear transformations \(L\), \(\vec{0}_V\in\ker(L)\). The second equality is derived from conjugate symmetry.
                \end{proof}
                \item \(\iprod{\vec{u}}{\vec{v}+\vec{w}}=\iprod{\vec{u}}{\vec{v}}+\iprod{\vec{u}}{\vec{w}}\).
                \begin{proof}
                    Consider \(\iprod{\vec{u}}{\vec{v}+\vec{w}}=\bar{\iprod{\vec{v}+\vec{w}}{\vec{u}}}=\bar{\iprod{\vec{v}}{\vec{u}}}+\bar{\iprod{\vec{w}}{\vec{u}}}=\iprod{\vec{u}}{\vec{v}}+\iprod{\vec{u}}{\vec{w}}\).
                \end{proof}
                \item \(\iprod{\vec{u}}{c\vec{v}}=\bar{c}\iprod{\vec{u}}{\vec{v}}\).
                \begin{proof}
                    Consider \(\iprod{\vec{u}}{c\vec{v}}=\bar{\iprod{c\vec{v}}{\vec{u}}}=\bar{c}\bar{\iprod{\vec{v}}{\vec{w}}}=\bar{c}\iprod{\vec{u}}{\vec{v}}\).
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \begin{definition}{\Stop\,\,Norms}{norms}

            The norm associated to the inner product \(\langle\cdot,\cdot\rangle\) is
            \begin{equation*}
                ||\cdot||:V\to[0,\infty),\vec{v}\mapsto\sqrt{\iprod{\vec{v}}{\vec{v}}}.
            \end{equation*}
            
        \end{definition}
        \pagebreak
        \begin{theorem}{\Stop\,\,Properties of Norms}{propnorm}

            Suppose \(V\) is an inner product space and \(\vec{v}\in V\). Then,
            \begin{enumerate}
                \item \(||\vec{v}||=0\iff\vec{v}=\vec{0}_V\).
                \begin{proof}
                    If \(\vec{v}=\vec{0}_V\), \(\iprod{\vec{v}}{\vec{v}}=\sqrt{\iprod{\vec{v}}{\vec{v}}}=||\vec{v}||=0\). If \(||\vec{v}||=0\), \(\sqrt{\iprod{\vec{v}}{\vec{v}}}=\iprod{\vec{v}}{\vec{v}}=0\), meaning that \(\vec{v}=\vec{0}_V\).
                \end{proof}
                \item \(\forall c\in\mathbb{F},||c\vec{v}||=|c|||\vec{v}||\).
                \begin{proof}
                    Consider
                    \begin{align*}
                        ||c\vec{v}||&=\sqrt{\iprod{c\vec{v}}{c\vec{v}}} \\
                        &=\sqrt{c\bar{c}\iprod{\vec{v}}{\vec{v}}} \\
                        &=\sqrt{|c|^2\iprod{\vec{v}}{\vec{v}}} \\
                        &=|c|\sqrt{\iprod{\vec{v}}{\vec{v}}} \\
                        &=|c|||\vec{v}||,
                    \end{align*}
                    as desired.
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \vphantom
        \\
        \\
        We will now present some generalized proofs of theorems we covered special cases of in Chapter \ref{chapter:vecmat}. 
        \begin{theorem}{\Stop\,\,The Cauchy-Schwarz Inequality}{cauchyschwarzgen}
            Let \(V\) be an inner product space with \(\vec{v},\vec{w}\in V\). Then,
            \begin{equation*}
                \left|\iprod{\vec{v}}{\vec{w}}\right|\leq||\vec{v}||||\vec{w}||.
            \end{equation*}
            \begin{proof}
                If \(\vec{w}=\vec{0}_V\), the statement is trivial since both sides are zero. We have that for all \(t\in\mathbb{F}\),
                \begin{align*}
                    0&\leq||\vec{v}-t\vec{w}||^2 \\
                    &=\iprod{\vec{v}-t\vec{w}}{\vec{v}-t\vec{w}} \\
                    &=\iprod{\vec{v}}{\vec{v}-t\vec{w}}-t\iprod{\vec{w}}{\vec{v}-t\vec{w}} \\
                    &=||\vec{v}||^2-t\iprod{\vec{w}}{\vec{v}}-\bar{t}\iprod{\vec{v}}{\vec{w}}+|t|^2||\vec{w}||^2.
                \end{align*}
                The quadratic has a minimum at \(t=\frac{\iprod{\vec{v}}{\vec{w}}}{||\vec{w}||^2}=\frac{\bar{\iprod{\vec{w}}{\vec{v}}}}{||\vec{w}||^2}\) and the inequality should hold. The inequality for this \(t\) is
                \begin{equation*}
                    0\leq||\vec{v}||^2-\frac{\iprod{\vec{v}}{\vec{w}}^2}{||\vec{w}||^2}.
                \end{equation*}
                Thus, \(\left|\iprod{\vec{v}}{\vec{w}}\right|^2\leq||\vec{v}||||\vec{w}||^2\implies\left|\iprod{\vec{v}}{\vec{w}}\right|\leq||\vec{v}||||\vec{w}||\).
            \end{proof}
        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,The Triangle Inequality}{triineqgen}

            Let \(V\) be an inner product space with \(\vec{v},\vec{w}\in V\). Then,
            \begin{equation*}
                ||\vec{v}+\vec{w}||\leq||\vec{v}||+||\vec{w}||.
            \end{equation*}
            \begin{proof}
                We have
                \begin{align*}
                    ||\vec{v}+\vec{w}||^2&=\iprod{\vec{v}+\vec{w}}{\vec{v}+\vec{w}} \\
                    &=\iprod{\vec{v}}{\vec{v}}+\iprod{\vec{v}}{\vec{w}}+\iprod{\vec{w}}{\vec{v}}+\iprod{\vec{w}}{\vec{w}} \\
                    &\leq\iprod{\vec{v}}{\vec{v}}+\iprod{\vec{w}}{\vec{w}}+2\left|\iprod{\vec{v}}{\vec{w}}\right| \\
                    &\leq\iprod{\vec{v}}{\vec{v}}+\iprod{\vec{w}}{\vec{w}}+2||\vec{v}||||\vec{w}|| \\
                    &=(||\vec{v}||+||\vec{w}||)^2,
                \end{align*}
                as desired.
            \end{proof}

        \end{theorem}
        \vphantom
        \\
        \\
        We now provide notions of distance and angle.
        \begin{definition}{\Stop\,\,Distance}{distance}

            Let \(V\) be an inner product space. The distance betwen \(\vec{v},\vec{w}\in V\) is \(||\vec{v}-\vec{w}||\).
            
        \end{definition}
        \begin{definition}{\Stop\,\,Angle}{angle}

            Let \(V\) be a real inner product space. That is, \(\mathbb{F}=\mathbb{R}\). The angle between \(\vec{v}\neq\vec{0}_V\) and \(\vec{w}\neq\vec{0}_V\) is given by
            \begin{equation*}
                \theta =\arccos\left(\frac{\langle\vec{v},\vec{w}\rangle}{||\vec{v}||||\vec{w}||}\right)
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,Orthogonality}{orthogonality}

            Let \(V\) be an inner product space. Then, 
            \begin{enumerate}
                \item \(\vec{v},\vec{w}\in V\) are orthogonal if and only if \(\langle\vec{v},\vec{w}\rangle=0\).
                \item A set \(S\subseteq V\) is orthogonal if and only if for each \(\vec{v},\vec{w}\in S\), \(\langle\vec{v},\vec{w}\rangle=0\).
                \item A set \(S\subseteq V\) is orthonormal if and only if it is orthogonal and each \(\vec{v}\in S\) has \(||\vec{v}||=1\).
            \end{enumerate}

        \end{definition}
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\,The Standard Basis of \(\mathbb{R}^n\)}{stdbasisrn}

            The set \(\{\vec{e}_1,\ldots,\vec{e}_n\}\) is orthonormal when considered as a subset of \(\mathbb{R}^n\) or \(\mathbb{C}^n\).
            
        \end{example}
        \pagebreak
        
    \subsection{Orthonormal Bases and the Gram-Schmidt Process}

        Consider the following theorems and definitions.
        \begin{theorem}{\Stop\,\,Orthonormal Implies Linearly Independent}{orthlinindep}
            
            If \(\{\vec{v}_1,\ldots,\vec{v}_n\}\) is orthonormal in an inner product space \(V\), \(\{\vec{v}_1,\ldots,\vec{v}_n\}\) is linearly independent.
            \begin{proof}
                Suppose
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0}_V
                \end{equation*}
                for some \(c\in\mathbb{F}\). We must show \(c_1=\cdots=c_n=0\). For each \(i\), \(1\leq i\leq n\), consider 
                \begin{align*}
                    0&=\langle c_1\vec{v}_1+\cdots+c_n\vec{v}_n,\vec{v}_i \rangle \\
                    &=\langle c_1\vec{v}_1,\vec{v}_i\rangle+\cdots+\langle c_n\vec{v}_n,\vec{v}_i\rangle \\
                    &=c_1\langle \vec{v}_1,\vec{v}_i\rangle+\cdots+c_n\langle \vec{v}_n,\vec{v}_i\rangle \\
                    &=c_i\langle \vec{v}_i,\vec{v}_i\rangle
                \end{align*}
                We see that \(\langle\vec{v}_i,\vec{v}_i\rangle>0\), since \(\vec{v}_i\neq\vec{0}_V\). We have \(0=c_i||\vec{v}_i||^2\), implying that \(c_i=0\) for each \(i\).
            \end{proof}
            The converse is false.

        \end{theorem}
        \begin{definition}{\Stop\,\,Orthogonal Bases}{orthogonalbasis}

            A set \(B\subseteq V\) is an orthogonal basis if and only if \(B\) is a basis of \(V\) and \(B\) is orthogonal.
            
        \end{definition}
        \begin{definition}{\Stop\,\,Orthonormal Bases}{orthonormalbasis}

            A set \(B\subseteq V\) is an orthonormal basis if and only if \(B\) is a basis of \(V\) and \(B\) is orthonormal.
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        We would like to form an orthogonal set of \(n\) vectors from any linearly independent set of \(n\) vectors such that both sets span the same subspace. We present the Gram-Schmidt process.
        \begin{theorem}{\Stop\,\,The Gram-Schmidt Process}{gramschmidt}
            
            Let \(S_1=\{\vec{w}_1,\ldots,\vec{w}_n\}\) such that \(S_1\) is linearly independent. We will create \(S_2=\{\vec{v}_1,\ldots,\vec{v}_n\}\) such that \(\Span(S_1)=\Span(S_2)\).
            \begin{itemize}
                \item Let \(\vec{v}_1=\vec{w}_1\).
                \item Let \(\vec{v}_2=\vec{w}_2-\left(\frac{\iprod{\vec{w}_2}{\vec{v}_1}}{\iprod{\vec{v}_1}{\vec{v}_1}}\right)\vec{v}_1\).
                \item Let \(\vec{v}_3=\vec{w}_3-\left(\frac{\iprod{\vec{w}_3}{\vec{v}_1}}{\iprod{\vec{v}_1}{\vec{v}_1}}\right)\vec{v}_1-\left(\frac{\iprod{\vec{w}_3}{\vec{v}_2}}{\iprod{\vec{v}_2}{\vec{v}_2}}\right)\vec{v}_2\).
                \item Continue.
                \item Let \(\vec{v}_n=\vec{w}_n-\left(\frac{\iprod{\vec{w}_n}{\vec{v}_1}}{\iprod{\vec{v}_1}{\vec{v}_1}}\right)\vec{v}_1-\left(\frac{\iprod{\vec{w}_n}{\vec{v}_2}}{\iprod{\vec{v}_2}{\vec{v}_2}}\right)\vec{v}_2-\cdots-\left(\frac{\iprod{\vec{w}_n}{\vec{v}_{n-1}}}{\iprod{\vec{v}_{n-1}}{\vec{v}_{n-1}}}\right)\vec{v}_{n-1}\).
            \end{itemize}

        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Gram-Schmidt 1}{gramschmidt1}

            Apply the Gram-Schmidt process to \(\{[1,1,0,0],[1,1,0,-1]\}\subseteq\mathbb{R}^4\).
            \\
            \\
            Let \(\vec{v}_1=[1,1,0,0]\). Then,
            \begin{align*}
                v_2&=[1,1,0,-1]-\frac{\iprod{[1,1,0,-1]}{[1,1,0,0]}}{\iprod{[1,1,0,0]}{[1,1,0,0]}}[1,1,0,0] \\
                &=[1,1,0,-1]-\frac{2}{2}[1,1,0,0] \\
                &=[0,0,0,-1].
            \end{align*}
            Let \(S=\{[1,1,0,0],[0,0,0,-1]\}\). Then, \(\Span(S)=\Span(\{[1,1,0,0],[1,1,0,-1]\})\).
            
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Gram-Schmidt 2}{gramschmidt2}

            Apply the Gram-Schmidt process to \(\{[i,-i,1,1],[i,0,0,1],[0,1,-i,0]\}\subseteq\mathbb{C}^4\).
            \\
            \\
            Let \(\vec{v}_1=[i,-i,1,1]\). Then,
            \begin{align*}
                \vec{v}_2&=[i,0,0,1]-\frac{\iprod{[i,0,0,1]}{[i,-i,1,1]}}{\iprod{[i,-i,1,1]}{[i,-i,1,1]}}[i,-i,1,1] \\
                &=[i,0,0,1]-\frac{1}{2}[i,-i,1,1] \\
                &=\left[\frac{1}{2}i,\frac{1}{2}i,-\frac{1}{2},\frac{1}{2}\right].
            \end{align*}
            Then,
            \begin{align*}
                \vec{v}_3&=[0,1,-i,0]-\frac{\iprod{[0,1,-i,0]}{[i,-i,1,1]}}{\iprod{[i,-i,1,1]}{[i,-i,1,1]}}[i,-i,1,1]\\&\quad-\frac{\iprod{[0,1,-i,0]}{[i,-i,1,1]}}{\iprod{\left[\frac{1}{2}i,\frac{1}{2}i,-\frac{1}{2},\frac{1}{2}\right]}{\left[\frac{1}{2}i,\frac{1}{2}i,-\frac{1}{2},\frac{1}{2}\right]}}\left[\frac{1}{2}i,\frac{1}{2}i,-\frac{1}{2},\frac{1}{2}\right] \\
                &=[0,1,-i,0].
            \end{align*}
            Let \(S=\left\{[i,-i,1,1],\left[\frac{1}{2}i,\frac{1}{2}i,-\frac{1}{2},\frac{1}{2}\right],[0,1,-i,0]\right\}\). Then, 
            \begin{equation*}
                \Span(S)=\Span(\{[i,-i,1,1],[i,0,0,1],[0,1,-i,0]\}).
            \end{equation*}

        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Gram-Schmidt 3}{gramschmidt3}
            
            Apply the Gram-Schmidt process to \(\{1,x,x^2\}\subseteq\mathcal{P}\) with inner product given by
            \begin{equation*}
                \iprod{p(x)}{q(x)}=\int_0^1 p(x)q(x) \dd x.
            \end{equation*}
            Let \(\vec{v}_1=1\). Then,
            \begin{align*}
                \vec{v}_2&=x-\frac{\iprod{x}{1}}{\iprod{1}{1}}1 \\
                &=x-\frac{1}{2}.
            \end{align*}
            Then,
            \begin{align*}
                \vec{v}_3&=x^2-\frac{\iprod{x^2}{1}}{\iprod{1}{1}}1-\frac{\iprod{x^2}{x-\frac{1}{2}}}{\iprod{x-\frac{1}{2}}{x-\frac{1}{2}}}\left(x-\frac{1}{2}\right) \\
                &=x^2-\frac{1}{3}-\frac{\frac{1}{4}-\frac{1}{6}}{\frac{\left(1-\frac{1}{2}\right)^3-\left(0-\frac{1}{2}\right)^3}{3}}\left(x-\frac{1}{2}\right) \\
                &=x^2-\frac{1}{3}-\frac{\frac{1}{4}}{\frac{1}{4}}\left(x-\frac{1}{2}\right) \\
                &=x^2-x+\frac{1}{6}.
            \end{align*}
            Let \(S=\left\{1,x-\frac{1}{2},x^2-x+\frac{1}{6}\right\}\). Then,
            \begin{equation*}
                \Span(S)=\Span(\{1,x,x^2\}).
            \end{equation*}
            
        \end{example}
        \vphantom
        \\
        \\
        We can also normalize an orthogonal set into an orthonormal one by just dividing each \(\vec{v}_i\) by \(||\vec{v}_i||\).
        \begin{theorem}{\Stop\,\,Every Inner Product Space Has an Orthonormal Basis}{innerproductspaceorthobasis}

            Every inner product space has an orthonormal basis.
            \begin{proof}
                Suppose \(V\) is a finite dimensional inner product space. Let \(B=\{\vec{w}_1,\ldots,\vec{w}_n\}\) be a basis for \(V\). Apply the Gram-Schmidt process to find orthogonal \(C=\{\vec{v}_1,\ldots,\vec{v}_n\}\) and normalize it. This set is orthonormal and, therefore, linearly independent, and \(\Span(B)=V=\Span(C)\).
            \end{proof}
            
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding an Orthonormal Basis}{findorthnormbasis}

            Let \(W=\{[x,y,z,w]:x+y-z-w=0;x,y,z,w\in\mathbb{C}\}\). Find an orthonormal basis for \(W\).
            \\
            \\
            By the membership condition of \(W\), we can rewrite it as
            \begin{equation*}
                W=\{[x,y,z,x+y-z]:x,y,z,w\in\mathbb{C}\}.
            \end{equation*}
            We note that \([1,-1,0,0],[1,0,1,0],[1,1,1,1]\in W\). Let \(B_\neg=\{[1,-1,0,0],[1,0,1,0],[1,1,1,1]\}\). Consider
            \begin{equation*}
                \begin{bmatrix}
                    1 & -1 & 0 & 0 \\
                    1 & 0 & 1 & 0 \\
                    1 & 1 & 1 & 1
                \end{bmatrix}\underbrace{\to}_\text{RREF}
                \begin{bmatrix}
                    1 & 0 & 0 & 1 \\
                    0 & 1 & 0 & 1 \\
                    0 & 0 & 1 & -1
                \end{bmatrix}
            \end{equation*}
            implying that \(\Span(B_\neg)=\{[c_1,c_2,c_3,c_1+c_2-c_3]:c_1,c_2,c_3\in\mathbb{C}\}=W\). Now, consider
            \begin{equation*}
                \begin{bmatrix}
                    1 & 1 & 1 \\
                    -1 & 0 & 1 \\
                    0 & 1 & 1 \\
                    0 & 0 & 1
                \end{bmatrix}\underbrace{\to}_\text{RREF}
                \begin{bmatrix}
                    1 & 0 & 0 \\
                    0 & 1 & 0 \\
                    0 & 0 & 1 \\
                    0 & 0 & 0
                \end{bmatrix},
            \end{equation*}
            implying that \(B_\neg\) is linearly independent and therefore a basis for \(W\). Now, we apply the Gram-Schmidt process to obtain an orthogonal basis. Let \(\vec{v}_1=[1,-1,0,0]\). Then,
            \begin{align*}
                \vec{v}_2&=[1,0,1,0]-\frac{\iprod{1,0,1,0}{[1,-1,0,0]}}{\iprod{[1,-1,0,0]}{[1,-1,0,0]}}[1,-1,0,0] \\
                &=[1,0,1,0]-\frac{1}{2}[1,-1,0,0] \\
                &=\left[\frac{1}{2},\frac{1}{2},1,0\right].
            \end{align*}
            Then,
            \begin{align*}
                \vec{v}_3&=[1,1,1,1]-\frac{\iprod{[1,1,1,1]}{[1,-1,0,0]}}{\iprod{1,-1,0,0}{1,-1,0,0}}[1,-1,0,0]-\frac{\iprod{[1,1,1,1]}{\left[\frac{1}{2},\frac{1}{2},1,0\right]}}{\iprod{\left[\frac{1}{2},\frac{1}{2},1,0\right]}{\left[\frac{1}{2},\frac{1}{2},1,0\right]}}\left[\frac{1}{2},\frac{1}{2},1,0\right] \\
                &=[1,1,1,1]-\frac{4}{3}\left[\frac{1}{2},\frac{1}{2},1,0\right] \\
                &=\left[\frac{1}{3},\frac{1}{3},-\frac{1}{3},1\right].
            \end{align*}
            Our orthogonal basis for \(W\) is then \(B_\perp=\left\{[1,-1,0,0],\left[\frac{1}{2},\frac{1}{2},1,0\right],\left[\frac{1}{3},\frac{1}{3},-\frac{1}{3},1\right]\right\}\). To find an orthonormal basis, we simply normalize each vector to obtain
            \begin{align*}
                B_{\hat{\perp}}&=\left\{\left[\frac{\sqrt{2}}{2},-\frac{\sqrt{2}}{2},0,0\right],\left[\frac{\sqrt{6}}{6},\frac{\sqrt{6}}{6},\frac{\sqrt{6}}{3},0\right],\left[\frac{\sqrt{3}}{6},\frac{\sqrt{3}}{6},-\frac{\sqrt{3}}{6},\frac{\sqrt{3}}{2}\right]\right\}.
            \end{align*}
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        We have mentioned many methods of finding orthogonal and orthonormal bases and spanning sets. The following theorems illustrate the utility of doing so.
        \begin{theorem}{\Stop\,\,Coordinatization With Respect to an Orthogonal Basis}{coordsorthogonal}

            Let \(V\) be a finite dimensional inner product space. Let \(W\) be a subspace of \(V\). Suppose \(B=(\vec{v}_1,\ldots,\vec{v}_n)\) is a nonempty ordered orthogonal basis for \(W\). Then, for all \(\vec{w}\in W\),
            \begin{equation*}
                [\vec{w}]_B=\left[\frac{\iprod{\vec{w}}{\vec{v}_1}}{\iprod{\vec{v}_1}{\vec{v}_1}},\ldots,\frac{\iprod{\vec{w}}{\vec{v}_n}}{\iprod{\vec{v}_n}{\vec{v}_n}}\right].
            \end{equation*}
            \begin{proof}
                We know that \([\vec{w}]_B=[c_1,\ldots,c_n]\) for \(c_1,\ldots,c_n\in\mathbb{F}\). We wish to show that for each \(c_i\), with \(1\leq i\leq n\),
                \begin{equation*}
                    c_i=\frac{\iprod{\vec{v}_i}{\vec{v}_i}}{\iprod{\vec{v}_i}{\vec{v}_i}}.
                \end{equation*}
                We have that
                \begin{equation*}
                    \vec{w}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n.
                \end{equation*}
                Then,
                \begin{align*}
                    \iprod{\vec{w}}{\vec{v}_i}&=\iprod{c_1\vec{v}_1+\cdots+c_n\vec{v}_n}{\vec{v}_i} \\
                    &=c_1\iprod{\vec{v}_1}{\vec{v}_i}+\cdots+c_n\iprod{\vec{v}_n}{\vec{v}_i} \\
                    &=c_i\iprod{\vec{v}_i}{\vec{v}_i}.
                \end{align*}
                Therefore,
                \begin{equation*}
                    c_i=\frac{\iprod{\vec{w}}{\vec{v}_i}}{\iprod{\vec{v}_i}{\vec{v}_i}},
                \end{equation*}
                as desired.
            \end{proof}
        \end{theorem}
        \begin{theorem}{\Stop\,\,Coordinatization With Respect to an Orthonormal Basis}{coordsorthonormal}

            Let \(V\) be a finite dimensional inner product space. Let \(W\) be a subspace of \(V\). Suppose \(B=(\vec{v}_1,\ldots,\vec{v}_n)\) is a nonempty ordered orthonormal basis for \(W\). Then, for all \(\vec{w}\in W\),
            \begin{equation*}
                [\vec{w}]_B=\left[\iprod{\vec{w}}{\vec{v}_1},\ldots,\iprod{\vec{w}}{\vec{v}_n}\right].
            \end{equation*}
            \begin{proof}
                If \(B\) is orthonormal, \(\iprod{\vec{v}_1}{\vec{v}_1}=\cdots=\iprod{\vec{v}_n}{\vec{v}_n}=1\), so Theorem \ref{thm:coordsorthogonal} simplifies to the above.
            \end{proof}

        \end{theorem}
        \pagebreak

\section{Lecture 36: November 30, 2022}

    \subsection{Orthogonal Complements}

        Consider the following theorems and definitions.
        \begin{definition}{\Stop\,\,Orthogonal Complements}{orthocomp}

            Let \(V\) be an inner product space. Let \(S\subseteq V\). Then,
            \begin{equation*}
                S^{\perp}=\{\vec{v}\in V:\forall \vec{w}\in S, \iprod{\vec{v}}{\vec{w}}=0\}.
            \end{equation*}

        \end{definition}
        \begin{theorem}{\Stop\,\,A Useful Lemma for Finding Orthogonal Complements}{lemmafindorthocomp}
            Let \(V\) be an inner product space, and let \(W\) be a subspace of \(V\). Let \(B=\{\vec{w}_1,\ldots,\vec{w}_n\}\) be a basis of \(W\). Then, \(\vec{v}\in W^\perp\) if and only if \(\iprod{\vec{v}}{\vec{w}_1}=\cdots=\iprod{\vec{v}}{\vec{w}_n}=0\).
            \begin{proof}
                If \(\vec{v}\in W^\perp\), we easily have that \(\iprod{\vec{v}}{\vec{w}_1}=\cdots=\iprod{\vec{v}}{\vec{w}_n}=0\) since \(B\subseteq W\). Now, suppose \(\iprod{\vec{v}}{\vec{w}_1}=\cdots=\iprod{\vec{v}}{\vec{w}_n}=0\). Since \(B\) is a basis for \(W\), for \(\vec{w}\in W\), we have
                \begin{equation*}
                    \vec{w}=c_1\vec{w}_1+\cdots+c_n\vec{w}_n
                \end{equation*}
                for \(c_1,\ldots,c_n\in\mathbb{F}\). Then,
                \begin{align*}
                    \iprod{\vec{v}}{\vec{w}}&=\iprod{\vec{v}}{c_1\vec{w}_1+\cdots+c_n\vec{w}_n} \\
                    &=\iprod{\vec{v}}{c_1\vec{w}_1}+\cdots+\iprod{\vec{v}}{c_n\vec{w}_n} \\
                    &=\bar{c}_1\iprod{\vec{v}}{\vec{w}_1}+\cdots+\bar{c}_n\iprod{\vec{v}}{\vec{w}_n} \\
                    &=c_1(0)+\cdots+c_n(0) \\
                    &=0,
                \end{align*}
                so \(\vec{v}\in W^\perp\), as desired.
            \end{proof}
        \end{theorem}
        \vphantom
        \\
        \\
        Theorem \ref{thm:lemmafindorthocomp} allows us to only consider the basis vectors of a subspace when finding basis vectors for the orthogonal complement of that subspace. In general, we will form a linear system by setting the inner product of an arbitrary vector and each basis vector to zero and solving.
        \pagebreak
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Orthogonal Complement 1}{findorthocomp1}

            Let \(W=\Span(\{[1,0,0],[0,1,0]\})\subseteq\mathbb{R}^3\). Find \(W^\perp\).
            \\
            \\
            We, without any degree of formality, see that 
            \begin{equation*}
                W^\perp=\Span(\{[0,0,1]\}).
            \end{equation*}
            Geometrically, \(W\) is the \(xy\) plane and \(W^\perp\) is the \(z\) axis.
            
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Orthogonal Complement 2}{findorthocomp2}

            Let \(W=\Span(\{[1,-1,0,0],[1,0,1,0]\})\). Find \(W^\perp\).
            \\
            \\
            We see that \(W\) is a subspace of \(\mathbb{C}^4\). We must find linearly independent \(\vec{v}_1,\vec{v}_2\in\mathbb{C}^4\) such that
            \begin{equation*}
                \iprod{\vec{v}_1}{[1,-1,0,0]}=\iprod{\vec{v}_1}{[1,0,1,0]}=0
            \end{equation*}
            and
            \begin{equation*}
                \iprod{\vec{v}_2}{[1,-1,0,0]}=\iprod{\vec{v}_2}{[1,0,1,0]}=0.
            \end{equation*}
            Let \(\vec{v}_1=[x_1,y_1,z_1,w_1]\) and \(\vec{v}_2=[x_2,y_2,z_2,w_2]\). We can then form the system
            \begin{equation*}
                \begin{bmatrix}
                    1 & -1 & 0 & 0 & | & 0 \\
                    1 & 0 & 1 & 0 & | & 0
                \end{bmatrix}\underbrace{\to}_{\text{RREF}}\begin{bmatrix}
                    1 & 0 & 1 & 0 & | & 0 \\
                    0 & 1 & 1 & 0 & | & 0
                \end{bmatrix},
            \end{equation*}
            implying that \(\vec{v}_1=[0,0,0,1]\) and \(\vec{v}_2=[-1,-1,1,0]\). Note that both vectors are particular solutions to the linear system. Then, we have
            \begin{equation*}
                W^\perp=\Span(\{[0,0,0,1],[-1,-1,1,0]\}).
            \end{equation*}
            
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorems.
        \begin{theorem}{\Stop\,\,Subsets and Subspaces}{subsub}

            Let \(V\) be an inner product space. Let \(S\subseteq V\). Then,
            \begin{enumerate}
                \item \(S^\perp\) is a subspace of \(V\).
                \begin{proof}
                    We know that \(\vec{0}_V\in S^\perp\) since \(\iprod{\vec{0}_V}{\vec{w}}=0\) for all \(\vec{w}\in S\). If we take \(\vec{v}_1,\vec{v}_2\in S^\perp\), we consider
                    \begin{align*}
                        \iprod{\vec{v}_1+\vec{v}_2}{\vec{w}}&=\iprod{\vec{v}_1}{\vec{w}}+\iprod{\vec{v}_2}{\vec{w}} \\
                        &=0+0 \\
                        &=0,
                    \end{align*}
                    so \(\vec{v}_1+\vec{v}_2\in S^\perp\). For \(c\in\mathbb{F}\), we consider
                    \begin{align*}
                        \iprod{c\vec{v}_1}{\vec{w}}&=c\iprod{\vec{v}_1}{\vec{w}} \\
                        &=c(0) \\
                        &=0,
                    \end{align*}
                    so \(c\vec{v}_1\in S^\perp\), as desired.
                \end{proof}
                \item If \(W\) is a subspace of \(V\), \(W\cap W^\perp=\{\vec{0}_V\}\).
                \begin{proof}
                    Since \(W\) is a subspace of \(V\), \(\vec{0}_V\in W\). Since \(W^\perp\) is a subspace of \(V\) because \(W\subseteq V\), \(\vec{0}_V\in W^\perp\). Then, suppose \(\vec{w}\in W\cap W^\perp\). By Definition \ref{def:orthocomp}, \(\iprod{\vec{w}}{\vec{w}}=0\), so \(\vec{w}=\vec{0}_V\).
                \end{proof}
                \item \(S\subseteq (S^\perp)^\perp\).
                \begin{proof}
                    Suppose we have \(\vec{v}\in S\). We wish to show that \(\vec{v}\in (S^{\perp})^\perp\). That is, we must show that \(\vec{v}\) is orthogonal to all vectors in \(S^\perp\), which is true since \(\vec{v}\in S\).
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,Finite Dimensional Inner Product Spaces and Subspaces}{subspcs}
            
            Suppose \(V\) is a finite dimensional inner product space and \(W\) is a subspace of \(V\). Then,
            \begin{enumerate}
                \item If \(B_W=\{\vec{v}_1,\ldots,\vec{v}_k\}\) is an orthonormal basis of \(W\) and \(B_V=\{\vec{v}_1,\ldots,\vec{v}_k,\vec{w}_1,\ldots,\vec{w}_{\ell}\}\) is an orthonormal basis for \(V\), \(\{\vec{w}_1,\ldots,\vec{w}_{\ell}\}\) is an orthonormal basis for \(W^\perp\).
                \begin{proof}
                    We have that \(\{\vec{w}_1,\ldots,\vec{w}_\ell\}\) is linearly independent because it is a subset of a linearly independent set. Let \(S=\Span(\{\vec{w}_1,\ldots,\vec{w}_\ell\})\). We wish to show that \(S\subseteq W^\perp\) and \(W^\perp\subseteq S\). Suppose we have some \(\vec{s}\in S\). Then,
                    \begin{equation*}
                        \vec{s}=c_1\vec{w}_1+\cdots+c_\ell\vec{w}_\ell
                    \end{equation*}
                    for \(c_1,\ldots,c_\ell\in\mathbb{F}\). Since \(B_V\) is orthonormal, each of \(\vec{w}_1,\ldots,\vec{w}_\ell\) is orthogonal to each of \(\vec{v}_1,\ldots,\vec{v}_k\). As a result, \(\vec{s}\) is orthogonal to each \(\vec{v}_i\in B_W\) for \(1\leq i \leq k\). Therefore, \(\vec{s}\in W^\perp\), so \(S\subseteq W\). Now, suppose \(\vec{s}\in W^\perp\). By Theorem \ref{thm:coordsorthogonal}, 
                    \begin{align*}
                        [\vec{s}]_{B_V}&=\left[\frac{\iprod{\vec{s}}{\vec{v}_1}}{\iprod{\vec{v}_1}{\vec{v}_1}},\ldots,\frac{\iprod{\vec{s}}{\vec{v}_k}}{\iprod{\vec{v}_k}{\vec{v}_k}},\frac{\iprod{\vec{s}}{\vec{w}_1}}{\iprod{\vec{w}_1}{\vec{w}_1}},\ldots,\frac{\iprod{\vec{s}}{\vec{w}_\ell}}{\iprod{\vec{w}_\ell}{\vec{w}_\ell}}\right] \\
                        &=\left[0,\ldots,0,\frac{\iprod{\vec{s}}{\vec{w}_1}}{\iprod{\vec{w}_1}{\vec{w}_1}},\ldots,\frac{\iprod{\vec{s}}{\vec{w}_\ell}}{\iprod{\vec{w}_\ell}{\vec{w}_\ell}}\right],
                    \end{align*}
                    so
                    \begin{equation*}
                        \vec{s}=\frac{\iprod{\vec{s}}{\vec{w}_1}}{\iprod{\vec{w}_1}{\vec{w}_1}}\vec{w}_1+\cdots+\frac{\iprod{\vec{s}}{\vec{w}_\ell}}{\iprod{\vec{w}_\ell}{\vec{w}_\ell}}\vec{w}_\ell,
                    \end{equation*} 
                    meaning that \(\vec{s}\in\Span(\{\vec{w}_1,\ldots,\vec{w}_\ell\})\), or equivalently, \(\vec{s}\in S\), so \(W^\perp\subseteq S\), as desired.
                \end{proof}
                \item \(\dim V=\dim W+\dim W^\perp\).
                \begin{proof}
                    By the previous part, \(B_W\) is a basis for \(W\), \(B_V\) is a basis for \(V\), and \(B_{W^\perp}=\{\vec{w}_1,\ldots,\vec{w}_\ell\}\) is a basis of \(W^\perp\). We see that \(|B_W|=k\), \(|B_{W^\perp}|=\ell\), and \(|B_V|=k+\ell\). Then,
                    \begin{equation*}
                        \dim W+\dim W^\perp=|B_W|+|B_{W^\perp}|=n+\ell=|B_V|=\dim V,
                    \end{equation*}
                    as desired.
                \end{proof}
                \item \(W=(W^\perp)^\perp\).
                \begin{proof}
                    By Theorem \ref{thm:subsub}, \(W\subseteq (W^\perp)^\perp\). Let \(\dim V=n\). Then, by the previous parts, \(\dim ((W^\perp)^\perp)=n-(n-\dim W)=\dim W\). Therefore, \(W=(W^\perp)^\perp\), as desired.
                \end{proof}
            \end{enumerate}

        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,The Orthogonal Complement of the Ambient Space}{orthocompambientspc}

            Let \(V\) be an inner product space. Then, \(V^\perp=\{\vec{0}_V\}\).
            \begin{proof}
                Let \(B_V=\{\vec{v}_1,\ldots,\vec{v}_n\}\) be a basis for \(V\). Then, by Theorem \ref{thm:subspcs}, the basis for \(V^\perp\), or \(B_{V^\perp}\) is \(\emptyset\). Therefore,
                \begin{equation*}
                    V^\perp=\Span(B_{V^\perp})=\vec{0}_V,
                \end{equation*}
                as desired.
            \end{proof}
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,The Orthogonal Complement of the Zero Vector}{orthocompzerovec}

            Let \(V\) be an inner product space. Then, \(\{\vec{0}_V\}^\perp=V\).
            \begin{proof}
                By definition, \(\emptyset\) is a basis for \(\vec{0}_V\). Then, let \(B_V=\{\vec{v}_1,\ldots,\vec{v}_n\}\) be a basis for \(V\). By Theorem \ref{thm:subspcs},
                \begin{equation*}
                    \{\vec{0}_V\}^\perp=\Span(B_V)=V,
                \end{equation*}
                as desired.
            \end{proof}
            
        \end{theorem}

        \pagebreak

\section{Lecture 37: December 2, 2022}

    \subsection{Projections Onto a Subspace}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Projections Onto a Subspace With Orthogonal Bases}{projectionssubspcog}

            Let \(V\) be an inner product space. Let \(W\) be a subspace of \(V\). Let \(\{\vec{w}_1,\ldots,\vec{w}_n\}\) be an orthogonal basis for \(W\). Let \(\vec{v}\in V\). Then,
            \begin{equation*}
                \proj_W{\vec{v}}=\frac{\iprod{\vec{v}}{\vec{w}_1}}{\iprod{\vec{w}_1}{\vec{w}_1}}\vec{w}_1+\cdots+\frac{\iprod{\vec{v}}{\vec{w}_n}}{\iprod{\vec{w}_n}{\vec{w}_n}}\vec{w}_n.
            \end{equation*}
            If \(W\) is the trivial subspace, \(\proj_W{\vec{v}}=\vec{0}_V\).
        \end{definition}
        \begin{theorem}{\Stop\,\,Projections Onto a Subspace With Orthonormal Bases}{projectionssubspcon}

            Let \(V\) be an inner product space. Let \(W\) be a subspace of \(V\). Let \(\{\vec{w}_1,\ldots,\vec{w}_n\}\) be an orthonormal basis for \(W\). Let \(\vec{v}\in V\). Then,
            \begin{equation*}
                \proj_W{\vec{v}}=\iprod{\vec{v}}{\vec{w}_1}\vec{w}_1+\cdots+\iprod{\vec{v}}{\vec{w}_n}\vec{w}_n.
            \end{equation*}
            If \(W\) is the trivial subspace, \(\proj_W{\vec{v}}=\vec{0}_V\).
            \begin{proof}
                If \(\{\vec{w}_1,\ldots,\vec{w}_n\}\) is orthonormal, \(\iprod{\vec{w}_1}{\vec{w}_1}=\cdots=\iprod{\vec{w}_n}{\vec{w}_n}=1\), so Definition \ref{def:projectionssubspcog} simplifies to the above.
            \end{proof}
        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Projection 1}{proj1}

            Let \(V=\mathbb{R}^3\). Let \(W\) be the \(xy\) plane with basis \(B_W=\{[1,0.0],[0,1,0]\}\). Find \(\proj_W{\vec{v}}\).
            \\
            \\
            Note that \(B_W\) is an orthonormal basis of \(W\). If \(\vec{v}=[v_1,v_2,v_3]\),
            \begin{align*}
                \proj_W\vec{v}&=\iprod{v}{[1,0,0]}[1,0,0]+\iprod{v}{[0,1,0]}[0,1,0] \\
                &=[v_1,v_2,0],
            \end{align*}
            
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Projection Theorem}{projthm}

            If \(W\) is a finite dimensional subspace of an inner product space \(V\), with \(\vec{v}\in V\), there exist unique \(\vec{w}\in W\) and \(\hat{w}\in W^\perp\) such that \(\vec{v}=\vec{w}+\hat{w}\).
            \begin{proof}
                Let \(B_W=\{\vec{w}_1,\ldots,\vec{w}_k\}\) be an orthonormal basis for \(W\). We extend this basis to be an orthonormal basis of \(B_V=\{\vec{w}_1,\ldots,\vec{w}_k,\vec{v}_1,\ldots,\vec{v}_\ell\}\). Then, \(B_{W^\perp}=\{\vec{v}_1,\ldots,\vec{v}_\ell\}\) is a basis of \(W^\perp\). For some \(\vec{v}\in V\), we can write
                \begin{equation*}
                    \vec{v}=c_1\vec{w}_1+\cdots+c_k\vec{w}_k+d_1\vec{v}_1+\cdots+d_\ell\vec{v}_\ell
                \end{equation*}
                for some \(c_1,\ldots,c_k,d_1,\ldots,d_\ell\in\mathbb{F}\). We know
                \begin{equation*}
                    c_1\vec{w}_1+\cdots+c_k\vec{w}_k\in W,\quad d_1\vec{v}_1+\cdots+d_\ell\vec{v}_\ell\in W^\perp.
                \end{equation*}
                Thus, \(\vec{w}=c_1\vec{w}_1+\cdots+c_k\vec{w}_k\) and \(\hat{w}=d_1\vec{v}_1+\cdots+d_\ell\vec{v}_\ell\) so \(\vec{v}=\vec{w}+\hat{w}\). We have shown existence. For uniqueness, suppose \(\vec{v}=\vec{q}+\hat{q}\) where \(\vec{q}\in W\) and \(\hat{q}\in W^\perp\). We will show \(\vec{w}=\vec{q}\) and \(\hat{w}=\hat{q}\). We have
                \begin{equation*}
                    \vec{v}=\vec{q}+\hat{q}=\vec{w}+\hat{w},
                \end{equation*}
                which implies
                \begin{equation*}
                    \vec{q}-\vec{w}=\hat{w}-\hat{q}.
                \end{equation*}
                We see that \(\vec{q}-\vec{w}\in W\), meaning that \(\hat{w}-\hat{q}\in W\). But, we also know \(\hat{w}-\hat{q}\in W^\perp\), so \(\vec{q}-\vec{w}\in W^\perp\). Thus, \(\vec{q}-\vec{w},\hat{w}-\hat{q}\in W\cap W^\perp\). Since \(W\cap W^\perp=\{\vec{0}_V\}\), we have \(\vec{q}-\vec{w}=\vec{0}_V\) and \(\hat{w}-\hat{q}=\vec{0}_V\), as desired.
            \end{proof}
            
        \end{theorem}
        \pagebreak
        \begin{example}{Projection 2}{projection2}

            Let \(V=\mathbb{R}^2\). Let \(W=\{[x,y]:x-y=0;x,y\in\mathbb{R}\}\). For \(\vec{v}\in\mathbb{R}^2\), we want to find \(\vec{w}_1+\vec{w}_2\) such that \(\vec{v}=\vec{w}_1+\vec{w}_2\).
            \\
            \\
            Notice that \(\Span(\{[1,1]\})=W\). Let \(\vec{v}=[v_1,v_2]\). Then,
            \begin{align*}
                \vec{w}_1&=\frac{\iprod{[v_1,v_2]}{[1,1]}}{\iprod{[1,1]}{[1,1]}}[1,1] \\
                &=\frac{v_1+v_2}{2}[1,1] \\
                &=\left[\frac{v_1+v_2}{2},\frac{v_1+v_2}{2}\right].
            \end{align*}
            Then,
            \begin{align*}
                \vec{w}_2&=[v_1,v_2]-\left[\frac{v_1+v_2}{2},\frac{v_1+v_2}{2}\right] \\
                &=\left[\frac{2v_1-v_1-v_2}{2},\frac{2v_2-v_1-v_2}{2}\right] \\
                &=\left[\frac{v_1-v_2}{2},\frac{v_1-v_2}{2}\right].
            \end{align*}
            
        \end{example}
        \begin{example}{Projection 3}{projection3}

            Let \(V\) be the vector space of real continuous functions. Define the inner product in \(V\) with
            \begin{equation*}
                \iprod{\vec{f}}{\vec{g}}=\int_{-\pi}^\pi f(t)g(t) \dd t.
            \end{equation*}
            Write every continuous function \(\vec{f}\in V\) as \(\vec{f}_1+\vec{f}_2\).
            
        \end{example}

\pagebreak

\section{Lecture 38: December 5, 2022}

    \subsection{Orthogonal and Unitary Matrices}

        Consider the following definitions and theorems.
        \begin{definition}{\Stop\,\,Orthogonal Matrices}{orthomat}

            The nonsingular matrix \(A\in\mathcal{M}_{nn}^\mathbb{R}\) is orthogonal if and only if \(A^{-1}=A^T\).
            
        \end{definition}
        \begin{definition}{\Stop\,\,Unitary Matrices}{unitmat}

            The nonsingular matrix \(A\in\mathcal{M}_{nn}^\mathbb{C}\) is unitary if and only if \(A^{-1}=A^*\).
            
        \end{definition}
        \begin{theorem}{\Stop\,\,Orthogonal Matrices and Orthonormal Bases}{orthomatorthobas}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{R}\) is orthogonal if and only if the columns of \(A\) form an orthonormal basis for \(\mathbb{R}^n\).
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,Unitary Matrices and Orthonormal Bases}{unitarymatorthobas}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{C}\) is unitary if and only if the columns of \(A\) form an orthonormal basis for \(\mathbb{C}^n\).
            
        \end{theorem}
        \begin{definition}{\Stop\,\,Orthogonal Diagonalizability}{orthodiag}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{R}\) is orthogonally diagonalizable if and only if there exists some orthogonal \(P\) such that
            \begin{equation*}
                D=P^{-1}AP=P^TAP
            \end{equation*}
            for some diagonal matrix \(D\).
            
        \end{definition}
        \begin{definition}{\Stop\,\,Unitary Diagonalizability}{unitdiag}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{C}\) is unitarily diagonalizable if and only if there exists some unitary \(P\) such that
            \begin{equation*}
                D=P^{-1}AP=P^*AP
            \end{equation*}
            for some diagonal matrix \(D\).
            
        \end{definition}
        \pagebreak
        \begin{theorem}{\Stop\,\,Orthogonal Diagonalizability}{orthodiag}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{R}\) is orthogonally diagonalizable if and only if \(A\) is symmetric. That is, \(A=A^T\).

        \end{theorem}
        \begin{theorem}{\Stop\,\,Unitary Diagonalizability}{unitdiag}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{C}\) is unitarily diagonalizable if and only if \(A\) is normal. That is, \(A^*A=AA^*\).

        \end{theorem}