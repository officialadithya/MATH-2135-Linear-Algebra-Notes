\section{Lecture 35: November 28, 2022}

    \subsection{Inner Product Spaces}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Inner Products and Inner Product Spaces}{innerprod}

            An \(\mathbb{F}\) valued inner product on a vector space \(V\) is a function 
            \begin{equation*}
                \langle\cdot,\cdot\rangle:V\times V\to \mathbb{F}
            \end{equation*}
            such that
            \begin{enumerate}
                \item \(\forall\vec{v}\in V,\iprod{\vec{v}}{\vec{v}}\geq 0\).
                \item \(\vec{v}=\vec{0}_V\iff\iprod{\vec{v}}{\vec{v}}=0\).
                \item \(\forall\vec{u},\vec{v}\in V,\iprod{\vec{u}}{\vec{v}}=\bar{\iprod{\vec{v}}{\vec{u}}}\).
                \item \(\forall\vec{u},\vec{v},\vec{w}\in V,\iprod{\vec{u}+\vec{v}}{\vec{w}}=\iprod{\vec{u}}{\vec{w}}+\iprod{\vec{v}}{\vec{w}}\).
                \item \(\forall c\in\mathbb{F},\forall\vec{u},\vec{v}\in V,\iprod{c\vec{u}}{\vec{v}}=c\iprod{\vec{u}}{\vec{v}}\).
            \end{enumerate}
            \vphantom
            \\
            \\
            The pair \((V,\langle\cdot,\cdot\rangle)\) is called an inner product space.
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following inner product spaces.
        \begin{enumerate}
            \item The pair \((\mathbb{R}^n, \iprod{\vec{u}}{\vec{v}}=\vec{u}\cdot\vec{v}=u_1v_1+\cdots+u_nv_n)\) is a real inner product space.
            \item The pair \((\mathbb{C}^n, \iprod{\vec{u}}{\vec{v}}=\vec{u}\cdot\vec{v}=u_1\bar{v_1}+\cdots+u_n\bar{v_n})\) is a complex inner product space.
            \item The pair 
                \begin{equation*}
                    \left(V,\iprod{\vec{f}}{\vec{g}}=\int_0^1 f(x)g(x)\dd x\right)
                \end{equation*}
                is an real inner product space, for 
                \begin{equation*}
                    V=\left\{f:[0,1]\to\mathbb{R}:\forall c, \lim_{x\to c}=f(c)\right\}.
                \end{equation*}
            \item The pair
            \begin{equation*}
                \left(\mathcal{P}_n,\iprod{\vec{p}}{\vec{q}}=\int_{-1}^1 p(x)\bar{q(x)}\dd x\right)
            \end{equation*}
            is a real inner product space.
        \end{enumerate}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorems and definitions.
        \begin{theorem}{\Stop\,\,Properties of Inner Products}{innerprodprops}

            Let \(V\) be an inner product space. Suppose \(\vec{u},\vec{v},\vec{w}\in V\) and \(c\in\mathbb{F}\). Then,
            \begin{enumerate}
                \item \(F:V\to \mathbb{F},\vec{v}\mapsto\iprod{\vec{v}}{\vec{u}}\) is a linear transformation.
                \begin{proof}
                    Consider \(\vec{v}_1,\vec{v}_2\in V\). Then,
                    \begin{align*}
                        F(\vec{v}_1+\vec{v}_2)&=\iprod{\vec{v}_1+\vec{v}_2}{\vec{u}} \\
                        &=\iprod{\vec{v}_1}{u}+\iprod{\vec{v}_2}{\vec{u}} \\
                        &=F(\vec{v}_1)+F(\vec{v}_2).
                    \end{align*}
                    For \(c\in\mathbb{F}\), we have
                    \begin{align*}
                        F(c\vec{v}_1)&=\iprod{c\vec{v}_1}{\vec{u}} \\
                        &=c\iprod{\vec{v}_1}{\vec{u}},
                    \end{align*}
                    as desired.
                \end{proof}
                \item \(\iprod{\vec{0}_V}{\vec{v}}=\vec{0}_V=\iprod{\vec{v}}{\vec{0}_V}\).
                \begin{proof}
                    The first equality is derived by the first part since, for all linear transformations \(L\), \(\vec{0}_V\in\ker(L)\). The second equality is derived from conjugate symmetry.
                \end{proof}
                \item \(\iprod{\vec{u}}{\vec{v}+\vec{w}}=\iprod{\vec{u}}{\vec{v}}+\iprod{\vec{u}}{\vec{w}}\).
                \begin{proof}
                    Consider \(\iprod{\vec{u}}{\vec{v}+\vec{w}}=\bar{\iprod{\vec{v}+\vec{w}}{\vec{u}}}=\bar{\iprod{\vec{v}}{\vec{u}}}+\bar{\iprod{\vec{w}}{\vec{u}}}=\iprod{\vec{u}}{\vec{v}}+\iprod{\vec{u}}{\vec{w}}\).
                \end{proof}
                \item \(\iprod{\vec{u}}{c\vec{v}}=\bar{c}\iprod{\vec{u}}{\vec{v}}\).
                \begin{proof}
                    Consider \(\iprod{\vec{u}}{c\vec{v}}=\bar{\iprod{c\vec{v}}{\vec{u}}}=\bar{c}\bar{\iprod{\vec{v}}{\vec{w}}}=\bar{c}\iprod{\vec{u}}{\vec{v}}\).
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \begin{definition}{\Stop\,\,Norms}{norms}

            The norm associated to the inner product \(\langle\cdot,\cdot\rangle\) is
            \begin{equation*}
                ||\cdot||:V\to[0,\infty),\vec{v}\mapsto\sqrt{\iprod{\vec{v}}{\vec{v}}}.
            \end{equation*}
            
        \end{definition}
        \pagebreak
        \begin{theorem}{\Stop\,\,Properties of Norms}{propnorm}

            Suppose \(V\) is an inner product space and \(\vec{v}\in V\). Then,
            \begin{enumerate}
                \item \(||\vec{v}||=0\iff\vec{v}=\vec{0}_V\).
                \begin{proof}
                    If \(\vec{v}=\vec{0}_V\), \(\iprod{\vec{v}}{\vec{v}}=\sqrt{\iprod{\vec{v}}{\vec{v}}}=||\vec{v}||=0\). If \(||\vec{v}||=0\), \(\sqrt{\iprod{\vec{v}}{\vec{v}}}=\iprod{\vec{v}}{\vec{v}}=0\), meaning that \(\vec{v}=\vec{0}_V\).
                \end{proof}
                \item \(\forall c\in\mathbb{F},||c\vec{v}||=|c|||\vec{v}||\).
                \begin{proof}
                    Consider
                    \begin{align*}
                        ||c\vec{v}||&=\sqrt{\iprod{c\vec{v}}{c\vec{v}}} \\
                        &=\sqrt{c\bar{c}\iprod{\vec{v}}{\vec{v}}} \\
                        &=\sqrt{|c|^2\iprod{\vec{v}}{\vec{v}}} \\
                        &=|c|\sqrt{\iprod{\vec{v}}{\vec{v}}} \\
                        &=|c|||\vec{v}||,
                    \end{align*}
                    as desired.
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \vphantom
        \\
        \\
        We will now present some generalized proofs of theorems we covered special cases of in Chapter \ref{chapter:vecmat}. 
        \begin{theorem}{\Stop\,\,The Cauchy-Schwarz Inequality}{cauchyschwarzgen}
            Let \(V\) be an inner product space with \(\vec{v},\vec{w}\in V\). Then,
            \begin{equation*}
                \left|\iprod{\vec{v}}{\vec{w}}\right|\leq||\vec{v}||||\vec{w}||.
            \end{equation*}
            \begin{proof}
                If \(\vec{w}=\vec{0}_V\), the statement is trivial since both sides are zero. We have that for all \(t\in\mathbb{F}\),
                \begin{align*}
                    0&\leq||\vec{v}-t\vec{w}||^2 \\
                    &=\iprod{\vec{v}-t\vec{w}}{\vec{v}-t\vec{w}} \\
                    &=\iprod{\vec{v}}{\vec{v}-t\vec{w}}-t\iprod{\vec{w}}{\vec{v}-t\vec{w}} \\
                    &=||\vec{v}||^2-t\iprod{\vec{w}}{\vec{v}}-\bar{t}\iprod{\vec{v}}{\vec{w}}+|t|^2||\vec{w}||^2.
                \end{align*}
                The quadratic has a minimum at \(t=\frac{\iprod{\vec{v}}{\vec{w}}}{||\vec{w}||^2}=\frac{\bar{\iprod{\vec{w}}{\vec{v}}}}{||\vec{w}||^2}\) and the inequality should hold. The inequality for this \(t\) is
                \begin{equation*}
                    0\leq||\vec{v}||^2-\frac{\iprod{\vec{v}}{\vec{w}}^2}{||\vec{w}||^2}.
                \end{equation*}
                Thus, \(\left|\iprod{\vec{v}}{\vec{w}}\right|^2\leq||\vec{v}||||\vec{w}||^2\implies\left|\iprod{\vec{v}}{\vec{w}}\right|\leq||\vec{v}||||\vec{w}||\).
            \end{proof}
        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,The Triangle Inequality}{triineqgen}

            Let \(V\) be an inner product space with \(\vec{v},\vec{w}\in V\). Then,
            \begin{equation*}
                ||\vec{v}+\vec{w}||\leq||\vec{v}||+||\vec{w}||.
            \end{equation*}
            \begin{proof}
                We have
                \begin{align*}
                    ||\vec{v}+\vec{w}||^2&=\iprod{\vec{v}+\vec{w}}{\vec{v}+\vec{w}} \\
                    &=\iprod{\vec{v}}{\vec{v}}+\iprod{\vec{v}}{\vec{w}}+\iprod{\vec{w}}{\vec{v}}+\iprod{\vec{w}}{\vec{w}} \\
                    &\leq\iprod{\vec{v}}{\vec{v}}+\iprod{\vec{w}}{\vec{w}}+2\left|\iprod{\vec{v}}{\vec{w}}\right| \\
                    &\leq\iprod{\vec{v}}{\vec{v}}+\iprod{\vec{w}}{\vec{w}}+2||\vec{v}||||\vec{w}|| \\
                    &=(||\vec{v}||+||\vec{w}||)^2,
                \end{align*}
                as desired.
            \end{proof}

        \end{theorem}
        \vphantom
        \\
        \\
        We now provide notions of distance and angle.
        \begin{definition}{\Stop\,\,Distance}{distance}

            Let \(V\) be an inner product space. The distance betwen \(\vec{v},\vec{w}\in V\) is \(||\vec{v}-\vec{w}||\).
            
        \end{definition}
        \begin{definition}{\Stop\,\,Angle}{angle}

            Let \(V\) be a real inner product space. That is, \(\mathbb{F}=\mathbb{R}\). The angle between \(\vec{v}\neq\vec{0}_V\) and \(\vec{w}\neq\vec{0}_V\) is given by
            \begin{equation*}
                \theta =\arccos\left(\frac{\langle\vec{v},\vec{w}\rangle}{||\vec{v}||||\vec{w}||}\right)
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,Orthogonality}{orthogonality}

            Let \(V\) be an inner product space. Then, 
            \begin{enumerate}
                \item \(\vec{v},\vec{w}\in V\) are orthogonal if and only if \(\langle\vec{v},\vec{w}\rangle=0\).
                \item A set \(S\subseteq V\) is orthogonal if and only if for each \(\vec{v},\vec{w}\in S\), \(\langle\vec{v},\vec{w}\rangle=0\).
                \item A set \(S\subseteq V\) is orthonormal if and only if it is orthogonal and each \(\vec{v}\in S\) has \(||\vec{v}||=1\).
            \end{enumerate}

        \end{definition}
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\,The Standard Basis of \(\mathbb{R}^n\)}{stdbasisrn}

            The set \(\{\vec{e}_1,\ldots,\vec{e}_n\}\) is orthonormal when considered as a subset of \(\mathbb{R}^n\) or \(\mathbb{C}^n\).
            
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorems and definitions.
        \begin{theorem}{\Stop\,\,Orthonormal Implies Linearly Independent}{orthlinindep}
            
            If \(\{\vec{v}_1,\ldots,\vec{v}_n\}\) is orthonormal in an inner product space \(V\), \(\{\vec{v}_1,\ldots,\vec{v}_n\}\) is linearly independent.
            \begin{proof}
                Suppose
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0}_V
                \end{equation*}
                for some \(c\in\mathbb{F}\). We must show \(c_1=\cdots=c_n=0\). For each \(i\), \(1\leq i\leq n\), consider 
                \begin{align*}
                    0&=\langle c_1\vec{v}_1+\cdots+c_n\vec{v}_n,\vec{v}_i \rangle \\
                    &=\langle c_1\vec{v}_1,\vec{v}_i\rangle+\cdots+\langle c_n\vec{v}_n,\vec{v}_i\rangle \\
                    &=c_1\langle \vec{v}_1,\vec{v}_i\rangle+\cdots+c_n\langle \vec{v}_n,\vec{v}_i\rangle \\
                    &=c_i\langle \vec{v}_i,\vec{v}_i\rangle
                \end{align*}
                We see that \(\langle\vec{v}_i,\vec{v}_i\rangle>0\), since \(\vec{v}_i\neq\vec{0}_V\). We have \(0=c_i||\vec{v}_i||^2\), implying that \(c_i=0\) for each \(i\).
            \end{proof}
            The converse is false.

        \end{theorem}
        \begin{definition}{\Stop\,\,Orthogonal Bases}{orthogonalbasis}

            A set \(B\subseteq V\) is an orthogonal basis if and only if \(B\) is a basis of \(V\) and \(B\) is orthogonal.
            
        \end{definition}
        \begin{definition}{\Stop\,\,Orthonormal Bases}{orthonormalbasis}

            A set \(B\subseteq V\) is an orthonormal basis if and only if \(B\) is a basis of \(V\) and \(B\) is orthonormal.
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        We would like to form an orthogonal set of \(n\) vectors from any linearly independent set of \(n\) vectors such that both sets span the same subspace. We present the Gram-Schmidt process.
        \begin{theorem}{\Stop\,\,The Gram-Schmidt Process}{gramschmidt}
            
            Let \(S_1=\{\vec{w}_1,\ldots,\vec{w}_n\}\) such that \(S_1\) is linearly independent. We will create \(S_2=\{\vec{v}_1,\ldots,\vec{v}_n\}\) such that \(\Span(S_1)=\Span(S_2)\).
            \begin{itemize}
                \item Let \(\vec{v}_1=\vec{w}_1\).
                \item Let \(\vec{v}_2=\vec{w}_2-\left(\frac{\iprod{\vec{w}_2}{\vec{v}_1}}{\iprod{\vec{v}_1}{\vec{v}_1}}\right)\vec{v}_1\).
                \item Let \(\vec{v}_3=\vec{w}_3-\left(\frac{\iprod{\vec{w}_3}{\vec{v}_1}}{\iprod{\vec{v}_1}{\vec{v}_1}}\right)\vec{v}_1-\left(\frac{\iprod{\vec{v}_3}{\vec{v}_2}}{\iprod{\vec{v}_2}{\vec{v}_2}}\right)\vec{v}_2\).
                \item Continue.
                \item Let \(\vec{v}_n=\vec{w}_n-\left(\frac{\iprod{\vec{w}_n}{\vec{v}_1}}{\iprod{\vec{v}_1}{\vec{v}_1}}\right)\vec{v}_1-\left(\frac{\iprod{\vec{v}_n}{\vec{v}_2}}{\iprod{\vec{v}_2}{\vec{v}_2}}\right)\vec{v}_2-\cdots-\left(\frac{\iprod{\vec{v}_n}{\vec{v}_{n-1}}}{\iprod{\vec{v}_{n-1}}{\vec{v}_{n-1}}}\right)\vec{v}_{n-1}\).
            \end{itemize}

        \end{theorem}
        \vphantom
        \\
        \\
        We can also normalize an orthogonal set into an orthonormal one by just dividing each \(\vec{v}_i\) by \(||\vec{v}_i||\).
        \begin{theorem}{\Stop\,\,Every Inner Product Space Has an Orthonormal Basis}{innerproductspaceorthobasis}

            Every inner product space has an orthonormal basis.
            \begin{proof}
                Suppose \(V\) is a finite dimensional inner product space. Let \(B=\{\vec{w}_1,\ldots,\vec{w}_n\}\) be a basis for \(V\). Apply the Gram-Schmidt process to find orthogonal \(C=\{\vec{v}_1,\ldots,\vec{v}_n\}\) and normalize it. This set is orthonormal and, therefore, linearly independent, and \(\Span(B)=V=\Span(C)\).
            \end{proof}
            
        \end{theorem}

        \pagebreak

\section{Lecture 36: November 30, 2022}

    \subsection{Orthogonal Complements}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Orthogonal Complements}{orthocomp}

            Let \(V\) be an inner product space. Let \(S\subseteq V\). Then,
            \begin{equation*}
                S^{\perp}=\{\vec{v}\in V:\forall \vec{w}\in S, \iprod{\vec{v}}{\vec{w}}=0\}.
            \end{equation*}

        \end{definition}
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Orthogonal Complement 1}{findorthocomp1}

            Let \(W=\Span(\{[1,0,0],[0,1,0]\})\subseteq\mathbb{R}^3\). Find \(W^\perp\).
            \\
            \\
            We see that 
            \begin{equation*}
                W^\perp=\Span(\{[0,0,1]\}).
            \end{equation*}
            Geometrically, \(W\) is the \(xy\) plane and \(W^\perp\) is the \(z\) axis.
            
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Orthogonal Complement 2}{findorthocomp2}

           Let \(V\) be an inner product space. Find \(\{\vec{0_V}\}^\perp\).
           \\
           \\
           We see \(\{\vec{0_V}\}^\perp=V\). Also, \(V^\perp=\{\vec{0}_V\}\).
        
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorems.
        \begin{theorem}{\Stop\,\,Subsets and Subspaces}{subsub}

            Let \(V\) be an inner product space. Let \(S\subseteq V\). Then,
            \begin{enumerate}
                \item \(S^\perp\) is a subspace of \(V\).
                \begin{proof}
                    We know that \(\vec{0}_V\in S^\perp\) since \(\iprod{\vec{0}_V}{\vec{w}}=0\) for all \(\vec{w}\in S\). If we take \(\vec{v}_1,\vec{v}_2\in S^\perp\), we consider
                    \begin{align*}
                        \iprod{\vec{v}_1+\vec{v}_2}{\vec{w}}&=\iprod{\vec{v}_1}{\vec{w}}+\iprod{\vec{v}_2}{\vec{w}} \\
                        &=0+0 \\
                        &=0,
                    \end{align*}
                    so \(\vec{v}_1+\vec{v}_2\in S^\perp\). For \(c\in\mathbb{F}\), we consider
                    \begin{align*}
                        \iprod{c\vec{v}_1}{\vec{w}}&=c\iprod{\vec{v}_1}{\vec{w}} \\
                        &=c(0) \\
                        &=0,
                    \end{align*}
                    so \(c\vec{v}_1\in S^\perp\), as desired.
                \end{proof}
                \item If \(W\) is a subspace of \(V\), \(W\cap W^\perp=\{\vec{0}_V\}\).
                \begin{proof}
                    Since \(W\) is a subspace of \(V\), \(\vec{0}_V\in W\). Since \(W^\perp\) is a subspace of \(V\) because \(W\subseteq V\), \(\vec{0}_V\in W^\perp\). Then, suppose \(\vec{w}\in W\cap W^\perp\). By Definition \ref{def:orthocomp}, \(\iprod{\vec{w}}{\vec{w}}=0\), so \(\vec{w}=\vec{0}_V\).
                \end{proof}
                \item \(S\subseteq (S^\perp)^\perp\).
            \end{enumerate}
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,Finite Dimensional Inner Product Spaces and Subspaces}{subspcs}
            
            Suppose \(V\) is a finite dimensional inner product space and \(W\) is a subspace of \(V\). Then,
            \begin{enumerate}
                \item If \(\{\vec{v}_1,\ldots,\vec{v}_k\}\) is an orthonormal basis of \(W\) and \(\{\vec{v}_1,\ldots,\vec{v}_k,\vec{w}_1,\ldots,\vec{w}_{\ell}\}\) is an orthonormal basis for \(V\), \(\{\vec{w}_1,\ldots,\vec{w}_{\ell}\}\) is an orthonormal basis for \(W^\perp\).
                \begin{proof}
                    Since \(\{\vec{w}_1,\ldots,\vec{w}_{\ell}\}\) is orthonormal, it is linearly independent. We wish to show that \(\vec{w}_i\in W^\perp\) for \(1\leq i \leq \ell\) and \(\Span(\{\vec{w}_1,\ldots,\vec{w}_\ell\})=W^\perp\). For the spanning property, let \(\vec{w}\in W^\perp\). Then, \(\vec{w}\in V\), so
                    \begin{equation*}
                        \vec{w}=c_1\vec{v}_1+\cdots+c_k\vec{v}_k+d_1\vec{w}_1+\cdots+d_\ell\vec{v}_\ell
                    \end{equation*}
                    for \(c_1,\ldots,c_k\in\mathbb{F}\) and \(d_1,\ldots,d_\ell\in\mathbb{F}\). We wish to show \(c_1=\cdots=c_k=0\). Since \(\vec{w}\in W^\perp\) and \(\vec{v}_i\in W\), we have that \(\iprod{\vec{w}}{\vec{v}_i}=0\) for \(1\leq i\leq k\). But, we have
                    \begin{equation*}
                        \iprod{\vec{w}}{\vec{v}}=\iprod{\vec{w}_1=c_1\vec{v}_1+\cdots+c_k\vec{v}_k+d_1\vec{w}_1+\cdots+d_\ell\vec{v}_\ell}{\vec{v}_i}=c_i\iprod{\vec{v}_i}{\vec{v}_i}.
                    \end{equation*}
                    This implies \(c_i\iprod{\vec{v}_i}{\vec{v}_i}=0\). Since \(\vec{v}_i\neq\vec{0}_V\), \(c_i=0\), so
                    \begin{equation*}
                        \vec{w}=d_1\vec{w}_1+\cdots+d_\ell\vec{v}_\ell,
                    \end{equation*}
                    so \(W=\Span(\{\vec{w}_1,\ldots,\vec{w}_\ell\})\).
                \end{proof}
                \item \(\dim V=\dim W+\dim W^\perp\).
                \item \(W=(W^\perp)^\perp\).
            \end{enumerate}

        \end{theorem}

        \DOTHISLATER
        \DOTHISLATER

        \pagebreak

\section{Lecture 37: December 2, 2022}

    \subsection{Projections Onto a Subspace}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Projections Onto a Subspace}{projectionssubspc}

            Let \(V\) be an inner product space. Let \(W\) be a subspace of \(V\). Let \(\{\vec{w}_1,\ldots,\vec{w}_n\}\) be an orthonormal basis for \(W\). Let \(\vec{v}\in V\). Then,
            \begin{equation*}
                \proj_W{\vec{v}}=\iprod{\vec{v}}{\vec{w}_1}\vec{w}_1+\cdots+\iprod{\vec{v}}{\vec{w}_n}\vec{w}_n.
            \end{equation*}
            If \(W\) is the trivial subspace, \(\proj_W{\vec{v}}=0\).
        \end{definition}
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Projection 1}{proj1}

            Let \(V=\mathbb{R}^3\). Let \(W\) be the \(xy\) plane with basis \(B_V=\{[1,0.0],[0,1,0]\}\). Find \(\proj_W{\vec{v}}\).
            \DOTHISLATER
            
        \end{example}
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Projection Theorem}{projthm}

            If \(W\) is a finite dimensional subspace of an inner product space \(V\), with \(\vec{v}\in V\), there exist unique \(\vec{w}\in W\) and \(\hat{w}\in W^\perp\) such that \(\vec{v}=\vec{w}+\hat{w}\).
            \begin{proof}
                Let \(B_W=\{\vec{w}_1,\ldots,\vec{w}_k\}\) be an orthonormal basis for \(W\). We extend this basis to be an orthonormal basis of \(B_V=\{\vec{w}_1,\ldots,\vec{w}_k,\vec{v}_1,\ldots,\vec{v}_\ell\}\). Then, \(B_{W^\perp}=\{\vec{v}_1,\ldots,\vec{v}_\ell\}\) is a basis of \(W^\perp\). For some \(\vec{v}\in V\), we can write
                \begin{equation*}
                    \vec{v}=c_1\vec{w}_1+\cdots+c_k\vec{w}_k+d_1\vec{v}_1+\cdots+d_\ell\vec{v}_\ell
                \end{equation*}
                for some \(c_1,\ldots,c_k,d_1,\ldots,d_\ell\in\mathbb{F}\). We know
                \begin{equation*}
                    c_1\vec{w}_1+\cdots+c_k\vec{w}_k\in W,\quad d_1\vec{v}_1+\cdots+d_\ell\vec{v}_\ell\in W^\perp.
                \end{equation*}
                Thus, \(\vec{w}=c_1\vec{w}_1+\cdots+c_k\vec{w}_k\) and \(\hat{w}=d_1\vec{v}_1+\cdots+d_\ell\vec{v}_\ell\) so \(\vec{v}=\vec{w}+\hat{w}\). We have shown existence. For uniqueness, suppose \(\vec{v}=\vec{q}+\hat{q}\) where \(\vec{q}\in W\) and \(\hat{q}\in W^\perp\). We will show \(\vec{w}=\vec{q}\) and \(\hat{w}=\hat{q}\). We have
                \begin{equation*}
                    \vec{v}=\vec{q}+\hat{q}=\vec{w}+\hat{w},
                \end{equation*}
                which implies
                \begin{equation*}
                    \vec{q}-\vec{w}=\hat{w}-\hat{q}.
                \end{equation*}
                We see that \(\vec{q}-\vec{w}\in W\), meaning that \(\hat{w}-\hat{q}\in W\). But, we also know \(\hat{w}-\hat{q}\in W^\perp\), so \(\vec{q}-\vec{w}\in W^\perp\). Thus, \(\vec{q}-\vec{w},\hat{w}-\hat{q}\in W\cap W^\perp\). Since \(W\cap W^\perp=\{\vec{0}_V\}\), we have \(\vec{q}-\vec{w}=\vec{0}_V\) and \(\hat{w}-\hat{q}=\vec{0}_V\), as desired.
            \end{proof}
            
        \end{theorem}
    
\pagebreak

\section{Lecture 38: December 5, 2022}

    \subsection{Orthogonal and Unitary Matrices}

        Consider the following definitions and theorems.
        \begin{definition}{\Stop\,\,Orthogonal Matrices}{orthomat}

            The nonsingular matrix \(A\in\mathcal{M}_{nn}^\mathbb{R}\) is orthogonal if and only if \(A^{-1}=A^T\).
            
        \end{definition}
        \begin{definition}{\Stop\,\,Unitary Matrices}{unitmat}

            The nonsingular matrix \(A\in\mathcal{M}_{nn}^\mathbb{C}\) is unitary if and only if \(A^{-1}=A^*\).
            
        \end{definition}
        \begin{theorem}{\Stop\,\,Orthogonal Matrices and Orthonormal Bases}{orthomatorthobas}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{R}\) is orthogonal if and only if the columns of \(A\) form an orthonormal basis for \(\mathbb{R}^n\).
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,Unitary Matrices and Orthonormal Bases}{unitarymatorthobas}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{C}\) is unitary if and only if the columns of \(A\) form an orthonormal basis for \(\mathbb{C}^n\).
            
        \end{theorem}
        \begin{definition}{\Stop\,\,Orthogonal Diagonalizability}{orthodiag}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{R}\) is orthogonally diagonalizable if and only if there exists some orthogonal \(P\) such that
            \begin{equation*}
                D=P^{-1}AP=P^TAP
            \end{equation*}
            for some diagonal matrix \(D\).
            
        \end{definition}
        \begin{definition}{\Stop\,\,Unitary Diagonalizability}{unitdiag}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{C}\) is unitarily diagonalizable if and only if there exists some unitary \(P\) such that
            \begin{equation*}
                D=P^{-1}AP=P^*AP
            \end{equation*}
            for some diagonal matrix \(D\).
            
        \end{definition}
        \pagebreak
        \begin{theorem}{\Stop\,\,Orthogonal Diagonalizability}{orthodiag}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{R}\) is orthogonally diagonalizable if and only if \(A\) is symmetric. That is, \(A=A^T\).

        \end{theorem}
        \begin{theorem}{\Stop\,\,Unitary Diagonalizability}{unitdiag}

            The matrix \(A\in\mathcal{M}_{nn}^\mathbb{C}\) is unitarily diagonalizable if and only if \(A\) is normal. That is, \(A^*A=AA^*\).

        \end{theorem}