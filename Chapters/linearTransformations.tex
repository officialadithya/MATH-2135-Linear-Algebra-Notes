\section{Lecture 30: November 4, 2022}

    \subsection{An Introduction to Linear Transformations}

        Before proceeding into linear transformations, for a review of functions and associated terminology, consult Appendix \ref{appendix:b}. Consider the following definition.
        \begin{definition}{\Stop\,\,Linear Transformations}{lineartransformation}

            Let \(V\) and \(W\) be vector spaces. Let \(F:V\to W\) be a function. Then, \(F\) is a linear transformation if and only if both the following conditions hold:
            \begin{enumerate}
                \item \(\forall \vec{v}_1,\vec{v}_2\in V, F(\vec{v}_1+\vec{v}_2)=F(\vec{v}_1)+F(\vec{v}_2)\).
                \item \(\forall c\in\mathbb{F},\forall\vec{v}\in V, F(c\vec{v})=cF(\vec{v})\).
            \end{enumerate}
            
        \end{definition}
        \vphantom
        \\
        \\
        We remark that a linear transformation ``preserves'' the operations that give structure to the vector spaces involved: vector addition and scalar multiplication.
        \pagebreak
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Linear Transformation? 1}{lintrans1}

            Let \(F:\mathcal{M}_{mn}\to \mathcal{M}_{nm}\) where \(F(A)=A^T\). Is \(F\) a linear transformation?
            \\
            \\
            For matrices \(A_1,A_2\in\mathcal{M}_{mn}\) and scalar \(c\in\mathbb{R}\), we have
            \begin{align*}
                F(A_1+A_2)&=(A_1+A_2)^T \\
                &=A_1^T+A_2^T \\
                &=F(A_1)+F(A_2)
            \end{align*}
            and
            \begin{align*}
                F(cA_1)&=(cA_1)^T \\
                &=cA_1^T \\
                &=cF(A_1).
            \end{align*}
            Thus, \(F\) is a linear transformation.
            
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Linear Transformation? 2}{lintrans2}

            Let \(F:\mathcal{P}_n\to\mathcal{P}_{n-1}\) where \(F(\vec{p})=\vec{p}'\), the derivative of \(\vec{p}\). Is \(F\) a linear transformation?
            \\
            \\
            For \(\vec{p}_1,\vec{p}_2\in \mathcal{P}_n\), we know, from Calculus, that the derivative of a sum is the sum of the derivatives, so
            \begin{align*}
                F(\vec{p}_1+\vec{p_2})&=(\vec{p}_1+\vec{p}_2)' \\
                &=\vec{p}_1'+\vec{p}_2' \\
                &=F(\vec{p}_1)+F(\vec{p}_2).
            \end{align*}
            For \(c\in\mathbb{R}\), the constant multiple rule, from Calculus, tells us that
            \begin{align*}
                F(c\vec{p}_1)&=(c\vec{p}_1)' \\
                &=c\vec{p_1}' \\
                &=cF(\vec{p_1}).
            \end{align*}
            Thus, \(F\) is a linear transformation.
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Linear Transformation? 3}{lintrans3}

            Let \(F:\mathcal{P}_{n}\to W\) where \(W=\Span\left\{\frac{1}{s},\ldots,\frac{1}{s^{n+1}}\right\}\) and
            \begin{align*}
                F(\vec{p})&=\laplace{\vec{p}(t)}(s) \\
                &=\int_0^\infty e^{-st}\vec{p}(t)\dd t.
            \end{align*}
            Is \(F\) a linear transformation?
            \\
            \\
            For \(\vec{p}_1(t),\vec{p}_2(t)\in\mathcal{P}_n\), we have
            \begin{align*}
                F(\vec{p}_1(t)+\vec{p}_2(t))&=\int_0^\infty e^{-st}(\vec{p}_1(t)+\vec{p}_2(t))\dd t \\
                &=\int_0^\infty e^{-st}\vec{p}_1(t)+e^{-st}\vec{p}_2(t)\dd t \\
                &=\int_0^\infty e^{-st}\vec{p}_1(t)\dd t+\int_0^\infty e^{-st}\vec{p}_2(t)\dd t \\
                &=F(\vec{p}_1(t))+F(\vec{p}_2(t)).
            \end{align*}
            For \(c\in\mathbb{R}\), we have 
            \begin{align*}
                F(c\vec{p}_1(t))&=\int_0^\infty ce^{-st}\vec{p}_1(t)\dd t \\
                &=c\int_0^\infty e^{-st}\vec{p}_1(t)\dd t \\
                &=cF(\vec{p}_1(t)).
            \end{align*}
            Thus, \(F\) is a linear transformation.
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Linear Transformation? 4}{lintrans4}

            Let \(V\) be a vector space with \(\dim V=n\). Let \(B\) be an ordered basis for \(V\). Then, every \(\vec{v}\in V\) has coordinatization \([\vec{v}]_B\) with respect to \(B\). Consider the function \(F:V\to\mathbb{R}^n\) given by 
            \begin{equation*}
                F(\vec{v})=[\vec{v}]_B.
            \end{equation*}
            Is \(F\) a linear transformation?
            \\
            \\
            For \(\vec{v}_1,\vec{v}_2\in V\), we have
            \begin{align*}
                F(\vec{v}_1+\vec{v}_2)&=[\vec{v}_1+\vec{v}_2]_B \\
                &=[\vec{v}_1]_B+[\vec{v}_2]_B \\
                &=F(\vec{v}_1)+F(\vec{v}_2),
            \end{align*}
            by Theorem \ref{thm:propcoords}. Then, for \(c\in\mathbb{R}\), we have
            \begin{align*}
                F(c\vec{v}_1)&=[c\vec{v}_1]_B \\
                &=c[\vec{v}_1]_B \\
                &=cF(\vec{v}),
            \end{align*}
            also by by Theorem \ref{thm:propcoords}. Thus, \(F\) is a linear transformation.

        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        We now state some properties of linear transformations.
        \begin{theorem}{\Stop\,\,Properties of Linear Transformations}{proplintrans}

            Let \(V\) and \(W\) be vector spaces, and let \(L:V\to W\) be a linear transformation. Let \(\vec{0}_V\) be the zero vector in \(V\) and \(\vec{0}_W\) be the zero vector in \(W\). Then,
            \begin{enumerate}
                \item \(L(\vec{0}_V)=L(\vec{0}_W)\).
                \begin{proof}
                    Consider \(L(\vec{0}_V)=L(0\vec{0}_V)=0L(\vec{0}_V)=\vec{0}_W\), as desired.
                \end{proof}
                \item \(L(-\vec{v})=-L(\vec{v})\).
                \begin{proof}
                    Consider \(L(-\vec{v})=L(-1\vec{v})=-L(\vec{v})\), as desired.
                \end{proof}
                \item \(L(c_1\vec{v}_1+\cdots+c_n\vec{v}_n)=c_1L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n)\) for \(c_1,\ldots,c_n\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_n\in V\) with \(n\geq2\).
                \begin{proof}
                    We proceed by induction. For the base case when \(n=2\), we have
                    \begin{align*}
                        L(c_1\vec{v}_1+c_2\vec{v}_2)&=L(c_1\vec{v}_1)+L(c_2\vec{v}_2) \\
                        &=c_1L(\vec{v}_1)+c_2L(\vec{v}_2).
                    \end{align*}
                    Then, suppose that for all \(n=k\), 
                    \begin{equation*}
                        L(c_1\vec{v}_1+\cdots+c_k\vec{v}_k)=c_1L(\vec{v}_1)+\cdots+c_kL(\vec{v}_k).
                    \end{equation*}
                    Then, we have
                    \begin{align*}
                        L(c_1\vec{v}_1+\cdots+c_k\vec{v}_k+c_{k+1}v_{k+1})&=c_1L(\vec{v}_1)+\cdots+c_kL(\vec{v}_k)+L(c_{k+1}\vec{v}_{k+1}) \\
                        &=c_1L(\vec{v}_1)+\cdots+c_kL(\vec{v}_k)+c_{k+1}L(\vec{v}_{k+1}),
                    \end{align*}
                    as desired.
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \vphantom
        \\
        \\
        We remark that not every function between vector spaces is a linear transformation. To show that some function between vector spaces is not a linear transformation, we must show a counterexample of the conditions in Definition \ref{def:lineartransformation}.
        \pagebreak
        \\
        \\
        We now turn to compositions of linear transformations.
        \begin{theorem}{\Stop\,\,Compositions of Linear Transformations}{compositionslintrans}

            Let \(V_1\), \(V_2\), and \(V_3\) be vector spaces and \(L_1:V_1\to V_2\) and \(L_2:V_2\to V_3\) be linear transformations. Then, \((L_2\circ L_1):V_1\to V_3\) with \((L_2\circ L_1)(\vec{v})=L_2(L_1(\vec{v}))\) is a linear transformation for \(\vec{v}\in V_1\).
            \begin{proof}
                For \(\vec{v}_1,\vec{v}_2\in V_1\), we have
                \begin{align*}
                    (L_2\circ L_1)(\vec{v}_1+\vec{v}_2)&=L_2(L_1(\vec{v}_1+\vec{v}_2)) \\
                    &=L_2(L_1(\vec{v}_1)+L_1(\vec{v}_2)) \\
                    &=L_2(L_1(\vec{v}_1))+L_2(L_1(\vec{v}_2)) \\
                    &=(L_2\circ L_1)(\vec{v}_1)+(L_2\circ L_1)(\vec{v}_2).
                \end{align*}
                Then, for \(c\in\mathbb{F}\), we have
                \begin{align*}
                    (L_2\circ L_1)(c\vec{v}_1)&=L_2(L_1(c\vec{v}_1)) \\
                    &=L_2(cL_1(\vec{v}_1)) \\
                    &=cL_2(L_1(\vec{v}_1)) \\
                    &=c(L_2\circ L_1)(\vec{v}_1),
                \end{align*}
                as desired.
            \end{proof}            
        \end{theorem}
        \vphantom
        \\
        \\
        We now define a special case of linear transformations: linear operators.
        \begin{definition}{\Stop\,\,Linear Operators}{linearoperator}

            Let \(V\) be a vector space. A linear operator on \(V\) is a linear transformation whose domain and codomain are both \(V\).
            
        \end{definition}
        \vphantom
        \\
        \\
        Two special linear operators are the identity linear operator and the zero linear operator. Consider the following definitions.
        \begin{definition}{\Stop\,\,The Identity Linear Operator}{idlinop}

            Let \(V\) be a vector space. Then, the function \(i:V\to V,\vec{v}\mapsto\vec{v}\) is the identity linear operator.
            
        \end{definition}
        \begin{definition}{\Stop\,\,The Zero Linear Operator}{zerolinop}

            Let \(V\) be a vector space. Then, the function \(z:V\to V,\vec{v}\mapsto\vec{0}_V\) is the zero linear operator.
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        We end this section with a result about subspaces and linear transformations.
        \begin{theorem}{\Stop\,\,Linear Transformations and Subspaces}{lintranssubspc}

            Let \(L:V\to W\) be a linear transformation. Then,
            \begin{enumerate}
                \item If \(V'\) is a subspace of \(V\), \(L(V')=\{L(\vec{v}):\vec{v}\in V'\}\), the image of \(V'\) in \(W\), is a subspace of \(W\). That is, the range of \(L\) is a subspace of \(W\).
                \begin{proof}
                    We know \(\vec{0}_V\in V'\) since \(V'\) is a subspace. Then, \(\vec{0}_W\in L(V')\) since \(L(\vec{0}_V)=\vec{0}_W\). Next, we take \(\vec{w}_1,\vec{w}_2\in L(V')\). By definition, \(\vec{w}_1=L(\vec{v}_1)\) and \(\vec{w}_2=L(\vec{v}_2)\) for some \(\vec{v}_1,\vec{v}_2\in V'\). Then,
                    \begin{align*}
                        \vec{w}_1+\vec{w}_2&=L(\vec{v}_1)+L(\vec{v}_2) \\
                        &=L(\vec{v}_1+\vec{v}_2).
                    \end{align*}
                    Since \(V'\) is a subspace, \(\vec{v}_1+\vec{v}_2\in V'\). Then, \(\vec{w}_1+\vec{w}_2\) is the image of \(\vec{v}_1+\vec{v}_2\), so \((\vec{w}_1+\vec{w}_2)\in L(V')\). Now, for \(c\in\mathbb{F}\), we have \(c\vec{w}_1=cL(\vec{v}_1)=L(c\vec{v}_1)\). Since \(V'\) is a subspace, \(c\vec{v}_1\in V'\), so \(c\vec{w}_1\) is the image of \(c\vec{v}_1\), so \(c\vec{w}_1\in L(V')\).
                \end{proof}
                \item If \(W'\) is subspace of \(W\), then \(L^{-1}(W')=\{\vec{v}\in V:L(\vec{v})\in W'\}\), the pre-image of \(W'\) in \(V\), is a subspace of \(V\).
                \begin{proof}
                    We know \(\vec{0}_W\in W'\) since \(W'\) is a subspace. Then, \(\vec{0}_V\in L^{-1}(W')\) since \(L(\vec{0}_V)=\vec{0}_W\in W'\). Next, we take \(\vec{v}_1,\vec{v}_2\in L^{-1}(W')\), hence \(L(\vec{v}_1),L(\vec{v}_2)\in W'\). Since \(W'\) is a subspace,
                    \begin{equation*}
                        L(\vec{v}_1)+L(\vec{v}_2)\in W.
                    \end{equation*}
                    Because \(L\) is linear, \(L(\vec{v}_1)+L(\vec{v}_2)=L(\vec{v}_1+\vec{v}_2)\). Thus, \(L(\vec{v}_1+\vec{v}_2)\in W'\). That is, \((\vec{v}_1+\vec{v}_2)\in L^{-1}(W')\). Finally, for \(c\in\mathbb{F}\), we have \(L(c\vec{v}_1)=cL(\vec{v}_1)\). Since \(W'\) is a subspace and \(L(\vec{v}_1)\in W'\), we have that \(cL(\vec{v}_1)\in W'\). Thus, \(L(c\vec{v}_1)\in W'\) , so \(c\vec{v}_1\in L^{-1}(W')\).
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \pagebreak
\section{Lecture 31: November 7, 2022}

    \subsection{Linear Transformations and Bases}

        We begin with an important theorem.
        \begin{theorem}{\Stop\,\,Linear Transformations and Bases}{lineartransformationsbases}

            Let \(B=\{\vec{v}_1,\ldots,\vec{v}_n\}\) be a basis for a vector space \(V\). Let \(W\) be a vector space with arbitrary \(\vec{w}_1,\ldots,\vec{w}_n\in W\). Then, there exists a unique linear transformation \(L:V\to W\) such that
            \begin{equation*}
                L(\vec{v}_1)=\vec{w}_1,\ldots,L(\vec{v}_n)=\vec{w}_n.
            \end{equation*}
            \begin{proof}
                Let \(L:V\to W\) be a linear transformation with
                \begin{equation*}
                    (c_1\vec{v}_1+\cdots+c_n\vec{v}_n)\mapsto(c_1\vec{w}_1+\cdots+c_n\vec{w}_n)
                \end{equation*}
                for scalars \(c_1,\ldots,c_n\). We note that \(L\) is well-defined because \(c_1,\ldots,c_n\) are unique. We will show \(L\) is linear by considering
                \begin{align*}
                    L(\vec{v}+\vec{v}')&=L(c_1\vec{v}_1+\cdots+c_n\vec{v}_n+c_1'\vec{v}_1+\cdots+c_n'\vec{v}_n) \\
                    &=L((c_1+c_1')\vec{v}_1+\cdots+(c_n+c_n')\vec{v}_n) \\
                    &=(c_1+c_1')\vec{w}_1+\cdots+(c_n+c_n')\vec{w}_n \\
                    &=c_1\vec{w}_1+c_1'\vec{w}_1+\cdots+c_n\vec{w}_n+c_2'\vec{w}_n \\
                    &=c_1\vec{w}_1+\cdots+c_n\vec{w}_n+c_1'\vec{w}_1+\cdots+c_n'\vec{w}_n \\
                    &=L(\vec{v})+L(\vec{v}').
                \end{align*}
                We now consider
                \begin{align*}
                    L(c\vec{v})&=L(cc_1\vec{v}_1+\cdots+cc_n\vec{v}_n) \\
                    &=cc_1\vec{w}_1+\cdots+cc_n\vec{w}_n \\
                    &=c(c_1\vec{w}_1+\cdots+c_n\vec{w}_n) \\
                    &=cL(\vec{v}).
                \end{align*}
                Now, we will show that \(L(\vec{v}_i)=\vec{w}_i\). We have that
                \begin{align*}
                    L(\vec{v}_i)&=L(0\vec{v}_1+\cdots+1\vec{v}_i+\cdots+0\vec{v}_n) \\
                    &=\vec{w}_i.
                \end{align*}
                We have shown existence, and now, will show uniqueness. Suppose \(R:V\to W\) is a linear transformation and \(R(\vec{v}_i)=\vec{w}_i\) for \(1\leq i\leq n, i\in \mathbb{N}\). We will show that \(R\) and \(L\) are equal. Let \(\vec{v}\in V\). Then,
                \begin{align*}
                    R(\vec{v})&=R(c_1\vec{v}_1+\cdots+c_n\vec{v}_n) \\
                    &=c_1R(\vec{v}_1)+\cdots+c_nR(\vec{v}_n) \\
                    &=c_1\vec{w}_1+\cdots+c_n\vec{w}_n \\
                    &=L(\vec{v}).
                \end{align*}
                Thus, \(L\) and \(R\) are the same transformation.
            \end{proof}
            
            
        \end{theorem}

\pagebreak

\section{Lecture 32: November 9, 2022}

    \subsection{The Matrix of a Linear Transformation}

        Consider the following theorem. 
        \begin{theorem}{\Stop\,\,Matrices and Linear Transformations}{matlintrans}
            
            Let \(V\) and \(W\) be nontrivial vector spaces. Let \(B=(\vec{v}_1,\ldots,\vec{v}_n)\) and \(C=(\vec{w}_1,\ldots,\vec{w}_m)\) be ordered bases for \(V\) and \(W\), respectively. Let \(L:V\to W\) be a linear transformation. Then, there exists a unique \(A_{BC}\in\mathcal{M}_{mn}\) such that
            \begin{equation*}
                A_{BC}[\vec{v}]_{B}=[L(\vec{v})]_{C}.
            \end{equation*}
            For \(1\leq i\leq n\), the \(i\)th column of \(A_{BC}\) is \([L(\vec{v}_i)]_C\).
            \begin{proof}
                Consider \(A_{BC}\in\mathcal{M}_{mn}\) with \(i\)th column \([L(\vec{v}_i)]_C\), for \(1\leq i\leq n\). We will first show that \(A_{BC}[\vec{v}]_B=[L(\vec{v})]_C\). Suppose that \([\vec{v}]_B=[c_1,\ldots,c_n]\). Then,
                \begin{equation*}
                    \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n.
                \end{equation*}
                Then, we have
                \begin{equation*}
                    L(\vec{v})=c_1L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n).
                \end{equation*}
                Next,
                \begin{align*}
                    [L(\vec{v})]_C&=[c_1L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n)]_C \\
                    &=c_1[L(\vec{v}_1)]_C+\cdots+c_n[L(\vec{v}_n)]_C \\
                    &=A_{BC}\begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix} \\
                    &=A_{BC}[\vec{v}]_B.
                \end{align*}
                Note that the third step in the above transitive chain comes from the fact that the \(i\)th column of \(A_{BC}\) is \([L(\vec{v}_i)]_C\). For uniqueness, suppose \(H\in\mathcal{M}_{nn}\) such that \(H[\vec{v}]_B=[L(\vec{v})]_C\). We can show that \(H=A_{BC}\) if we can show that the \(i\)th column of \(H\) is the \(i\)th column of \(A_{BC}\), or equivalently, \([L(\vec{v}_i)]_C\). Consider \(\vec{v}_i\in B\). We know \([\vec{v}_i]_B=\vec{e}_i\). Then, the \(i\)th column of \(H\) is \(H\vec{e}_i=H[\vec{v}_i]_B=[L(\vec{v}_i)]_C\), which is also the \(i\)th column of \(A_{BC}\).
            \end{proof}
            
        \end{theorem}
        \vphantom
        \\
        \\
        As a remark, Theorem \ref{thm:matlintrans} shows that once we have picked ordered bases for \(V\) and \(W\), each linear transformation \(L:V\to W\) is equivalent to multiplication by a unique corresponding matrix. This matrix, \(A_{BC}\) is called the matrix of the linear transformation \(L\) with respect to the ordered bases \(B\) and \(C\). To compute \(A_{BC}\), we simply apply the linear transformation on each basis element \(\vec{v}_i\), and then express the result with respect to \(C\) to get the respective columns of \(A_{BC}\).
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Matrix for a Linear Transformation 1}{findmat1}
            
            Consider
            \begin{equation*}
                L:\mathbb{R}^2\to\mathbb{R}^2, \begin{bmatrix} x \\ y \end{bmatrix} \mapsto \begin{bmatrix} x+y \\ x \end{bmatrix}.
            \end{equation*}
            Find the matrix for the linear transformation \(L\) with respect to the ordered bases \(B=([1,0],[0,1])\) and \(C=([1,0],[0,1])\).
            \\
            \\
            Consider
            \begin{align*}
                A_{BC}&=\begin{bmatrix}
                    L\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\right) & L\left(\begin{bmatrix} 0 \\ 1 \end{bmatrix}\right)
                \end{bmatrix} \\
                &=\begin{bmatrix}
                    1 & 1 \\
                    1 & 0 \\
                \end{bmatrix}.
            \end{align*}
            Note that we did not have to explicitly coordinatize after finding the image of each element in \(B\) under \(L\) since we are using the standard basis for the codomain as well.
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Matrix for a Linear Transformation 2}{findmat2}
            
            Consider
            \begin{equation*}
                L:\mathbb{R}^2\to\mathbb{R}^2, \begin{bmatrix} x \\ y \end{bmatrix} \mapsto \begin{bmatrix} x+y \\ x \end{bmatrix}.
            \end{equation*}
            Find the matrix for the linear transformation \(L\) with respect to the ordered bases \(B=([1,0],[0,1])\) and \(C=([0,1],[1,0])\).
            \\
            \\
            Consider
            \begin{align*}
                A_{BC}&=\begin{bmatrix}
                    \left[L\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\right)\right]_C & \left[L\left(\begin{bmatrix} 0 \\ 1 \end{bmatrix}\right)\right]_C
                \end{bmatrix} \\
                &=\begin{bmatrix}
                    \left[\begin{bmatrix} 1 \\ 1 \end{bmatrix}\right]_C & \left[\begin{bmatrix} 1 \\ 0 \end{bmatrix}\right]_C
                \end{bmatrix} \\
                &=\begin{bmatrix}
                    1 & 0 \\
                    1 & 1
                \end{bmatrix}.
            \end{align*}
            Note that \(L\) is the same linear transformation as the one given in Example \ref{exa:findmat1}; however, we did need to explicitly coordinatize, since we had a nonstandard basis.
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Matrix for a Linear Transformation 3}{findmat3}
            
            Consider
            \begin{equation*}
                L:\mathcal{P}_3\to\mathbb{R}^3, c_0+c_1x+c_2x^2+c_3x^3\mapsto[c_0+c_1,2c_2,c_3-c_0].
            \end{equation*}
            Find the matrix for the linear transformation \(L\) with respect to the ordered bases \(B=(x^3,x^2,x,1)\) for \(\mathcal{P}_{n}\) and \(C=([1,0,0],[0,1,0],[0,0,1])\).
            \\
            \\
            By the definition of \(L\), we see that \(L(x^3)=[0,0,1]\), \(L(x^2)=[0,2,0]\), \(L(x)=[1,0,0]\), and \(L(1)=[1,0,-1]\). We need not perform any explicit coordinatization since we are using the standard basis for \(\mathbb{R}^3\), so,
            \begin{equation*}
                A_{BC}=\begin{bmatrix}
                    0 & 0 & 1 & 1 \\
                    0 & 2 & 0 & 0 \\
                    1 & 0 & 0 & -1
                \end{bmatrix}.
            \end{equation*}
            
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Matrix for a Linear Transformation 4}{findmat4}
            
            Consider
            \begin{equation*}
                L:\mathcal{P}_3\to\mathbb{R}^3, c_0+c_1x+c_2x^2+c_3x^3\mapsto[c_0+c_1,2c_2,c_3-c_0].
            \end{equation*}
            Find the matrix for the linear transformation \(L\) with respect to the ordered bases \(B=(x^3+x^2,x^2+x,x+1,1)\) for \(\mathcal{P}_{n}\) and \(C=([-2,1,-3],[1,-3,0],[3,-6,2])\).
            \\
            \\
            By the definition of \(L\), we see that \(L(x^3+x^2)=[0,2,0]\), \(L(x^2+x)=[1,2,0]\), \(L(x+1)=[2,0,-1]\), and \(L(1)=[1,0,-1]\). Now, we must find \([0,2,1]_C\), \([1,2,0]_C\), \([2,0,-1]_C\), and \([1,0,-1]_C\). We consider
            \begin{equation*}
                \begin{bmatrix}
                    -2 & 1 & 3 & | & 0 & 1 & 2 & 1 \\
                    1 & -3 & -6 & | & 2 & 2 & 0 & 0 \\
                    -3 & 0 & 2 & | & 1 & 0 & -1 & -1
                \end{bmatrix}\underbrace{\to}_{\text{RREF}}\begin{bmatrix}
                    1 & 0 & 0 & | & -1 & -10 & -15 & -9 \\
                    0 & 1 & 0 & | & 1 & 26 & 41 & 25 \\
                    0 & 0 & 1 & | & -1 & -15 & -23 & -14
                \end{bmatrix}
            \end{equation*}
            Thus,
            \begin{equation*}
                A_{BC}=\begin{bmatrix}
                    -1 & -10 & -15 & -9 \\
                    1 & 26 & 41 & 25 \\
                    -1 & -15 & -23 & -14
                \end{bmatrix}.
            \end{equation*}
            
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Matrices for Linear Transformation, Considering Different Bases}{matricesconsdiffbases}
            Let \(V\) and \(W\) be nontrivial vector spaces with \(B\) and \(D\) be distinct ordered bases for \(V\) and \(C\) and \(E\) be distinct ordered bases for \(W\). Suppose \(L:V\to W\) is a linear transformation with matrix \(A_{BC}\). Then,
            \begin{equation*}
                A_{DE}=QA_{BC}P^{-1}
            \end{equation*}
            where \(P\) is the transition matrix from \(B\) to \(D\) and \(Q\) is the transition matrix from \(C\) to \(E\).
            \begin{proof}
                For \(\vec{v}\in V\), consider \(A_{BC}[\vec{v}]_B=[L(\vec{v})]_C\). First, we see that \(P^{-1}[\vec{v}]_D=[\vec{v}]_B\), so we may substitute to obtain \(A_{BC}P^{-1}[\vec{v}]_D=[L(\vec{v})]_C\). If we multiply by \(Q\) on both sides, on the left, we have
                \begin{align*}
                    QA_{BC}P^{-1}[\vec{v}]_D&=Q[L(\vec{v})]_C \\
                    &=[L(\vec{v})]_E.
                \end{align*}
                Since \(A_{DE}\) is the unique matrix such that \(A_{DE}[\vec{v}]_D=[L(\vec{v})]_E\), \(A_{DE}=QA_{BC}P^{-1}\).
            \end{proof}
        \end{theorem}
        \vphantom
        \\
        \\
        As an important special case, we revisit and consider similar matrices.
        \begin{theorem}{\Stop\,\,Similar Matrices and Linear Operators}{simmatlinops}

            Let \(V\) be a vector space with bases \(C\) and \(D\). Let \(L:V\to V\) be a linear operator, so there exists some \(A_{CC}\) and \(A_{DD}\). Let \(P\) be the transition matrix from \(D\) to \(C\). Then, by Theorem \ref{thm:matricesconsdiffbases}, 
            \begin{equation*}
                A_{CC}=PA_{DD}P^{-1}
            \end{equation*}
            and
            \begin{equation*}
                A_{DD}=P^{-1}A_{CC}P.
            \end{equation*}
            Thus, \(A_{CC}\) and \(A_{DD}\) are similar. Generally, any two matrices for the same linear operator, with respect to different bases, are similar, by Definition \ref{def:similarity}.
             
        \end{theorem}
        \vphantom
        \\
        \\
        Finally, we present an important result about compositions of linear transformations and matrix multiplication.
        \begin{theorem}{\Stop\,\,The Matrix of a Composition of Linear Transformations}{matcomplintrans}

            Let \(V_1\), \(V_2\) and \(V_3\) be nontrivial finite dimensional vector spaces with ordered bases \(B\), \(C\), and \(D\), respectively. Let \(L_1:V_1\to V_2\) be a linear transformation with matrix \(A_{BC}\), and let \(L_2:V_2\to V_3\) be a linear transformation with matrix \(A_{CD}\). Then, the matrix for the composite linear transformation \(L_2\circ L_1:V_1\to V_3\), with respect to bases \(B\) and \(D\), is \(A_{CD}A_{BC}\).
            
        \end{theorem}

\pagebreak

\section{Lecture 33: November 11, 2022}

    \subsection{Kernel and Range}

        We now define some important concepts.
        \begin{definition}{\Stop\,\,Kernel}{kernel}

            The kernel of a linear transformation \(L:V\to W\), is given by
            \begin{equation*}
                \ker(L)=\{\vec{v}\in V:L(\vec{v})=\vec{0}\}.
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,Range}{range}

            The range of a linear transformation \(L:V\to W\), is given by
            \begin{equation*}
                \range(L)=\{\vec{w}\in W:\exists\vec{v}\in V, L(\vec{v})=\vec{w}\}.
            \end{equation*}
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Kernel and Range are Subspaces}{kerransubspc}

            Let \(L:V\to W\) be a linear transformation. Then, \(\ker(L)\) is a subspace of \(V\) and \(\range(L)\) is a subspace of \(W\).
            \begin{proof}
                For \(\ker(L)\), we have \(\vec{0}_V\in\ker(L)\) since \(L(\vec{0}_V)=\vec{0}_W\). Then, for \(\vec{v}_1,\vec{v}_2\in \ker(L)\), we have
                \begin{align*}
                    L(\vec{v}_1+\vec{v}_2)&=L(\vec{v}_1)+L(\vec{v}_2) \\
                    &=\vec{0}_W+\vec{0}_W \\
                    &=\vec{0}_W,
                \end{align*}
                so \(\vec{v}_1+\vec{v}_2\in\ker(L)\). Then, we also have
                \begin{align*}
                    L(c\vec{v}_1)&=cL(\vec{v}_1) \\
                    &=c\vec{0}_W \\
                    &=\vec{0}_W,
                \end{align*}
                for some \(c\in\mathbb{F}\). Thus, \(c\vec{v}_1\in\ker(L)\), as desired. For \(\range(L)\), we have \(\vec{0}_W\in\range(L)\) since \(L(\vec{0}_V)=\vec{0}_W\). Then, if \(\vec{w}_1,\vec{w}_2\in\range(L)\), \(L(\vec{v}_1)=\vec{w}_1\) and \(L(\vec{v}_2)=\vec{w}_2\) for some \(\vec{v}_1,\vec{v}_2\in V\). Then,
                \begin{align*}
                    L(\vec{v}_1+\vec{v}_2)&=L(\vec{v}_1)+L(\vec{v}_2) \\
                    &=\vec{w}_1+\vec{w}_2,
                \end{align*}
                meaning \(\vec{w}_1+\vec{w}_2\in \range(L)\). Then, we also have
                \begin{align*}
                    L(c\vec{v}_1)&=cL(\vec{v}_1) \\
                    &=c\vec{w}
                \end{align*}
                for some \(c\in\mathbb{F}\). Thus, \(c\vec{w}\in\range(L)\), as desired.
            \end{proof}
        
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the linear transformation
        \begin{equation*}
            L_A:\mathbb{R}^n\to\mathbb{R}^m, \vec{v}\mapsto A\vec{v}
        \end{equation*}
        for \(A\in\mathcal{A}_{mn}\). Now, we want to find \(\ker(L_A)\) and \(\range(L_A)\). Consider the following theorems.
        \begin{theorem}{\Stop\,\,Finding the Kernel of a Linear Transformation}{findker}

            Let \(A\in\mathcal{M}_{mn}\). Let \(L_A\) be a linear transformation with
            \begin{equation*}
                L_A:\mathbb{R}^n\to\mathbb{R}^m, \vec{v}\mapsto A\vec{v}.
            \end{equation*}
            We find a basis of \(\ker(L)\) by finding particular solutions to \([A|\vec{0}]\). We find each solution \(\vec{v}_i\) by setting the \(i\)th free variable in the system to \(1\) and the other free variables to \(0\). We end up with the set \(\{\vec{v}_1,\ldots,\vec{v}_k\}\) as a basis for \(\ker(L)\). Then, \(\ker(L)=\Span(\{\vec{v}_1,\ldots,\vec{v}_k\})\). As a remark, \(\dim(\ker(L))\) is the number of free variables in the homogeneous solution set.
        \end{theorem}
        \begin{theorem}{\Stop\,\,Finding the Range of a Linear Transformation}{findrange}

            Let \(A\in\mathcal{M}_{mn}\). Let \(L_A\) be a linear transformation with
            \begin{equation*}
                L_A:\mathbb{R}^n\to\mathbb{R}^m, \vec{v}\mapsto A\vec{v}.
            \end{equation*}
            To find \(\range(L)\), we need to row reduce \(A\). The pivot columns form a basis of \(\range(L)\). Then, \(\dim(\range(L))=\rank A\), the number of pivots.

        \end{theorem}
        \vphantom
        \\
        \\
        The next theorems combine the results of Theorem \ref{thm:findker} and Theorem \ref{thm:findrange} to find \(\dim(\ker(L))+\dim(\range(L))\), culminating in Theorem \ref{thm:dimthm}.
        \begin{theorem}{\Stop\,\,The Dimension Theorem, in \(\mathbb{R}^n\)}{dimthmrn}

            Let \(L_A\) be a linear transformation with
            \begin{equation*}
                L_A:\mathbb{R}^n\to\mathbb{R}^m, \vec{v}\mapsto A\vec{v}.
            \end{equation*}
            Then,
            \begin{enumerate}
                \item \(\dim(\range(L))=\rank A\).
                \item \(\dim(\ker(L))=n-\rank A\).
                \item \(\dim(\ker(L))+\dim(\range(L))=n\).
            \end{enumerate}
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,The Dimension Theorem}{dimthm}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation. Then,
            \begin{equation*}
                \dim(\ker(L))+\dim(\range(L))=\dim V.
            \end{equation*}
            
        \end{theorem}

    \pagebreak

    \subsection{Injections, Surjections, Bijections, and Isomorphisms}

        We state two theorems about if linear transformations are injective or surjective.
        \begin{theorem}{\Stop\,\,Determining Injectivity and Surjectivity}{detinjsurj}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation. Then,
            \begin{enumerate}
                \item The linear transformation \(L\) is injective if and only if \(\ker(L)=\{\vec{0}_V\}\).
                \begin{proof}
                    Suppose \(L\) is injective and let \(\vec{v}\in\ker(L)\). Now, \(L(\vec{v})=\vec{0}_W\). Similarly, \(L(\vec{0}_V)=\vec{0}_W\), and since \(L\) is injective, \(\vec{v}=\vec{0}_V\). Now, we suppose \(\ker(L)=\{\vec{0}_V\}\). We must show \(L\) is injective. Let \(\vec{v}_1,\vec{v}_2\in V\) with \(L(\vec{v}_1)=L(\vec{v}_2)\). We wish to show \(\vec{v}_1=\vec{v}_2\). Now, we have \(L(\vec{v}_1)-L(\vec{v}_2)=\vec{0}_W\), implying that \(L(\vec{v}_1-\vec{v}_2)=\vec{0}_W\). Thus, \(\vec{v}_1-\vec{v}_2\in\ker(L)\). Since \(\ker(L)=\{\vec{0}_V\}\), \(\vec{v}_1-\vec{v}_2=\vec{0}_V\), and so, \(\vec{v}_1=\vec{v}_2\), as desired.
                \end{proof}
                \item The linear transformation \(L\) is surjective if and only if \(\dim(\range(L))=\dim W\).
                \begin{proof}
                    By definition, \(L\) is surjective if and only if \(\range(L)=W\). Then, since \(\range(L)\) is a subspace of \(W\), \(\range(L)=W\) if and only if \(\dim(\range(L))=\dim W\) by Theorem \ref{thm:dimsubspc}.
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following definition.
        \begin{definition}{\Stop\,\,Isomorphisms}{isomorphisms}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation; \(L\) an isomorphism from \(V\) to \(W\) if and only if \(L\) is both injective and surjective, or bijective.
            
        \end{definition}
        \begin{definition}{\Stop\,\,Invertible Linear Transformations}{invtrans}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation. Then, \(L\) is an invertible linear transformation if and only if there exists some function \(M:W\to V\) such that
            \begin{equation*}
                (M\circ L)(\vec{v})=\vec{v}
            \end{equation*}
            for all \(\vec{v}\in V\) and
            \begin{equation*}
                (L\circ M)(\vec{w})=\vec{w}
            \end{equation*}
            for all \(\vec{w}\in W\).
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        \begin{theorem}{\Stop\,\,Isomorphism If And Only If Invertible}{isoinv}
            
            Let \(L:V\to W\) be a linear transformation. Then, \(L\) is an isomorphism if and only if \(L\) is an invertible linear transformation. If \(L\) is invertible, \(L^{-1}\) is also a linear transformation.
            \begin{proof}
                The first part of this theorem follows from Theorem \ref{thm:existinv}, as by definition, \(L\) is an isomorphism if and only if \(L\) is bijective. Now, we just seek to show that \(L^{-1}\) is a linear transformation. First, consider \(\vec{w}_1,\vec{w}_2\in W\)
            \end{proof}

        \end{theorem}
        \vphantom
        \\
        \\
        \DOTHISLATER
        \pagebreak

\section{Lecture 2, November 18, 2022}

    \subsection{Finding Inverses of Linear Transformations}

        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Invertibility in \(mathbb{R}^n\to\mathbb{R}^n\)}{invreals}

            If \(L:\mathbb{R}^n\to\mathbb{R}^m\) is a linear transformation, \(L\) is invertible if and only if \([L(\vec{v})]_{BC}\) is nonsingular for each ordered basis \(B\) of \(\mathbb{R}^n\) and \(C\) of \(\mathbb{R}^m\). As a remark, note that if \(L\) is invertible, \([L(\vec{v})]_{BC}\) is square.
            
        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following example.
      %  \begin{example}{\Stop\,\,Find Inverse 1}{findinv1}
%
      %      Consider
      %      \begin{equation*}
       %         L:\mathbb{R}^2\to\mathbb{R}^2,\begin{bmatrix} x \\ y \end{bmatrix}\mapsto\begin{bmatrix} 3x+y \\ x+y \end{bmatrix}.
       %     \end{equation*}
       %     Is \(L\) invertible, if so, find its inverse.
       %     \\
       %     \\
        %    Let \(B\) be an ordered basis of \(\mathbb{R}^2\) with
       %     \begin{equation*}
              %  B=\left(begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\right).
         %   \end{equation*}
          %  Then,
           % \begin{equation*}
             %   [L(\vec{v})]_{BB}=\begin{bmatrix} 3 & 1 \\ 1 & 1 \end{bmatrix}.
            %\end{equation*}
            %Since this matrix is invertible, as \(\det [L(\vec{v})]_{BB} \neq 0\), the inverse is given by
           % \begin{equation*}
            %    L^{-1}:\mathbb{R}^2\to\mathbb{R}^2,\begin{bmatrix} x \\ y \end{bmatrix}\mapsto([L(\vec{v})]_{BB})^{-1}\begin{bmatrix} x \\ y \end{bmatrix}.
           % \end{equation*}
        %\end{example}
        \vphantom
        \\
        \\
        We will now define the notion of equivalence of vector spaces.
        \begin{definition}{\Stop\,\,Isomorphic Vector Spaces}{isovec}

            Suppose \(V\) and \(W\) are vector spaces. Then, we write \(V\cong W\) if and only if there exists some linear transformation \(T:V\to W\) that is an isomorphism.
            
        \end{definition}
        \begin{theorem}{\Stop\,\,\(\cong\) is an Equivalence Relation}{isoequivrel}

            Suppose \(V\) and \(W\) are vector spaces. Then, \(\cong\) is an equivalence relation.
            \begin{proof}
                \begin{enumerate}
                    \item We must show \(V\cong V\), and use that 
                    \item Suppose \(V\cong W\) via \(L:V\to W\), and we must show that \(L^{-1}:W\to V\) is an isomorphism from \(W\) to \(V\).
                    \item Suppose \(V_1 \cong V_2\) via \(L_1:V_1\to V_2\) and \(V_2\cong V_3\) via \(L_2:V_2\to V_3\). Show that \(L_2\circ L_1:V_1\to V_3\) is an isomorphism.
                \end{enumerate}
                \DOTHISLATER
            \end{proof}

        \end{theorem}
        \begin{theorem}{\Stop\,\,Isomorphisms Preserve Linear Independence and Span}{isopreslinspan}

            Let \(V\) and \(W\) be vector spaces with \(S\subseteq V\). Let \(L:V\to W\) be an isomorphism. Then,
            \begin{enumerate}
                \item If \(S\) is linearly independent, \(L(S)=\{L(\vec{v}):\vec{v}\in S\}\).
                \begin{proof}
                    See Assignment \(5\), Question \(2\), for a similar proof.
                    \DOTHISLATER
                \end{proof}
                \item If \(\Span(S)=V\), \(\Span(L(S))=W\).
                \begin{proof}
                    Let \(\vec{w}\in W\). Since \(L\) is surjective, there exists \(\vec{v}\in V\) such that \(L(\vec{v})=\vec{w}\). Since \(\Span(S)=V\). We can write
                    \begin{equation*}
                        \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
                    \end{equation*}
                    for \(c_1,\ldots,c_n\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_n\in S\). We have that
                    \begin{align*}
                        \vec{w}&=L(vec{v}) \\
                        &=L(c_1\vec{v}_1+\cdots+c_n\vec{v}_n) \\
                        &=c_1L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n).
                    \end{align*}
                    Thus, \(\vec{w}\in\Span(L(S))\) since \(L(\vec{v}_1),\ldots,L(\vec{v}_n)\in L(S)\).
                \end{proof}
                \item If \(S\) is basis for \(V\), \(L(S)\) is a basis for \(W\).
                \begin{proof}
                    Parts \(1\) and \(2\) imply that \(L(S)\) is linearly independent and \(Span(L(S))=W\). As a remark, \(L\) maps a basis of \(V\) to a basis of \(W\). If \(B\) is a basis of \(V\) with \(|B|=n\), \(L(B)=n\).
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,Isomorphism Implies Equivalent Dimension}{equivdim}

            Let \(V\) and \(W\) be vector spaces. Then, \(V\cong W\) if and only if \(\dim V=\dim W\).
            \begin{proof}
                Let \(\dim V=n\). We wish to show \(V\cong\mathbb{R}^n\). Let \(B=(\vec{v}_1,\ldots,\vec{v}_n))\) be an ordered basis for \(V\). We will now define
                \begin{equation*}
                    L:V\to\mathbb{R}^n,\vec{v}\mapsto[\vec{v}]_B.
                \end{equation*}
                To show \(L\) is an isomorphism, we will find \(L^{-1}\). Consider 
                \begin{equation*}
                    R:\mathbb{R}^n\to V,[c_1,\ldots,c_n]\mapsto c_1\vec{v}_1+\cdots+c_n\vec{v}_n.
                \end{equation*}
                We see that \(L^{-1}=R\).
            \end{proof}
            
        \end{theorem}
