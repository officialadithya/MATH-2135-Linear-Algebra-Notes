\section{Lecture 30: November 4, 2022}

    \subsection{An Introduction to Linear Transformations}

        Before proceeding into linear transformations, for a review of functions and associated terminology, consult Appendix \ref{appendix:b}. Consider the following definition.
        \begin{definition}{\Stop\,\,Linear Transformations}{lineartransformation}

            Let \(V\) and \(W\) be vector spaces. Let \(F:V\to W\) be a function. Then, \(F\) is a linear transformation if and only if both the following conditions hold:
            \begin{enumerate}
                \item \(\forall \vec{v}_1,\vec{v}_2\in V, F(\vec{v}_1+\vec{v}_2)=F(\vec{v}_1)+F(\vec{v}_2)\).
                \item \(\forall c\in\mathbb{F},\forall\vec{v}\in V, F(c\vec{v})=cF(\vec{v})\).
            \end{enumerate}
            
        \end{definition}
        \vphantom
        \\
        \\
        We remark that a linear transformation ``preserves'' the operations that give structure to the vector spaces involved: vector addition and scalar multiplication.
        \pagebreak
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Linear Transformation? 1}{lintrans1}

            Let \(F:\mathcal{M}_{mn}\to \mathcal{M}_{nm}\) where \(F(A)=A^T\). Is \(F\) a linear transformation?
            \\
            \\
            For matrices \(A_1,A_2\in\mathcal{M}_{mn}\) and scalar \(c\in\mathbb{R}\), we have
            \begin{align*}
                F(A_1+A_2)&=(A_1+A_2)^T \\
                &=A_1^T+A_2^T \\
                &=F(A_1)+F(A_2)
            \end{align*}
            and
            \begin{align*}
                F(cA_1)&=(cA_1)^T \\
                &=cA_1^T \\
                &=cF(A_1).
            \end{align*}
            Thus, \(F\) is a linear transformation.
            
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Linear Transformation? 2}{lintrans2}

            Let \(F:\mathcal{P}_n\to\mathcal{P}_{n-1}\) where \(F(\vec{p})=\vec{p}'\), the derivative of \(\vec{p}\). Is \(F\) a linear transformation?
            \\
            \\
            For \(\vec{p}_1,\vec{p}_2\in \mathcal{P}_n\), we know, from Calculus, that the derivative of a sum is the sum of the derivatives, so
            \begin{align*}
                F(\vec{p}_1+\vec{p_2})&=(\vec{p}_1+\vec{p}_2)' \\
                &=\vec{p}_1'+\vec{p}_2' \\
                &=F(\vec{p}_1)+F(\vec{p}_2).
            \end{align*}
            For \(c\in\mathbb{R}\), the constant multiple rule, from Calculus, tells us that
            \begin{align*}
                F(c\vec{p}_1)&=(c\vec{p}_1)' \\
                &=c\vec{p_1}' \\
                &=cF(\vec{p_1}).
            \end{align*}
            Thus, \(F\) is a linear transformation.
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Linear Transformation? 3}{lintrans3}

            Let \(F:\mathcal{P}_{n}\to W\) where \(W=\Span\left\{\frac{1}{s},\ldots,\frac{1}{s^{n+1}}\right\}\) and
            \begin{align*}
                F(\vec{p})&=\laplace{\vec{p}(t)}(s) \\
                &=\int_0^\infty e^{-st}\vec{p}(t)\dd t.
            \end{align*}
            Is \(F\) a linear transformation?
            \\
            \\
            For \(\vec{p}_1(t),\vec{p}_2(t)\in\mathcal{P}_n\), we have
            \begin{align*}
                F(\vec{p}_1(t)+\vec{p}_2(t))&=\int_0^\infty e^{-st}(\vec{p}_1(t)+\vec{p}_2(t))\dd t \\
                &=\int_0^\infty e^{-st}\vec{p}_1(t)+e^{-st}\vec{p}_2(t)\dd t \\
                &=\int_0^\infty e^{-st}\vec{p}_1(t)\dd t+\int_0^\infty e^{-st}\vec{p}_2(t)\dd t \\
                &=F(\vec{p}_1(t))+F(\vec{p}_2(t)).
            \end{align*}
            For \(c\in\mathbb{R}\), we have 
            \begin{align*}
                F(c\vec{p}_1(t))&=\int_0^\infty ce^{-st}\vec{p}_1(t)\dd t \\
                &=c\int_0^\infty e^{-st}\vec{p}_1(t)\dd t \\
                &=cF(\vec{p}_1(t)).
            \end{align*}
            Thus, \(F\) is a linear transformation.
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Linear Transformation? 4}{lintrans4}

            Let \(V\) be a vector space with \(\dim V=n\). Let \(B\) be an ordered basis for \(V\). Then, every \(\vec{v}\in V\) has coordinatization \([\vec{v}]_B\) with respect to \(B\). Consider the function \(F:V\to\mathbb{R}^n\) given by 
            \begin{equation*}
                F(\vec{v})=[\vec{v}]_B.
            \end{equation*}
            Is \(F\) a linear transformation?
            \\
            \\
            For \(\vec{v}_1,\vec{v}_2\in V\), we have
            \begin{align*}
                F(\vec{v}_1+\vec{v}_2)&=[\vec{v}_1+\vec{v}_2]_B \\
                &=[\vec{v}_1]_B+[\vec{v}_2]_B \\
                &=F(\vec{v}_1)+F(\vec{v}_2),
            \end{align*}
            by Theorem \ref{thm:propcoords}. Then, for \(c\in\mathbb{R}\), we have
            \begin{align*}
                F(c\vec{v}_1)&=[c\vec{v}_1]_B \\
                &=c[\vec{v}_1]_B \\
                &=cF(\vec{v}),
            \end{align*}
            also by by Theorem \ref{thm:propcoords}. Thus, \(F\) is a linear transformation.

        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        We now state some properties of linear transformations.
        \begin{theorem}{\Stop\,\,Properties of Linear Transformations}{proplintrans}

            Let \(V\) and \(W\) be vector spaces, and let \(L:V\to W\) be a linear transformation. Let \(\vec{0}_V\) be the zero vector in \(V\) and \(\vec{0}_W\) be the zero vector in \(W\). Then,
            \begin{enumerate}
                \item \(L(\vec{0}_V)=L(\vec{0}_W)\).
                \begin{proof}
                    Consider \(L(\vec{0}_V)=L(0\vec{0}_V)=0L(\vec{0}_V)=\vec{0}_W\), as desired.
                \end{proof}
                \item \(L(-\vec{v})=-L(\vec{v})\).
                \begin{proof}
                    Consider \(L(-\vec{v})=L(-1\vec{v})=-L(\vec{v})\), as desired.
                \end{proof}
                \item \(L(c_1\vec{v}_1+\cdots+c_n\vec{v}_n)=c_1L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n)\) for \(c_1,\ldots,c_n\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_n\in V\) with \(n\geq2\).
                \begin{proof}
                    We proceed by induction. For the base case when \(n=2\), we have
                    \begin{align*}
                        L(c_1\vec{v}_1+c_2\vec{v}_2)&=L(c_1\vec{v}_1)+L(c_2\vec{v}_2) \\
                        &=c_1L(\vec{v}_1)+c_2L(\vec{v}_2).
                    \end{align*}
                    Then, suppose that for all \(n=k\), 
                    \begin{equation*}
                        L(c_1\vec{v}_1+\cdots+c_k\vec{v}_k)=c_1L(\vec{v}_1)+\cdots+c_kL(\vec{v}_k).
                    \end{equation*}
                    Then, we have
                    \begin{align*}
                        L(c_1\vec{v}_1+\cdots+c_k\vec{v}_k+c_{k+1}v_{k+1})&=c_1L(\vec{v}_1)+\cdots+c_kL(\vec{v}_k)+L(c_{k+1}\vec{v}_{k+1}) \\
                        &=c_1L(\vec{v}_1)+\cdots+c_kL(\vec{v}_k)+c_{k+1}L(\vec{v}_{k+1}),
                    \end{align*}
                    as desired.
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \vphantom
        \\
        \\
        We remark that not every function between vector spaces is a linear transformation. To show that some function between vector spaces is not a linear transformation, we must show a counterexample of the conditions in Definition \ref{def:lineartransformation}.
        \pagebreak
        \\
        \\
        We now turn to compositions of linear transformations.
        \begin{theorem}{\Stop\,\,Compositions of Linear Transformations}{compositionslintrans}

            Let \(V_1\), \(V_2\), and \(V_3\) be vector spaces and \(L_1:V_1\to V_2\) and \(L_2:V_2\to V_3\) be linear transformations. Then, \((L_2\circ L_1):V_1\to V_3\) with \((L_2\circ L_1)(\vec{v})=L_2(L_1(\vec{v}))\) is a linear transformation for \(\vec{v}\in V_1\).
            \begin{proof}
                For \(\vec{v}_1,\vec{v}_2\in V_1\), we have
                \begin{align*}
                    (L_2\circ L_1)(\vec{v}_1+\vec{v}_2)&=L_2(L_1(\vec{v}_1+\vec{v}_2)) \\
                    &=L_2(L_1(\vec{v}_1)+L_1(\vec{v}_2)) \\
                    &=L_2(L_1(\vec{v}_1))+L_2(L_1(\vec{v}_2)) \\
                    &=(L_2\circ L_1)(\vec{v}_1)+(L_2\circ L_1)(\vec{v}_2).
                \end{align*}
                Then, for \(c\in\mathbb{F}\), we have
                \begin{align*}
                    (L_2\circ L_1)(c\vec{v}_1)&=L_2(L_1(c\vec{v}_1)) \\
                    &=L_2(cL_1(\vec{v}_1)) \\
                    &=cL_2(L_1(\vec{v}_1)) \\
                    &=c(L_2\circ L_1)(\vec{v}_1),
                \end{align*}
                as desired.
            \end{proof}            
        \end{theorem}
        \vphantom
        \\
        \\
        We now define a special case of linear transformations: linear operators.
        \begin{definition}{\Stop\,\,Linear Operators}{linearoperator}

            Let \(V\) be a vector space. A linear operator on \(V\) is a linear transformation whose domain and codomain are both \(V\).
            
        \end{definition}
        \vphantom
        \\
        \\
        Two special linear operators are the identity linear operator and the zero linear operator. Consider the following definitions.
        \begin{definition}{\Stop\,\,The Identity Linear Operator}{idlinop}

            Let \(V\) be a vector space. Then, the function \(i:V\to V,\vec{v}\mapsto\vec{v}\) is the identity linear operator.
            
        \end{definition}
        \begin{definition}{\Stop\,\,The Zero Linear Operator}{zerolinop}

            Let \(V\) be a vector space. Then, the function \(z:V\to V,\vec{v}\mapsto\vec{0}_V\) is the zero linear operator.
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        We end this section with a result about subspaces and linear transformations.
        \begin{theorem}{\Stop\,\,Linear Transformations and Subspaces}{lintranssubspc}

            Let \(L:V\to W\) be a linear transformation. Then,
            \begin{enumerate}
                \item If \(V'\) is a subspace of \(V\), \(L(V')=\{L(\vec{v}):\vec{v}\in V'\}\), the image of \(V'\) in \(W\), is a subspace of \(W\). That is, the range of \(L\) is a subspace of \(W\).
                \begin{proof}
                    We know \(\vec{0}_V\in V'\) since \(V'\) is a subspace. Then, \(\vec{0}_W\in L(V')\) since \(L(\vec{0}_V)=\vec{0}_W\). Next, we take \(\vec{w}_1,\vec{w}_2\in L(V')\). By definition, \(\vec{w}_1=L(\vec{v}_1)\) and \(\vec{w}_2=L(\vec{v}_2)\) for some \(\vec{v}_1,\vec{v}_2\in V'\). Then,
                    \begin{align*}
                        \vec{w}_1+\vec{w}_2&=L(\vec{v}_1)+L(\vec{v}_2) \\
                        &=L(\vec{v}_1+\vec{v}_2).
                    \end{align*}
                    Since \(V'\) is a subspace, \(\vec{v}_1+\vec{v}_2\in V'\). Then, \(\vec{w}_1+\vec{w}_2\) is the image of \(\vec{v}_1+\vec{v}_2\), so \((\vec{w}_1+\vec{w}_2)\in L(V')\). Now, for \(c\in\mathbb{F}\), we have \(c\vec{w}_1=cL(\vec{v}_1)=L(c\vec{v}_1)\). Since \(V'\) is a subspace, \(c\vec{v}_1\in V'\), so \(c\vec{w}_1\) is the image of \(c\vec{v}_1\), so \(c\vec{w}_1\in L(V')\).
                \end{proof}
                \item If \(W'\) is subspace of \(W\), then \(L^{-1}(W')=\{\vec{v}\in V:L(\vec{v})\in W'\}\), the pre-image of \(W'\) in \(V\), is a subspace of \(V\).
                \begin{proof}
                    We know \(\vec{0}_W\in W'\) since \(W'\) is a subspace. Then, \(\vec{0}_V\in L^{-1}(W')\) since \(L(\vec{0}_V)=\vec{0}_W\in W'\). Next, we take \(\vec{v}_1,\vec{v}_2\in L^{-1}(W')\), hence \(L(\vec{v}_1),L(\vec{v}_2)\in W'\). Since \(W'\) is a subspace,
                    \begin{equation*}
                        L(\vec{v}_1)+L(\vec{v}_2)\in W.
                    \end{equation*}
                    Because \(L\) is linear, \(L(\vec{v}_1)+L(\vec{v}_2)=L(\vec{v}_1+\vec{v}_2)\). Thus, \(L(\vec{v}_1+\vec{v}_2)\in W'\). That is, \((\vec{v}_1+\vec{v}_2)\in L^{-1}(W')\). Finally, for \(c\in\mathbb{F}\), we have \(L(c\vec{v}_1)=cL(\vec{v}_1)\). Since \(W'\) is a subspace and \(L(\vec{v}_1)\in W'\), we have that \(cL(\vec{v}_1)\in W'\). Thus, \(L(c\vec{v}_1)\in W'\) , so \(c\vec{v}_1\in L^{-1}(W')\).
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \pagebreak
\section{Lecture 31: November 7, 2022}

    \subsection{Linear Transformations and Bases}

        We begin with an important theorem.
        \begin{theorem}{\Stop\,\,Linear Transformations and Bases}{lineartransformationsbases}

            Let \(B=\{\vec{v}_1,\ldots,\vec{v}_n\}\) be a basis for a vector space \(V\). Let \(W\) be a vector space with arbitrary \(\vec{w}_1,\ldots,\vec{w}_n\in W\). Then, there exists a unique linear transformation \(L:V\to W\) such that
            \begin{equation*}
                L(\vec{v}_1)=\vec{w}_1,\ldots,L(\vec{v}_n)=\vec{w}_n.
            \end{equation*}
            \begin{proof}
                Let \(L:V\to W\) be a linear transformation with
                \begin{equation*}
                    (c_1\vec{v}_1+\cdots+c_n\vec{v}_n)\mapsto(c_1\vec{w}_1+\cdots+c_n\vec{w}_n)
                \end{equation*}
                for scalars \(c_1,\ldots,c_n\). We note that \(L\) is well-defined because \(c_1,\ldots,c_n\) are unique. We will show \(L\) is linear by considering
                \begin{align*}
                    L(\vec{v}+\vec{v}')&=L(c_1\vec{v}_1+\cdots+c_n\vec{v}_n+c_1'\vec{v}_1+\cdots+c_n'\vec{v}_n) \\
                    &=L((c_1+c_1')\vec{v}_1+\cdots+(c_n+c_n')\vec{v}_n) \\
                    &=(c_1+c_1')\vec{w}_1+\cdots+(c_n+c_n')\vec{w}_n \\
                    &=c_1\vec{w}_1+c_1'\vec{w}_1+\cdots+c_n\vec{w}_n+c_2'\vec{w}_n \\
                    &=c_1\vec{w}_1+\cdots+c_n\vec{w}_n+c_1'\vec{w}_1+\cdots+c_n'\vec{w}_n \\
                    &=L(\vec{v})+L(\vec{v}').
                \end{align*}
                We now consider
                \begin{align*}
                    L(c\vec{v})&=L(cc_1\vec{v}_1+\cdots+cc_n\vec{v}_n) \\
                    &=cc_1\vec{w}_1+\cdots+cc_n\vec{w}_n \\
                    &=c(c_1\vec{w}_1+\cdots+c_n\vec{w}_n) \\
                    &=cL(\vec{v}).
                \end{align*}
                Now, we will show that \(L(\vec{v}_i)=\vec{w}_i\). We have that
                \begin{align*}
                    L(\vec{v}_i)&=L(0\vec{v}_1+\cdots+1\vec{v}_i+\cdots+0\vec{v}_n) \\
                    &=\vec{w}_i.
                \end{align*}
                We have shown existence, and now, will show uniqueness. Suppose \(R:V\to W\) is a linear transformation and \(R(\vec{v}_i)=\vec{w}_i\) for \(1\leq i\leq n, i\in \mathbb{N}\). We will show that \(R\) and \(L\) are equal. Let \(\vec{v}\in V\). Then,
                \begin{align*}
                    R(\vec{v})&=R(c_1\vec{v}_1+\cdots+c_n\vec{v}_n) \\
                    &=c_1R(\vec{v}_1)+\cdots+c_nR(\vec{v}_n) \\
                    &=c_1\vec{w}_1+\cdots+c_n\vec{w}_n \\
                    &=L(\vec{v}).
                \end{align*}
                Thus, \(L\) and \(R\) are the same transformation.
            \end{proof}
            
            
        \end{theorem}

\pagebreak

\section{Lecture 32: November 9, 2022}

    \subsection{The Matrix of a Linear Transformation}

        Consider the following theorem. 
        \begin{theorem}{\Stop\,\,Matrices and Linear Transformations}{matlintrans}
            
            Let \(V\) and \(W\) be nontrivial vector spaces. Let \(B=(\vec{v}_1,\ldots,\vec{v}_n)\) and \(C=(\vec{w}_1,\ldots,\vec{w}_m)\) be ordered bases for \(V\) and \(W\), respectively. Let \(L:V\to W\) be a linear transformation. Then, there exists a unique \(A_{BC}\in\mathcal{M}_{mn}\) such that
            \begin{equation*}
                A_{BC}[\vec{v}]_{B}=[L(\vec{v})]_{C}.
            \end{equation*}
            For \(1\leq i\leq n\), the \(i\)th column of \(A_{BC}\) is \([L(\vec{v}_i)]_C\).
            \begin{proof}
                Consider \(A_{BC}\in\mathcal{M}_{mn}\) with \(i\)th column \([L(\vec{v}_i)]_C\), for \(1\leq i\leq n\). We will first show that \(A_{BC}[\vec{v}]_B=[L(\vec{v})]_C\). Suppose that \([\vec{v}]_B=[c_1,\ldots,c_n]\). Then,
                \begin{equation*}
                    \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n.
                \end{equation*}
                Then, we have
                \begin{equation*}
                    L(\vec{v})=c_1L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n).
                \end{equation*}
                Next,
                \begin{align*}
                    [L(\vec{v})]_C&=[c_1L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n)]_C \\
                    &=c_1[L(\vec{v}_1)]_C+\cdots+c_n[L(\vec{v}_n)]_C \\
                    &=A_{BC}\begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix} \\
                    &=A_{BC}[\vec{v}]_B.
                \end{align*}
                Note that the third step in the above transitive chain comes from the fact that the \(i\)th column of \(A_{BC}\) is \([L(\vec{v}_i)]_C\). For uniqueness, suppose \(H\in\mathcal{M}_{nn}\) such that \(H[\vec{v}]_B=[L(\vec{v})]_C\). We can show that \(H=A_{BC}\) if we can show that the \(i\)th column of \(H\) is the \(i\)th column of \(A_{BC}\), or equivalently, \([L(\vec{v}_i)]_C\). Consider \(\vec{v}_i\in B\). We know \([\vec{v}_i]_B=\vec{e}_i\). Then, the \(i\)th column of \(H\) is \(H\vec{e}_i=H[\vec{v}_i]_B=[L(\vec{v}_i)]_C\), which is also the \(i\)th column of \(A_{BC}\).
            \end{proof}
            
        \end{theorem}
        \vphantom
        \\
        \\
        As a remark, Theorem \ref{thm:matlintrans} shows that once we have picked ordered bases for \(V\) and \(W\), each linear transformation \(L:V\to W\) is equivalent to multiplication by a unique corresponding matrix. This matrix, \(A_{BC}\) is called the matrix of the linear transformation \(L\) with respect to the ordered bases \(B\) and \(C\). To compute \(A_{BC}\), we simply apply the linear transformation on each basis element \(\vec{v}_i\), and then express the result with respect to \(C\) to get the respective columns of \(A_{BC}\).
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Matrix for a Linear Transformation 1}{findmat1}
            
            Consider
            \begin{equation*}
                L:\mathbb{R}^2\to\mathbb{R}^2, \begin{bmatrix} x \\ y \end{bmatrix} \mapsto \begin{bmatrix} x+y \\ x \end{bmatrix}.
            \end{equation*}
            Find the matrix for the linear transformation \(L\) with respect to the ordered bases \(B=([1,0],[0,1])\) and \(C=([1,0],[0,1])\).
            \\
            \\
            Consider
            \begin{align*}
                A_{BC}&=\begin{bmatrix}
                    L\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\right) & L\left(\begin{bmatrix} 0 \\ 1 \end{bmatrix}\right)
                \end{bmatrix} \\
                &=\begin{bmatrix}
                    1 & 1 \\
                    1 & 0 \\
                \end{bmatrix}.
            \end{align*}
            Note that we did not have to explicitly coordinatize after finding the image of each element in \(B\) under \(L\) since we are using the standard basis for the codomain as well.
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Matrix for a Linear Transformation 2}{findmat2}
            
            Consider
            \begin{equation*}
                L:\mathbb{R}^2\to\mathbb{R}^2, \begin{bmatrix} x \\ y \end{bmatrix} \mapsto \begin{bmatrix} x+y \\ x \end{bmatrix}.
            \end{equation*}
            Find the matrix for the linear transformation \(L\) with respect to the ordered bases \(B=([1,0],[0,1])\) and \(C=([0,1],[1,0])\).
            \\
            \\
            Consider
            \begin{align*}
                A_{BC}&=\begin{bmatrix}
                    \left[L\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\right)\right]_C & \left[L\left(\begin{bmatrix} 0 \\ 1 \end{bmatrix}\right)\right]_C
                \end{bmatrix} \\
                &=\begin{bmatrix}
                    \left[\begin{bmatrix} 1 \\ 1 \end{bmatrix}\right]_C & \left[\begin{bmatrix} 1 \\ 0 \end{bmatrix}\right]_C
                \end{bmatrix} \\
                &=\begin{bmatrix}
                    1 & 0 \\
                    1 & 1
                \end{bmatrix}.
            \end{align*}
            Note that \(L\) is the same linear transformation as the one given in Example \ref{exa:findmat1}; however, we did need to explicitly coordinatize, since we had a nonstandard basis.
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Matrix for a Linear Transformation 3}{findmat3}
            
            Consider
            \begin{equation*}
                L:\mathcal{P}_3\to\mathbb{R}^3, c_0+c_1x+c_2x^2+c_3x^3\mapsto[c_0+c_1,2c_2,c_3-c_0].
            \end{equation*}
            Find the matrix for the linear transformation \(L\) with respect to the ordered bases \(B=(x^3,x^2,x,1)\) for \(\mathcal{P}_{3}\) and \(C=([1,0,0],[0,1,0],[0,0,1])\) for \(\mathbb{R}^3\).
            \\
            \\
            By the definition of \(L\), we see that \(L(x^3)=[0,0,1]\), \(L(x^2)=[0,2,0]\), \(L(x)=[1,0,0]\), and \(L(1)=[1,0,-1]\). We need not perform any explicit coordinatization since we are using the standard basis for \(\mathbb{R}^3\), so,
            \begin{equation*}
                A_{BC}=\begin{bmatrix}
                    0 & 0 & 1 & 1 \\
                    0 & 2 & 0 & 0 \\
                    1 & 0 & 0 & -1
                \end{bmatrix}.
            \end{equation*}
            
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Matrix for a Linear Transformation 4}{findmat4}
            
            Consider
            \begin{equation*}
                L:\mathcal{P}_3\to\mathbb{R}^3, c_0+c_1x+c_2x^2+c_3x^3\mapsto[c_0+c_1,2c_2,c_3-c_0].
            \end{equation*}
            Find the matrix for the linear transformation \(L\) with respect to the ordered bases \(B=(x^3+x^2,x^2+x,x+1,1)\) for \(\mathcal{P}_{3}\) and \(C=([-2,1,-3],[1,-3,0],[3,-6,2])\) for \(\mathbb{R}^3\).
            \\
            \\
            By the definition of \(L\), we see that \(L(x^3+x^2)=[0,2,0]\), \(L(x^2+x)=[1,2,0]\), \(L(x+1)=[2,0,-1]\), and \(L(1)=[1,0,-1]\). Now, we must find \([0,2,1]_C\), \([1,2,0]_C\), \([2,0,-1]_C\), and \([1,0,-1]_C\). We consider
            \begin{equation*}
                \begin{bmatrix}
                    -2 & 1 & 3 & | & 0 & 1 & 2 & 1 \\
                    1 & -3 & -6 & | & 2 & 2 & 0 & 0 \\
                    -3 & 0 & 2 & | & 1 & 0 & -1 & -1
                \end{bmatrix}\underbrace{\to}_{\text{RREF}}\begin{bmatrix}
                    1 & 0 & 0 & | & -1 & -10 & -15 & -9 \\
                    0 & 1 & 0 & | & 1 & 26 & 41 & 25 \\
                    0 & 0 & 1 & | & -1 & -15 & -23 & -14
                \end{bmatrix}
            \end{equation*}
            Thus,
            \begin{equation*}
                A_{BC}=\begin{bmatrix}
                    -1 & -10 & -15 & -9 \\
                    1 & 26 & 41 & 25 \\
                    -1 & -15 & -23 & -14
                \end{bmatrix}.
            \end{equation*}
            
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Matrices for Linear Transformation, Considering Different Bases}{matricesconsdiffbases}
            Let \(V\) and \(W\) be nontrivial vector spaces with \(B\) and \(D\) be distinct ordered bases for \(V\) and \(C\) and \(E\) be distinct ordered bases for \(W\). Suppose \(L:V\to W\) is a linear transformation with matrix \(A_{BC}\). Then,
            \begin{equation*}
                A_{DE}=QA_{BC}P^{-1}
            \end{equation*}
            where \(P\) is the transition matrix from \(B\) to \(D\) and \(Q\) is the transition matrix from \(C\) to \(E\).
            \begin{proof}
                For \(\vec{v}\in V\), consider \(A_{BC}[\vec{v}]_B=[L(\vec{v})]_C\). First, we see that \(P^{-1}[\vec{v}]_D=[\vec{v}]_B\), so we may substitute to obtain \(A_{BC}P^{-1}[\vec{v}]_D=[L(\vec{v})]_C\). If we multiply by \(Q\) on both sides, on the left, we have
                \begin{align*}
                    QA_{BC}P^{-1}[\vec{v}]_D&=Q[L(\vec{v})]_C \\
                    &=[L(\vec{v})]_E.
                \end{align*}
                Since \(A_{DE}\) is the unique matrix such that \(A_{DE}[\vec{v}]_D=[L(\vec{v})]_E\), \(A_{DE}=QA_{BC}P^{-1}\).
            \end{proof}
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\Difficulty\,\,Finding a Matrix for a Linear Transformation 5}{findmat5}
            
            Consider
            \begin{equation*}
                L:\mathcal{P}_3\to\mathbb{R}^3, c_0+c_1x+c_2x^2+c_3x^3\mapsto[c_0+c_1,2c_2,c_3-c_0].
            \end{equation*}
            As seen in Example \ref{exa:findmat3}, the matrix for \(L\) using the standard bases for \(\mathcal{P_3}\) and \(\mathbb{R}^3\) was
            \begin{equation*}
                A_{BC}=\begin{bmatrix}
                    0 & 0 & 1 & 1 \\
                    0 & 2 & 0 & 0 \\
                    1 & 0 & 0 & -1
                \end{bmatrix},
            \end{equation*}
            where, again \(B=(x^3,x^2,x,1)\) and \(C=([1,0,0],[0,1,0],[0,0,1])\). We will now check our work in Example \ref{exa:findmat4}, where we saw the matrix for \(L\), with respect to the bases \(D=(x^3+x^2,x^2+x,x+1,1)\) and \(E=([-2,1,-3],[1,3,0],[3,-6,2])\) was
            \begin{equation*}
                A_{DE}=\begin{bmatrix}
                    -1 & -10 & -15 & -9 \\
                    1 & 26 & 41 & 25 \\
                    -1 & -15 & -23 & -14
                \end{bmatrix}.
            \end{equation*}
            To calculate the transition matrix \(P^{-1}\) from \(D\) to \(B\), we have
            \begin{equation*}
                \begin{bmatrix}
                    1 & 0 & 0 & 0 & | & 1 & 0 & 0 & 0 \\
                    0 & 1 & 0 & 0 & | & 1 & 1 & 0 & 0 \\
                    0 & 0 & 1 & 0 & | & 0 & 1 & 1 & 0 \\
                    0 & 0 & 0 & 1 & | & 0 & 0 & 1 & 1
                \end{bmatrix},
            \end{equation*}
            so
            \begin{equation*}
                P^{-1}=\begin{bmatrix}
                    1 & 0 & 0 & 0 \\
                    1 & 1 & 0 & 0 \\
                    0 & 1 & 1 & 0 \\
                    0 & 0 & 1 & 1
                \end{bmatrix}.
            \end{equation*}
            To calculate the transition matrix from \(C\) to \(E\), we have
            \begin{equation*}
                \begin{bmatrix}
                    -2 & 1 & 3 & | & 1 & 0 & 0 \\
                    1 & 3 & -6 & | & 0 & 1 & 0 \\
                    -3 & 0 & 2 & | & 0 & 0 & 1
                \end{bmatrix}\underbrace{\to}_\text{RREF}\begin{bmatrix}
                    1 & 0 & 0 & | & -6 & -2 & 3 \\
                    0 & 1 & 0 & | & 16 & 5 & -9 \\
                    0 & 0 & 1 & | & -9 & -3 & 5
                \end{bmatrix}.
            \end{equation*}
            so
            \begin{equation*}
                Q=\begin{bmatrix}
                    -6 & -2 & 3 \\
                    16 & 5 & -9 \\
                    -9 & -3 & 5
                \end{bmatrix}.
            \end{equation*}
            Then,
            \begin{align*}
                A_{DE}=QA_{BC}P^{-1}&=\begin{bmatrix}
                    -6 & -2 & 3 \\
                    16 & 5 & -9 \\
                    -9 & -3 & 5
                \end{bmatrix}\begin{bmatrix}
                    0 & 0 & 1 & 1 \\
                    0 & 2 & 0 & 0 \\
                    1 & 0 & 0 & -1
                \end{bmatrix}\begin{bmatrix}
                1 & 0 & 0 & 0 \\
                1 & 1 & 0 & 0 \\
                0 & 1 & 1 & 0 \\
                0 & 0 & 1 & 1
            \end{bmatrix} \\
            &=\begin{bmatrix}
                -1 & -10 & -15 & -9 \\
                1 & 26 & 41 & 25 \\
                -1 & -15 & -23 & -14
            \end{bmatrix},
            \end{align*}
            as desired.
            
        \end{example}
        \vphantom
        \\
        \\
        We now revisit and consider similar matrices.
        \begin{theorem}{\Stop\,\,Similar Matrices and Linear Operators}{simmatlinops}

            Let \(V\) be a vector space with bases \(C\) and \(D\). Let \(L:V\to V\) be a linear operator, so there exists some \(A_{CC}\) and \(A_{DD}\). Let \(P\) be the transition matrix from \(D\) to \(C\). Then, by Theorem \ref{thm:matricesconsdiffbases}, 
            \begin{equation*}
                A_{CC}=PA_{DD}P^{-1}
            \end{equation*}
            and
            \begin{equation*}
                A_{DD}=P^{-1}A_{CC}P.
            \end{equation*}
            Thus, \(A_{CC}\) and \(A_{DD}\) are similar. Generally, any two matrices for the same linear operator, with respect to different bases, are similar, by Definition \ref{def:similarity}.
             
        \end{theorem}
        \vphantom
        \\
        \\
        Finally, we present an important result about compositions of linear transformations and matrix multiplication.
        \begin{theorem}{\Stop\,\,The Matrix of a Composition of Linear Transformations}{matcomplintrans}

            Let \(V_1\), \(V_2\) and \(V_3\) be nontrivial finite dimensional vector spaces with ordered bases \(B\), \(C\), and \(D\), respectively. Let \(L_1:V_1\to V_2\) be a linear transformation with matrix \(A_{BC}\), and let \(L_2:V_2\to V_3\) be a linear transformation with matrix \(A_{CD}\). Then, the matrix, \(A_{BD}\), for the composite linear transformation \(L_2\circ L_1:V_1\to V_3\), with respect to bases \(B\) and \(D\), is \(A_{CD}A_{BC}\).
            
        \end{theorem}

\pagebreak

\section{Lecture 33: November 11, 2022}

    \subsection{Kernel and Range}

        We now define some important concepts.
        \begin{definition}{\Stop\,\,Kernel}{kernel}

            The kernel of a linear transformation \(L:V\to W\), is given by
            \begin{equation*}
                \ker(L)=\{\vec{v}\in V:L(\vec{v})=\vec{0}_W\}.
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,Range}{range}

            The range of a linear transformation \(L:V\to W\), is given by
            \begin{equation*}
                \range(L)=\{\vec{w}\in W:\exists\vec{v}\in V, L(\vec{v})=\vec{w}\}.
            \end{equation*}
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Kernel and Range are Subspaces}{kerransubspc}

            Let \(L:V\to W\) be a linear transformation. Then, \(\ker(L)\) is a subspace of \(V\) and \(\range(L)\) is a subspace of \(W\).
            \begin{proof}
                For \(\ker(L)\), we have \(\vec{0}_V\in\ker(L)\) since \(L(\vec{0}_V)=\vec{0}_W\). Then, for \(\vec{v}_1,\vec{v}_2\in \ker(L)\), we have
                \begin{align*}
                    L(\vec{v}_1+\vec{v}_2)&=L(\vec{v}_1)+L(\vec{v}_2) \\
                    &=\vec{0}_W+\vec{0}_W \\
                    &=\vec{0}_W,
                \end{align*}
                so \(\vec{v}_1+\vec{v}_2\in\ker(L)\). Then, we also have
                \begin{align*}
                    L(c\vec{v}_1)&=cL(\vec{v}_1) \\
                    &=c\vec{0}_W \\
                    &=\vec{0}_W,
                \end{align*}
                for some \(c\in\mathbb{F}\). Thus, \(c\vec{v}_1\in\ker(L)\), as desired. For \(\range(L)\), we have \(\vec{0}_W\in\range(L)\) since \(L(\vec{0}_V)=\vec{0}_W\). Then, if \(\vec{w}_1,\vec{w}_2\in\range(L)\), \(L(\vec{v}_1)=\vec{w}_1\) and \(L(\vec{v}_2)=\vec{w}_2\) for some \(\vec{v}_1,\vec{v}_2\in V\). Then,
                \begin{align*}
                    L(\vec{v}_1+\vec{v}_2)&=L(\vec{v}_1)+L(\vec{v}_2) \\
                    &=\vec{w}_1+\vec{w}_2,
                \end{align*}
                meaning \(\vec{w}_1+\vec{w}_2\in \range(L)\). Then, we also have
                \begin{align*}
                    L(c\vec{v}_1)&=cL(\vec{v}_1) \\
                    &=c\vec{w}
                \end{align*}
                for some \(c\in\mathbb{F}\). Thus, \(c\vec{w}\in\range(L)\), as desired.
            \end{proof}
        
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the linear transformation
        \begin{equation*}
            L_A:\mathbb{R}^n\to\mathbb{R}^m, \vec{v}\mapsto A\vec{v}
        \end{equation*}
        for \(A\in\mathcal{A}_{mn}\). Now, we want to find \(\ker(L_A)\) and \(\range(L_A)\). Consider the following theorems.
        \begin{theorem}{\Stop\,\,Finding the Kernel of a Linear Transformation}{findker}

            Let \(A\in\mathcal{M}_{mn}\). Let \(L_A\) be a linear transformation with
            \begin{equation*}
                L_A:\mathbb{R}^n\to\mathbb{R}^m, \vec{v}\mapsto A\vec{v}.
            \end{equation*}
            We find a basis of \(\ker(L)\) by finding particular solutions to \([A|\vec{0}]\). We find each particular solution \(\vec{v}_i\) by setting the \(i\)th free variable in the system to \(1\) and the other free variables to \(0\). We end up with the set \(\{\vec{v}_1,\ldots,\vec{v}_k\}\) as a basis for \(\ker(L)\). Then, \(\ker(L)=\Span(\{\vec{v}_1,\ldots,\vec{v}_k\})\). As a remark, \(\dim(\ker(L))\) is the number of free variables in the homogeneous solution set.
        \end{theorem}
        \begin{theorem}{\Stop\,\,Finding the Range of a Linear Transformation}{findrange}

            Let \(A\in\mathcal{M}_{mn}\). Let \(L_A\) be a linear transformation with
            \begin{equation*}
                L_A:\mathbb{R}^n\to\mathbb{R}^m, \vec{v}\mapsto A\vec{v}.
            \end{equation*}
            To find \(\range(L)\), we need to row reduce \(A\). The columns in \(A\) corresponding to the pivot columns after row reduction is complete form a basis of \(\range(L)\). This is because the span of the columns of \(A\) is \(\range(L)\), and we must then form a basis by keeping only the pivot columns. As a remark, \(\dim(\range(L))=\rank A\), the number of pivot columns.

        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Kernel}{findker}

            Let \(L:\mathbb{R}^5\to\mathbb{R}^4,\vec{v}\mapsto A\vec{v}\), where
            \begin{equation*}
                A=\begin{bmatrix}
                    8 & 4 & 16 & 32 & 0 \\
                    4 & 2 & 10 & 22 & -4 \\
                    -2 & -1 & -5 & -11 & 7 \\
                    6 & 3 & 15 & 33 & -7
                \end{bmatrix}.
            \end{equation*}
            Find \(\ker(L)\).
            \\
            \\
            To find \(\ker(L)\), we solve \([A|\vec{0}]\) by row reduction to obtain
            \begin{equation*}
                \begin{bmatrix}
                    1 & \frac{1}{2} & 0 & -2 & 0 & | & 0 \\
                    0 & 0 & 1 & 3 & 0 & | & 0 \\
                    0 & 0 & 0 & 0 & 1 & | & 0 \\
                    0 & 0 & 0 & 0 & 0 & | & 0 
                \end{bmatrix}.
            \end{equation*}
            We see that there are free variables in the second and fourth columns. The solution set to the system is
            \begin{equation*}
                \left\{\left[-\frac{1}{2}c_1+2c_2,c_1,-3c_2,c_2,0\right]:c_1,c_2\in\mathbb{R}\right\}.
            \end{equation*}
            If \(c_1=1\) and \(c_2=0\), we have the particular solution \(\vec{v}_1=[-\frac{1}{2},1,0,0,0]\). If \(c_1=0\) and \(c_2=1\), we have \(\vec{v}_2=[2,0,-3,1,0]\). Thus,
            \begin{equation*}
                \ker(L)=\left\{c_1\left[-\frac{1}{2},1,0,0,0\right]+c_2[2,0,-3,1,0]:c_1,c_2\in\mathbb{R}\right\}.
            \end{equation*}
            It is worth noting that the initial solution set was indeed also \(\ker(L)\), but it is nice to see \(\ker(L)\) as the span of a basis of \(\ker(L)\). We will further simplify to obtain
            \begin{equation*}
                \ker(L)=\left\{c_1\left[-1,2,0,0,0\right]+c_2[2,0,-3,1,0]:c_1,c_2\in\mathbb{R}\right\}.
            \end{equation*}

        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Range}{findran}

            Let \(L:\mathbb{R}^5\to\mathbb{R}^4,\vec{v}\mapsto A\vec{v}\), where
            \begin{equation*}
                A=\begin{bmatrix}
                    8 & 4 & 16 & 32 & 0 \\
                    4 & 2 & 10 & 22 & -4 \\
                    -2 & -1 & -5 & -11 & 7 \\
                    6 & 3 & 15 & 33 & -7
                \end{bmatrix}.
            \end{equation*}
            Find \(\range(L)\).
            \\
            \\
            To find \(\range(L)\), row reduce \(A\) to obtain
            \begin{equation*}
                \begin{bmatrix}
                    1 & \frac{1}{2} & 0 & -2 & 0 \\
                    0 & 0 & 1 & 3 & 0 \\
                    0 & 0 & 0 & 0 & 1 \\
                    0 & 0 & 0 & 0 & 0 
                \end{bmatrix}.
            \end{equation*}
            We see that there are pivots in the first, third, and fifth columns. Thus, 
            \begin{equation*}
                \range(L)=\{c_1[8,4,-2,6]+c_2[16,10,-5,15]+c_3[0,-4,7,-7]:c_1,c_2,c_3\in\mathbb{R}\}.
            \end{equation*}
            
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        The next theorems combine the results of Theorem \ref{thm:findker} and Theorem \ref{thm:findrange} to find 
        \begin{equation*}
            \dim(\ker(L))+\dim(\range(L)).
        \end{equation*}
        But first, consider the following definitions.
        \begin{definition}{\Stop\,\,Nullity of a Linear Transformation}{nullity}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation. Then,
            \begin{equation*}
                \nullity(L)=\dim(\ker(L)).
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,Rank of a Linear Transformation}{ranklintrans}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation. Then,
            \begin{equation*}
                \rank(L)=\dim(\range(L)).
            \end{equation*}
            
        \end{definition}
        \vphantom
        \\
        \\
        Now, consider the following theorems.
        \begin{theorem}{\Stop\,\,The Dimension Theorem (The Rank-Nullity Theorem), in \(\mathbb{R}^n\)}{dimthmrn}

            Let \(A\in\mathcal{M}_{mn}\). Let \(L_A\) be a linear transformation with
            \begin{equation*}
                L_A:\mathbb{R}^n\to\mathbb{R}^m, \vec{v}\mapsto A\vec{v}.
            \end{equation*}
            Then,
            \begin{enumerate}
                \item \(\dim(\range(L))=\rank A\).
                \item \(\dim(\ker(L))=n-\rank A\).
                \item \(\dim(\ker(L))+\dim(\range(L))=n\).
            \end{enumerate}
            These results are verified by Theorems \ref{thm:findker} and \ref{thm:findrange}.
            
        \end{theorem}
        \begin{theorem*}{\Stop\,\,The Dimension Theorem (The Rank-Nullity Theorem)}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation. Then,
            \begin{equation*}
                \dim(\ker(L))+\dim(\range(L))=\dim V.
            \end{equation*}
            
        \end{theorem*}
        \vphantom
        \\
        \\
        We will postpone the proof of the above theorem to a later section; hence, we have omitted the reference number.

    \pagebreak

    \subsection{Injections, Surjections, Bijections, and Isomorphisms}

        We state two theorems about if linear transformations are injective or surjective.
        \begin{theorem}{\Stop\,\,Determining Injectivity and Surjectivity}{detinjsurj}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation. Then,
            \begin{enumerate}
                \item The linear transformation \(L\) is injective if and only if \(\ker(L)=\{\vec{0}_V\}\).
                \begin{proof}
                    Suppose \(L\) is injective and let \(\vec{v}\in\ker(L)\). Now, \(L(\vec{v})=\vec{0}_W\). Similarly, \(L(\vec{0}_V)=\vec{0}_W\), and since \(L\) is injective, \(\vec{v}=\vec{0}_V\). Now, we suppose \(\ker(L)=\{\vec{0}_V\}\). We must show \(L\) is injective. Let \(\vec{v}_1,\vec{v}_2\in V\) with \(L(\vec{v}_1)=L(\vec{v}_2)\). We wish to show \(\vec{v}_1=\vec{v}_2\). Now, we have \(L(\vec{v}_1)-L(\vec{v}_2)=\vec{0}_W\), implying that \(L(\vec{v}_1-\vec{v}_2)=\vec{0}_W\). Thus, \(\vec{v}_1-\vec{v}_2\in\ker(L)\). Since \(\ker(L)=\{\vec{0}_V\}\), \(\vec{v}_1-\vec{v}_2=\vec{0}_V\), and so, \(\vec{v}_1=\vec{v}_2\), as desired.
                \end{proof}
                \item The linear transformation \(L\) is surjective if and only if \(\dim(\range(L))=\dim W\).
                \begin{proof}
                    By definition, \(L\) is surjective if and only if \(\range(L)=W\). Then, since \(\range(L)\) is a subspace of \(W\), \(\range(L)=W\) if and only if \(\dim(\range(L))=\dim W\) by Theorem \ref{thm:dimsubspc}.
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,Determining Injectivity and Surjectivity With Equivalent Dimensions}{detinjsurjequivdim}
            
            Suppose \(V\) and \(W\) are finite dimensional vector spaces with \(\dim V=\dim W\). Let \(L:V\to W\) be a linear transformation. Then, \(L\) is injective if and only if \(L\) is surjective.
            \begin{proof}
                We know \(L\) is injective if and only if \(\ker(L)=\{\vec{0}_V\}\), meaning \(\dim (\ker(L))=0\). By the dimension theorem, \(\dim V=\dim(\range(L))+\dim(\ker(L))=\dim(\range(L))\). Since \(\dim V=\dim W\), \(\dim W=\dim(\range(L))\), meaning \(L\) is surjective, by definition. Conversely, if \(L\) is surjective, \(\dim(\ker(L))=0\), meaning that \(\ker(L)=\{\vec{0}_V\}\), which is equivalent to \(L\) being injective.
            \end{proof}

        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorem about linear independence and spanning, with regards to linear transformations.
        \begin{theorem}{\Stop\,\,Injectivity Implies Linear Independence, Surjectivity Implies Spanning}{injlinindepsurjspan}

            Suppose \(V\) and \(W\) are vector spaces and \(L:V\to W\) is a linear transformation. Then,
            \begin{enumerate}
                \item If \(L\) is injective, and \(T\) is a linearly independent subset of \(V\), \(L(T)\) is linearly independent in \(W\).
                \begin{proof}
                    Suppose that \(L\) is injective, and \(T\) is a linearly independent subset of \(V\). We wish to show that all finite subsets of \(L(T)\) are linearly independent. Suppose \(\{L(\vec{v}_1),\ldots,L(\vec{v}_n)\}\subseteq L(T)\) for \(\vec{v}_1,\ldots,\vec{v}_n\in T\). Suppose
                    \begin{equation*}
                        c_1L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n)=\vec{0}_W,
                    \end{equation*}
                    which implies 
                    \begin{equation*}
                        L(c_1\vec{v}_1+\cdots+c_n\vec{v}_n)=\vec{0}_W
                    \end{equation*}
                    for scalars \(c_1,\ldots,c_n\). Thus, \((c_1\vec{v}_1+\cdots+c_n\vec{v}_n)\in\ker(L)\). Since \(\ker(L)=\{\vec{0}_V\}\) because \(L\) is injective,
                    \begin{equation*}
                        c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0}_V.
                    \end{equation*}
                    Since \(\{\vec{v}_1,\ldots,\vec{v}_n\}\subseteq T\) are linearly independent, \(c_1=\cdots=c_n=0\). Thus, \(\{L(\vec{v}_1),\ldots,L(\vec{v}_n)\}\) is linearly independent as well, meaning that \(L(T)\) is linearly independent, as desired.
                \end{proof}
                \item If \(L\) is surjective and \(S\) spans \(V\), \(L(S)\) spans \(W\).
                \begin{proof}
                    Suppose that \(L\) is surjective and \(S\) spans \(V\). We wish to show that all \(\vec{w}\in W\) can be written as a linear combination of vectors in \(L(T)\). Since \(L\) is surjective, there exists \(\vec{v}\in V\) with \(L(\vec{v})=\vec{w}\). Since \(S\) spans \(V\), we have
                    \begin{equation*}
                        \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
                    \end{equation*}
                    for \(\vec{v}_1,\ldots,\vec{v}_n\in S\) and scalars \(c_1,\ldots,c_n\). Then, 
                    \begin{align*}
                        \vec{w}=L(\vec{v})&=L(c_1\vec{v}_1+\cdots+c_n\vec{v}_n) \\
                        &=c_1L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n).
                    \end{align*}
                    We have written arbitrary \(\vec{w}\in W\) as a linear combination of elements in \(L(S)\), so \(L(S)\) spans \(W\), as desired.
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following definition.
        \begin{definition}{\Stop\,\,Isomorphisms}{isomorphisms}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation; \(L\) an isomorphism from \(V\) to \(W\) if and only if \(L\) is both injective and surjective, or bijective.
            
        \end{definition}
        \begin{definition}{\Stop\,\,Invertible Linear Transformations}{invtrans}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation. Then, \(L\) is an invertible linear transformation if and only if there exists some function \(M:W\to V\) such that
            \begin{equation*}
                (M\circ L)(\vec{v})=\vec{v}
            \end{equation*}
            for all \(\vec{v}\in V\) and
            \begin{equation*}
                (L\circ M)(\vec{w})=\vec{w}
            \end{equation*}
            for all \(\vec{w}\in W\).
            
        \end{definition}
        \begin{theorem}{\Stop\,\,Isomorphism If And Only If Invertible}{isoinv}
            
            Let \(L:V\to W\) be a linear transformation. Then, \(L\) is an isomorphism if and only if \(L\) is an invertible linear transformation. If \(L\) is invertible, \(L^{-1}\) is also a linear transformation.
            \begin{proof}
                The first part of this theorem follows from Theorem \ref{thm:existinv}, as by definition, \(L\) is an isomorphism if and only if \(L\) is bijective. Now, we just seek to show that \(L^{-1}\) is a linear transformation. First, consider \(\vec{w}_1,\vec{w}_2\in W\). Since \(L\) is surjective, we have \(\vec{w}_1=L(\vec{v}_1)\) and \(\vec{w}_2=L(\vec{v}_2)\) for \(\vec{v}_1,\vec{v}_2\in V\). We have
                \begin{align*}
                    L^{-1}(\vec{w}_1+\vec{w}_2)&=L^{-1}(L(\vec{v}_1)+L(\vec{v}_2)) \\
                    &=L^{-1}(L(\vec{v}_1+\vec{v}_2)) \\
                    &=\vec{v}_1+\vec{v}_2 \\
                    &=L^{-1}(\vec{w}_1)+L^{-1}(\vec{w}_2).
                \end{align*}
                Now, for some \(c\in\mathbb{F}\), we have
                \begin{align*}
                    L^{-1}(c\vec{w}_1)&=L^{-1}(cL(\vec{v}_1)) \\
                    &=cL^{-1}(L(\vec{v}_1)) \\
                    &=c\vec{v}_1 \\
                    &=cL^{-1}(\vec{w}_1),
                \end{align*}
                as desired. Note that for the last step of both transitive chains, we used the fact that \(L\) is injective.
            \end{proof}

        \end{theorem}
        \vphantom
        \\
        \\
        The following theorem allows us to determine whether a linear transformation between finite dimensional vector spaces is an isomorphism, and if so, how to find the inverse.
        \begin{theorem}{\Stop\,\,Finding an Inverse, if it Exists}{findinv}

            Suppose \(V\) and \(W\) are nontrivial finite dimensional vector spaces with ordered bases \(B\) and \(C\), respectively. Let \(L:V\to W\) be a linear transformation. Then, \(L\) is an isomorphism if and only if the matrix representation \(A_{BC}\) associated to \(L\) is nonsingular. If \(L\) is indeed an isomorphism, the matrix \(A_{CB}\) for \(L^{-1}\) is \(A_{BC}^{-1}\).
            
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Inverse 1}{findinv1}

            Consider
            \begin{equation*}
                L:\mathbb{R}^2\to\mathbb{R}^2,\begin{bmatrix} x \\ y \end{bmatrix}\mapsto\begin{bmatrix} 3x+y \\ x+y \end{bmatrix}.
            \end{equation*}
            Is \(L\) invertible? If so, find its inverse.
            \\
            \\
            Let \(B\) be an ordered basis of \(\mathbb{R}^2\) with
            \begin{equation*}
                B=\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right).
            \end{equation*}
            Then,
            \begin{equation*}
                A_{BB}=\begin{bmatrix} 3 & 1 \\ 1 & 1 \end{bmatrix}.
            \end{equation*}
            Since this matrix is invertible, as \(\det A_{BB}\neq 0\), the inverse is given by
            \begin{align*}
                L^{-1}:\mathbb{R}^2\to\mathbb{R}^2&,\begin{bmatrix} x \\ y \end{bmatrix}\mapsto A_{BB}^{-1}\begin{bmatrix} x \\ y \end{bmatrix} \\
                &,\begin{bmatrix} x \\ y \end{bmatrix}\mapsto \begin{bmatrix} \frac{1}{2} & -\frac{1}{2} \\ -\frac{1}{2} & \frac{3}{2} \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}.
            \end{align*}
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Inverse 2}{findinv2}

            Consider
            \begin{equation*}
                L:\mathbb{R}^3\to\mathbb{R}^3,\begin{bmatrix} x \\ y \\ z \end{bmatrix}\mapsto A\begin{bmatrix} x \\ y \\ z \end{bmatrix}.
            \end{equation*}
            where
            \begin{equation*}
                A=\begin{bmatrix}
                    1 & 0 & 3 \\
                    0 & 1 & 3 \\
                    0 & 0 & 1
                \end{bmatrix}.
            \end{equation*}
            Is \(L\) invertible? If so, find its inverse.
            \\
            \\
            Since this matrix is invertible, as \(\det A_{BB}\neq 0\), the inverse is given by
            \begin{align*}
                L^{-1}:\mathbb{R}^2\to\mathbb{R}^2&,\begin{bmatrix} x \\ y \\ z \end{bmatrix}\mapsto A^{-1}\begin{bmatrix} x \\ y  \\ z \end{bmatrix} \\
                &,\begin{bmatrix} x \\ y \\ z \end{bmatrix}\mapsto \begin{bmatrix} 1 & 0 & -3 \\ 0 & 1 & -3 \\ 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix}.
            \end{align*}
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        We end with an important, yet unsurprising, theorem.
        \begin{theorem}{\Stop\,\,Isomorphisms Preserve Linear Independence and Span}{isopreslinindepspan}
            
            Let \(V\) and \(W\) be vector spaces, and let \(L:V\to W\) be an isomorphism.
            \begin{enumerate}
                \item If \(T\) is a linearly independent subset of \(V\), \(L(T)\) is linearly independent in \(W\).
                \begin{proof}
                    Because \(L\) is an isomorphism, \(L\) is injective. Therefore, \(L(T)\) is linearly independent in \(W\) by the first part of Theorem \ref{thm:injlinindepsurjspan}.
                \end{proof}
                \item If \(S\) spans \(V\), \(L(S)\) spans \(W\).
                \begin{proof}
                    Because \(L\) is an isomorphism, \(L\) is surjective. Therefore, \(L(S)\) spans \(W\) by the second part of Theorem \ref{thm:injlinindepsurjspan}.
                \end{proof}
                \item If \(B\) is a basis of \(V\), \(L(B)\) is a basis for \(W\).
                \begin{proof}
                    If \(B\) is a basis for \(V\), \(B\) is linearly independent, so \(L(B)\) is linearly independent. We also have that \(\Span(B)=V\), so \(\Span(L(B))=W\). Since \(L(B)\) is linearly independent and spans \(W\), \(L(B)\) is a basis for \(W\).
                \end{proof}
            \end{enumerate}

        \end{theorem}

        \pagebreak

\section{Lecture 34, November 18, 2022}

    \subsection{Isomorphic Vector Spaces}

        We will now define the notion of equivalence of vector spaces.
        \begin{definition}{\Stop\,\,Isomorphic Vector Spaces}{isovec}

            Suppose \(V\) and \(W\) are vector spaces. Then, \(V\) is isomorphic to \(W\), that is, \(V\cong W\), if and only if there exists some linear transformation \(L:V\to W\) that is an isomorphism.
            
        \end{definition}
        \begin{theorem}{\Stop\,\,\(\cong\) is an Equivalence Relation}{isoequivrel}

            Suppose \(V\) and \(W\) are vector spaces. Then, \(\cong\) is an equivalence relation.
            \begin{proof}
                We must show that \(\cong\) is reflexive, symmetric and transitive. Consider the following.
                \begin{enumerate}
                    \item We wish to show \(V\cong V\). Consider the linear transformation \(i:V\to V,\vec{v}\mapsto\vec{v}\), the identity linear operator, defined by Definition \ref{def:idlinop}. We wish to show that \(i:V\to V\) is an isomorphism. We need only show that \(i\) is injective. Since \(i(\vec{0}_V)=\vec{0}_V\), \(\{\vec{0}_V\}\subseteq\ker(i)\).
                    \item Suppose \(V\cong W\) via \(L:V\to W\), and we must show that \(L^{-1}:W\to V\) is an isomorphism from \(W\) to \(V\).
                    \item Suppose \(V_1 \cong V_2\) via \(L_1:V_1\to V_2\) and \(V_2\cong V_3\) via \(L_2:V_2\to V_3\). Show that \(L_2\circ L_1:V_1\to V_3\) is an isomorphism.
                \end{enumerate}
                \DOTHISLATER
            \end{proof}

        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        We will now restate the Dimension Theorem, or the Rank-Nullity Theorem, with an accompanying proof.
        \begin{theorem}{\Stop\,\,The Dimension Theorem (The Rank-Nullity Theorem)}{dimensionthm}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces and \(L:V\to W\) is a linear transformation. Then,
            \begin{equation*}
                \dim(\ker(L))+\dim(\range(L))=\dim V.
            \end{equation*}
            \begin{proof}
                Let \(B_{\ker(L)}=\{\vec{u}_1,\ldots,\vec{u}_m\}\) be a basis for \(\ker(L)\). We can extend \(B_{\ker(L)}\) to form a basis for \(V\), \(B_V\), with \(B_V=\{\vec{u}_1,\ldots,\vec{u}_m,\vec{v}_1,\ldots,\vec{v}_n\}\). Note that \(\dim(\ker(L))=m\) and \(\dim V=m+n\). Then, for some \(\vec{v}\in V\), we have
                \begin{equation*}
                    \vec{v}=c_1\vec{u}_1+\cdots+c_m\vec{u}_m+c_{m+1}\vec{v}_1+\cdots+c_n\vec{v}_n
                \end{equation*}
                for \(c_1,\ldots,c_m,c_{m+1},\ldots,c_n\in\mathbb{F}\). If we apply \(L\) to both sides, we have
                \begin{align*}
                    L(\vec{v})&=L(c_1\vec{u}_1+\cdots+c_m\vec{u}_m+c_{m+1}\vec{v}_1+\cdots+c_n\vec{v}_n) \\
                    &=c_1L(\vec{u}_1)+\cdots+c_mL(\vec{u}_m)+c_{m+1}L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n).
                \end{align*}
                Since \(B_{\ker(L)}\) is a basis of \(\ker(L)\), \(L(\vec{u}_1)=\cdots=L(\vec{u_m})=\vec{0}_W\) and we have
                \begin{equation*}
                    L(\vec{v})=c_{m+1}L(\vec{v}_1)+\cdots+c_nL(\vec{v}_n).
                \end{equation*}
                Therefore, since we have written arbitrary \(L(\vec{v})\) as a linear combination of \(L(\vec{v}_1),\ldots,L(\vec{v}_n)\), we have 
                \begin{equation*}
                    \range(L)=\Span(\{L(\vec{v}_1),\ldots,L(\vec{v}_n)\}).
                \end{equation*}
                Consider
                \begin{align*}
                    \vec{0}_W&=b_1L(\vec{v}_1)+\cdots+b_nL(\vec{v}_n) \\
                    &=L(b_1\vec{v}_1+\cdots+b_n\vec{v}_n)
                \end{align*}
                for \(b_1,\ldots,b_n\in\mathbb{F}\). We see that \(b_1\vec{v}_1+\cdots+b_n\vec{v}_n\in\ker(L)\), so
                \begin{equation*}
                    b_1\vec{v}_1+\cdots+b_n\vec{v}_n=d_1\vec{u}_1+\cdots+d_m\vec{u}_m.
                \end{equation*}
                If we subtract the right hand side from both sides, we have
                \begin{equation*}
                    b_1\vec{v}_1+\cdots+b_n\vec{v}_n+(-d_1\vec{u}_1)+\cdots+(-d_m)\vec{u}_m=\vec{0}_V
                \end{equation*}
                Since \(B_V\) is a basis for \(V\), \(B_V\) is linearly independent, so \(b_1=\cdots=b_n=d_1=\cdots=d_m=0\). Since \(b_1=\cdots=b_n=0\), \(B_{\range(L)}:=\{L(\vec{v}_1),\ldots,L(\vec{v}_n)\}\) is linearly independent. Since \(B_{\range(L)}\) both spans \(\range(L)\) and is linearly independent, \(B_{\range(L)}\) is a basis for \(\range(L)\). We see that \(\dim(\range(L))=n\), so
                \begin{align*}
                    \dim V&=m+n \\
                    &=\dim(\ker(L))+\dim(\range(L)),
                \end{align*}
                as desired.

            \end{proof}

        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following important theorems.
        \begin{theorem}{\Stop\,\,Isomorphism Implies Equivalent Dimension}{equivdim}

            Suppose \(V\) and \(W\) are finite dimensional vector spaces. Then, \(V\cong W\) if and only if \(\dim V=\dim W\).
            \begin{proof}
                Suppose \(V\cong W\). Then, there exists some linear transformation \(L:V\to W\) where \(L\) is an isomorphism. Therefore, we have that \(\dim(\range(L))=\dim W\). We also have \(\ker(L)=\{\vec{0}_V\}\), so \(\dim(\ker(L))=0\). Therefore, by Theorem \ref{thm:dimensionthm},
                \begin{align*}
                    \dim V&=\dim(\ker(L))+\dim(\range(L)) \\
                    &=0+\dim W \\
                    &=\dim W.
                \end{align*}
                Now, suppose \(\dim V=\dim W\). Let \(B_V=\{\vec{v}_1,\ldots,\vec{v}_n\}\) be a basis for \(V\) and \(B_W=\{\vec{w}_1,\ldots,\vec{w}_n\}\) be a basis for \(W\). Let 
                \begin{equation*}
                    L:V\to W, c_1\vec{v}_1+\cdots+c_n\vec{v}_n\mapsto c_1\vec{w}_1+\cdots+c_n\vec{w}_n
                \end{equation*}
                for \(c_1,\ldots,c_n\in\mathbb{F}\). Consider arbitrary \(\vec{w}\in W\). We wish to find some \(\vec{v}\in V\) such that \(L(\vec{v})=\vec{w}\) to show that \(L\) is surjective. Under the supposition \(\dim V=\dim W\), \(L\) is an isomorphism if and only if \(L\) is surjective by Theorem \ref{thm:detinjsurjequivdim}. We have
                \begin{equation*}
                    \vec{w}=c_1\vec{w}_1+\cdots+c_n\vec{w}_n.
                \end{equation*}
                By the definition of \(L\), we know
                \begin{equation*}
                    L(c_1\vec{v}_1+\cdots+c_n\vec{v}_n)=c_1\vec{w}_1+\cdots+c_n\vec{w}_n,
                \end{equation*}
                so \(L\) is surjective and is an isomorphism, so \(V\cong W\), as desired.
            \end{proof}
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,All \(n\)-Dimensional Vector Spaces are Isomorphic to \(\mathbb{R}^n\)}{allisorn}
            
            Suppose \(V\) is a finite dimensional vector space with \(\dim V=n\). Then, \(V\cong \mathbb{R}^n\).
            \begin{proof}
                We have that \(\dim V=n\), and we know that \(\dim\mathbb{R}^n=n\), so by Theorem \ref{thm:equivdim}, \(V\cong\mathbb{R}^n\).
            \end{proof}

        \end{theorem}

      %  \begin{example}{\Stop\,\,Find Inverse 1}{findinv1}
%
      %      Consider
      %      \begin{equation*}
       %         L:\mathbb{R}^2\to\mathbb{R}^2,\begin{bmatrix} x \\ y \end{bmatrix}\mapsto\begin{bmatrix} 3x+y \\ x+y \end{bmatrix}.
       %     \end{equation*}
       %     Is \(L\) invertible, if so, find its inverse.
       %     \\
       %     \\
        %    Let \(B\) be an ordered basis of \(\mathbb{R}^2\) with
       %     \begin{equation*}
              % B=\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right).
         %   \end{equation*}
          %  Then,
           % \begin{equation*}
             %   [L(\vec{v})]_{BB}=\begin{bmatrix} 3 & 1 \\ 1 & 1 \end{bmatrix}.
            %\end{equation*}
            %Since this matrix is invertible, as \(\det [L(\vec{v})]_{BB} \neq 0\), the inverse is given by
           % \begin{equation*}
            %    L^{-1}:\mathbb{R}^2\to\mathbb{R}^2,\begin{bmatrix} x \\ y \end{bmatrix}\mapsto([L(\vec{v})]_{BB})^{-1}\begin{bmatrix} x \\ y \end{bmatrix}.
           % \end{equation*}
        %\end{example}

    \pagebreak
    \subsection{Diagonalization of Linear Operators}

        Consider the following definitions, similar to the notions discussed in Chapter \ref{chapter:deteigen}, but in the context of linear transformations.
        \begin{definition}{\Stop\,\,Eigenvalues and Eigenvectors}{eigenvaluesandvectorslintrans}

            Suppose \(V\) is a vector space. Let \(L:V\to V\) be a linear operator. A scalar \(\lambda\) is an eigenvalue of \(L\) if and only if there exists \(\vec{v}\in V\), where \(\vec{v}\neq\vec{0}_V\), such that \(L(\vec{v})=\lambda \vec{v}\). If \(\lambda\) is an eigenvalue of \(L\), \(\vec{v}\) is an eigenvector of \(L\) with eigenvalue \(\lambda\). 

        \end{definition}
        \begin{definition}{\Stop\,\,Eigenspace}{eigenspacelintrans}

            Suppose \(V\) is a vector space. Let \(L:V\to V\) be a linear operator. The eigenspace of a given eigenvalue \(\lambda\) is
            \begin{equation*}
                E_\lambda=\{\vec{v}\in V:L(\vec{v})=\lambda \vec{v}\}\cup\{\vec{0}_V\}.
            \end{equation*}
            Note that ``\(\cup\{\vec{0}_V\}\)'' is somewhat redundant, as it will always satisfy the equation. However, the zero vector is never an eigenvector.
        
        \end{definition}
        \vphantom
        \\
        \\
        Our goal is to find all the eigenvalues and eigenspaces of \(L\). Consider the following theorem.
        \begin{theorem}{\Stop\,\,Finding Eigenvectors and Eigenvalues}{findeigenvs}
    
            Let \(L\) be a linear operator on a nontrivial finite dimensional vector space \(V\). Suppose \(A\in\mathcal{M}_{nn}\) is the matrix representation of \(L\) with respect to some ordered basis of \(V\). The scalar \(\lambda\) is an eigenvalue of \(L\) if and only if \(\lambda\) satisfies
            \begin{equation*}
                \det(A-\lambda I_n)=0.
            \end{equation*}
            
        \end{theorem}
        \vphantom
        \\
        \\
        We will now define what it means for a linear operator to be diagonizable, while also providing a theorem to provide an equivalent condition.
        \begin{definition}{\Stop\,\,Diagonalizability of a Linear Operator}{diaglintrans}

            A linear operator \(L\) on a finite dimensional vector space is diagonizable if and only if the matrix representation of \(L\) with respect to some ordered basis for \(V\) is a diagonal matrix.
            
        \end{definition}
        \pagebreak
        \begin{theorem}{\Stop\,\,Diagonalizability of a Linear Operator}{diaglintrans}

            Suppose \(L\) is a linear operator on an \(n\)-dimensional vector space \(V\). Then, \(L\) is diagonalizable if and only if there exists a set of \(n\) linearly independent eigenvectors for \(L\).
            \begin{proof}
                Suppose \(L\) is diagonalizable. Then, there exists some ordered basis \(B=(\vec{v}_1,\ldots,\vec{v}_n)\) for \(V\) such that the matrix representation for \(L\) is a diagonal matrix \(D\). Because \(B\) is a basis, \(B\) is linearly independent. We wish to show that each \(\vec{v}_i\in B\), with \(1\leq i\leq n\), is an eigenvector corresponding to some eigenvalue for \(L\). Let \(d_{ii}\) be the \((i,i)\) element of \(D\). For each \(\vec{v}_i\), we have
                \begin{equation*}
                    [L(\vec{v_i})]_B=D[\vec{v_i}]_B=D\vec{e}_i=d_{ii}\vec{e}_i=d_{ii}[\vec{v_i}]_B=[d_{ii}\vec{v}_i]_B.
                \end{equation*}
                We have shown that \(L(\vec{v}_i)=d_{ii}\vec{v}_i\). That is, we have shown that each \(\vec{v}_i\in B\) is an eigenvector of \(L\) with eigenvalue \(d_{ii}\). Thus, \(B\) is a set of \(n\) linearly independent eigenvectors for \(L\). Conversely, suppose \(B=\{\vec{w}_1,\ldots,\vec{w}_n\}\) is a set of \(n\) linearly independent eigenvectors for \(L\), corresponding to eigenvalues \(\lambda_1,\ldots,\lambda_n\). These eigenvalues need not be distinct. We also note that \(B\) is a basis for \(V\), by Theorem \ref{thm:lindim2}. We wish to show that the matrix representation for \(L\), with respect to \(B\) is diagonal. The \(i\)th column for \(A\) is given by
                \begin{equation*}
                    [L(\vec{w}_i)]_B=[\lambda_i\vec{w}_i]_B=\lambda_i[\vec{w}_i]_B=\lambda_i\vec{e}_i.
                \end{equation*}
                Thus, \(A\) is diagonal, and \(L\) is diagonalizable, as desired.
            \end{proof}

        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Note that Theorem \ref{thm:diaglintrans} requires that we find ``enough'' linearly independent eigenvectors. We now provide a theorem guaranteeing the linear independence of eigenvectors in certain conditions.
        \begin{theorem}{\Stop\,\,Eigenvectors With Distinct Eigenvalues are Linearly Independent}{eigenvecsdisteigenvalslinindep}

            Suppose \(L\) is a linear operator on \(V\). Let \(\lambda_1,\ldots,\lambda_n\) be distinct eigenvalues for \(L\). If \(\vec{v}_1,\ldots,\vec{v}_n\) are eigenvectors for \(L\) corresponding to \(\lambda_1,\ldots,\lambda_n\), respectively, the set \(\{\vec{v}_1,\ldots,\vec{v}_n\}\) is linearly independent.
            \begin{proof}
                
                We proceed by induction on \(n\). For \(n=1\), any eigenvector \(\vec{v}_1\), for any eigenvalue, by definition is nonzero, so \(\{\vec{v}_1\}\) is linearly independent. Suppose that for all \(k\in\mathbb{N}\), the proposition holds. Now, for distinct eigenvalues \(\lambda_1,\ldots,\lambda_{k+1}\). We wish to show that \(\{\vec{v}_1,\ldots,\vec{v}_{k+1}\}\) is linearly independent. Consider
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_{k+1}\vec{v}_{k+1}=\vec{0}_V
                \end{equation*}
                for scalars \(c_1,\ldots,c_n\). If we apply \(L\) to both sides, we have
                \begin{align*}
                    L(\vec{0}_V)&=L(c_1\vec{v}_1+\cdots+c_{k+1}\vec{v}_{k+1}) \\
                    &=c_1L(\vec{v}_1)+\cdots+c_{k+1}L(\vec{v}_{k+1})
                \end{align*}
                which implies
                \begin{equation*}
                    c_1\lambda_1\vec{v}_1+\cdots+c_{k+1}\lambda_{k+1}\vec{v}_{k+1}=\vec{0}_V.
                \end{equation*}
                If we multiply \(c_1\vec{v}_1+\cdots+c_{k+1}\vec{v}_{k+1}=\vec{0}_V\) by \(\lambda_{k+1}\), we have
                \begin{equation*}
                    c_1\lambda_{k+1}\vec{v}_1+\cdots+c_{k+1}\lambda_{k+1}\vec{v}_{k+1}=\vec{0}_V=c_1\lambda_1\vec{v}_1+\cdots+c_{k+1}\lambda_{k+1}\vec{v}_{k+1}=\vec{0}_V.
                \end{equation*}
                which we can rewrite as
                \begin{equation*}
                    c_1(\lambda_1-\lambda_{k+1})\vec{v}_1+\cdots+c_k(\lambda_k-\lambda_{k+1})\vec{v}_k=\vec{0}_V
                \end{equation*}
                By the inductive hypothesis,
                \begin{equation*}
                    c_1(\lambda_1-\lambda_{k+1})=\cdots=c_k(\lambda_k-\lambda_{k+1})=0.
                \end{equation*}
                Since \(\lambda_1,\ldots,\lambda_{k+1}\) are distinct, none of the differences in the above equation can be zero, so \(c_1=\cdots=c_k=0\). Thus, for
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_{k+1}\vec{v}_{k+1}=\vec{0}_V,
                \end{equation*}
                we have \(c_{k+1}\vec{v}_{k+1}=\vec{0}_V\). Since \(\vec{v}_{k+1}\neq\vec{0}_V\), we have \(c_{k+1}=0\), as desired.

            \end{proof}
            
        \end{theorem}
        \vphantom
        \\
        \\
        Note that Theorem \ref{thm:eigenvecsdisteigenvalslinindep} provides that if \(L\) is linear operator on an \(n\)-dimensional vector space and \(L\) has \(n\) distinct eigenvalues, \(L\) is diagonalizable. The converse is false.
        \pagebreak
        \\
        \\
        Consider the following theorems.
        \begin{theorem}{\Stop\,\,Union and Intersection of Bases for Eigenspaces}{unioninterbaseseigenspc}

            Suppose \(V\) is a finite dimensional vector space. Let \(L:V\to V\) be a linear operator, and let \(B_1,\ldots,B_k\) be bases for eigenspaces \(E_{\lambda_1},\ldots,E_{\lambda_k}\) for \(L\), where \(\lambda_1,\ldots,\lambda_k\) are distinct eigenvalues for \(L\). Then, \(B_i\cap B_j=\emptyset\) for \(1\leq i<j\leq k\), and \(B_1\cup \cdots\cup B_k\) is a linearly independent subset of \(V\).
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,The Process of Diagonalization for Linear Operators}{procdiaglinops}

            Let \(V\) be an \(n\)-dimensional vector space and let \(L:V\to V\) be a linear operator. Consider the following steps.
            \begin{enumerate}
                \item Find a basis, \(C\), for \(V\). Then, find the matrix representation \(A\) of \(L\) with respect to \(C\).
                \item Apply the steps of Theorem \ref{thm:diagonalization} on \(A\) to find the eigenvalues \(\lambda_1,\ldots,\lambda_k\) and a basis in \(\mathbb{R}^n\) for each eigenspace \(E_\lambda\). If \(\left|\bigcup_{i}E_{\lambda_i}\right|<n\), \(L\) is not diagonalizable. Otherwise, let \(Z=(\vec{w}_1,\ldots,\vec{w}_n)=\bigcup_i E_{\lambda_i}\) be an ordered basis for \(\mathbb{R}^n\).
                \item Find an ordered basis \(B=(\vec{v}_1,\ldots,\vec{v}_n)\) of \(V\) such that \([\vec{v}_i]_C=\vec{w}_i\).
                \item Form \(D\) by finding the matrix representation for \(L\) with respect to \(B\).
                \item If needed, form \(P=\begin{bmatrix} [\vec{v}_1]_C & \cdots & [\vec{v}_n]_C \end{bmatrix}=\begin{bmatrix} \vec{w}_1 & \cdots & \vec{w}_n \end{bmatrix}\).
            \end{enumerate}

        \end{theorem}