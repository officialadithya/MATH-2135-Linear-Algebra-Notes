\section{Lecture 20: October 7, 2022}

    \subsection{The Process of Abstraction}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Vector Spaces}{vcspc}

            Let \(\mathbb{F}\) be a field of scalars. For now, \(\mathbb{F}=\mathbb{R}\vee\mathbb{C}\). A vector space \(V\) over \(\mathbb{F}\) is a set with two operations: 
            \begin{enumerate}
                \item Vector Addition: \(V\times V\to V, (\vec{v},\vec{w})\mapsto \vec{v}+\vec{w}\).
                \item Scalar Multiplication: \(\mathbb{F}\times V\to V, (c,\vec{v})\mapsto c\vec{v}\).
            \end{enumerate}
            \vphantom
            \\
            \\
            The following axioms must hold for each \(\vec{u},\vec{v},\vec{w}\in V\) and \(c_1,c_2\in\mathbb{F}\).
            \begin{enumerate}
                \item \(\vec{u}+\vec{v}=\vec{v}+\vec{u}\)
                \item \(\vec{u}+(\vec{v}+\vec{w})=(\vec{u}+\vec{v})+\vec{w}\).
                \item \(\exists \vec{0}\in V, \forall \vec{v}\in V, \vec{0}+\vec{v}=\vec{v}\)
                \item \(\forall \vec{v}\in V, \exists! (-\vec{v})\in V, \vec{v}+(-\vec{v})=\vec{0}\).
                \item \(c_1(\vec{u}+\vec{v})=c_1\vec{u}+c_1\vec{v}\).
                \item \((c_1+c_2)\vec{u}=c_1\vec{u}+c_2\vec{u}\).
                \item \((c_1c_2)\vec{u}=c_1(c_2\vec{u})\).
                \item \(1\vec{u}=\vec{u}\).
            \end{enumerate}
            
        \end{definition}
        \vphantom
        \\
        \\
        We remark that \(0\vec{v}=\vec{0}\) is \textit{not} an axiom of a vector space; we must \textit{prove} that it holds. Now, we will justify our use of abstraction.
        \begin{enumerate}
            \item The set \(\mathbb{R}^n\) is a vector space.
            \item The set \(\mathbb{C}^n\) is a vector space.
            \item The set \(\{\vec{0}\}\), with \(\vec{0}+\vec{0}+\vec{0}\) and \(c\vec{0}=\vec{0}\), is a vector space.
            \item The set \(\mathcal{M}_{mn}\) is a vector space.
            \item Let \(S\) be a nonempty set and \(F(S)=\{f:S\to\mathbb{R}\}\) with \((f+g)(s)=f(s)+g(s)\) and \((cf)(s)=cf(s)\). The set \(F(S)\) is a vector space.
            \item The set \(\{a_nx^n+\cdots+a_0x^0:a_0,\ldots a_n\in\mathbb{R}\}\) is a vector space.
            \item The set \(\{a_nx^n+\cdots+a_0x^0:a_0,\ldots a_n\in\mathbb{R}, \text{degree is less than or equal to \(n\)}\}\) is a vector space.
        \end{enumerate}
        \vphantom
        \\
        \\
        We note that all the above examples need \textit{proof}. We remark that, for now, we will primarily consider vector spaces over \(\mathbb{R}\). We will emphasize when we consider vector spaces over \(\mathbb{C}\).  Consider the following examples of sets that are not vector spaces.
        \begin{enumerate}
            \item The set \(\mathbb{R}^+=\{x\in\mathbb{R}:x\geq0\}\), with usual addition and usual multiplication in \(\mathbb{R}\), is not a vector space. The set does not satisfy the closure axiom for scalar multiplication; \((-1)\cdot1=-1\nin\mathbb{R}^+\).
            \item The set \(S=\{f:\mathbb{R}\to\mathbb{R}:f(0)=1\}\), with \((f+g)(x)=f(x)+g(x)\) and \((cf)(x)=cf(x)\), is not a vector space. The set does not satisfy the closure axiom for vector addition; \((f+g)(0)=f(0)+g(0)=2\), so \((f+g)(x)\nin S\).
        \end{enumerate}

\pagebreak

\section{Lecture 21: October 14, 2022}

    \subsection{Derived Properties of Vector Spaces}

        Consider the following theorems. While they may seem obvious, they require proof by the vector space axioms, provided in Definition \ref{def:vcspc}.
        \begin{theorem}{\Stop\,\,Derived Property of Vector Space 1}{derpropvcspc1}

            Suppose \(V\) is a vector space, \(\vec{v}\in V\), and \(c\in\mathbb{F}\). Then,
            \begin{equation*}
                c\vec{0}=\vec{0}.
            \end{equation*}
            \begin{proof}
                We have that
                \begin{align*}
                    c\vec{0}&=c\vec{0}+\vec{0} &\text{ by axiom \(3\),} \\
                    &=c\vec{0}+(c\vec{0}+(-(c\vec{0}))) &\text{ by axiom \(4\),} \\
                    &=(c\vec{0}+c\vec{0})+(-(c\vec{0})) &\text{ by axiom \(2\),} \\
                    &=c(\vec{0}+\vec{0})+(-(c\vec{0})) &\text{ by axiom \(5\),} \\
                    &=c\vec{0}+(-(c\vec{0})) &\text{ by axiom \(3\),} \\
                    &=\vec{0} &\text{ by axiom \(4\),} 
                \end{align*}
                as desired.
            \end{proof}
        \end{theorem}
        \begin{theorem}{\Stop\,\,Derived Property of Vector Space 2}{derpropvcspc2}

            Suppose \(V\) is a vector space, \(\vec{v}\in V\), and \(c\in\mathbb{F}\). Then,
            \begin{equation*}
                0\vec{v}=\vec{0}.
            \end{equation*}
            \begin{proof}
                We have that
                \begin{align*}
                    0\vec{v}&=0\vec{v}+\vec{0} &\text{ by axiom \(3\),} \\
                    &=0\vec{v}+(0\vec{v}+(-(0\vec{v}))) &\text{ by axiom \(4\),} \\
                    &=(0\vec{v}+0\vec{v})+(-(0\vec{v})) &\text{ by axiom \(2\),} \\
                    &=(0+0)\vec{v}+(-(0\vec{v})) &\text{ by axiom \(6\),} \\
                    &=0\vec{v}+(-(0\vec{v})) \\
                    &=\vec{0} &\text{ by axiom \(4\),}
                \end{align*}
                as desired.
            \end{proof}
        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,Derived Property of Vector Space 3}{derpropvcspc3}

            Suppose \(V\) is a vector space, \(\vec{v}\in V\), and \(c\in\mathbb{F}\). Then,
            \begin{equation*}
                (-1)\vec{v}=-\vec{v}.
            \end{equation*}
            \begin{proof}
                We have that
                \begin{align*}
                    \vec{v}+(-1)\vec{v}&=1\vec{v}+(-1)\vec{v} &\text{ by axiom \(8\),} \\
                    &=(1+(-1))\vec{v} &\text{ by axiom \(6\),} \\
                    &=0\vec{v} \\
                    &=\vec{0} &\text{ by Theorem \ref{thm:derpropvcspc2},}
                \end{align*}
                which implies \((-1)\vec{v}=-\vec{v}\), the additive inverse of \(\vec{v}\).
            \end{proof}
        \end{theorem}
        \begin{theorem}{\Stop\,\,Derived Property of Vector Space 4}{derpropvcspc4}

            Suppose \(V\) is a vector space, \(\vec{v}\in V\), and \(c\in\mathbb{F}\). Then,
            \begin{equation*}
                c\vec{v}=\vec{0}\iff c=0\vee \vec{v}=\vec{0}
            \end{equation*}
            \begin{proof}
                The statement \(c=0\vee\vec{v}=\vec{0}\implies c\vec{v}=\vec{0}\) is governed by the first two derived results. Then, to show that \(c\vec{v}=\vec{0}\implies c=0\vee\vec{v}=\vec{0}\), we suppose that \(c\neq0\) and wish to show that \(\vec{v}=\vec{0}\). We have
                \begin{equation*}
                    c\vec{v}=\vec{0}
                \end{equation*}
                with \(c\neq0\). Then,
                \begin{equation*}
                    \left(\frac{1}{c}\right)(c\vec{v})=\frac{1}{c}\vec{0}=\vec{0},
                \end{equation*}
                by Theorem \ref{thm:derpropvcspc1}. But, by axiom \(7\), 
                \begin{equation*}
                    \frac{1}{c}(c\vec{v})=\left(\frac{1}{c}c\vec{v}\right)=\vec{v},
                \end{equation*}
                which implies \(\vec{v}=\vec{0}\).
            \end{proof}
        \end{theorem}

\pagebreak

\section{Lecture 22: October 17, 2022}

    \subsection{Subspaces}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Subspaces}{subspc}

            Suppose \(V\) is a vector space and the set \(W\subseteq V\). Then, \(W\) is a subspace of \(V\) if and only if \(W\) is a vector space with the same operations as \(V\).
            
        \end{definition}
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Showing a Vector Space is a Subspace}{showsubspc}

            Suppose \(V\) is a vector space and the set \(W\) is a subset of \(V\). Then, \(W\) is a subspace of \(V\) if and only if
            \begin{enumerate}
                \item \(W\neq\emptyset\).
                \item \(\vec{w}_1,\vec{w}_2\in W\implies \vec{w}_1+\vec{w}_2\in W\).
                \item \(\vec{w}\in W\implies c\vec{w}\in W, c\in\mathbb{F}\).
            \end{enumerate}
            \begin{proof}
                We know that \(W\neq\emptyset\), because \(\vec{0}\in W\). Next, \(\vec{w}_1,\vec{w}_2\in W\implies \vec{w}_1+\vec{w}_2\in W\) by the closure property of addition of a vector space \(W\). Then, \(\vec{w}\in W\implies c\vec{w}\in W, c\in\mathbb{F}\) by the closure property of scalar multiplication of a vector space \(W\). Now, we must show that all vector space axioms hold for \(W\). The closure properties are given by conditions \(2\) and \(3\). For vector space axiom \(1\), we take \(\vec{w}_1,\vec{w}_2\in W\). Then,
                \begin{align*}
                    \vec{w}_1+_W\vec{w}_2&=\vec{w}_1+_V\vec{w}_2 \\
                    &=\vec{w}_2+_V\vec{w}_1 \\
                    &=\vec{w}_2+_W\vec{w}_1.
                \end{align*}
                For vector space axiom \(3\), we take \(\vec{w}\in W\) and by the closure property of scalar multiplication, \((-1)\vec{w}\in W\). By the closure property of vector addition \(\vec{w}+(-1)\vec{w}=\vec{0}\), meaning \(\vec{0}\in W\). Moreover, for any \(\vec{w}\in W\), \(\vec{w}+\vec{0}=\vec{0}+\vec{w}=\vec{w}\). For vector space axiom \(4\), \(-1\vec{w}\) is the additive inverse of \(\vec{w}\), and by the closure property, \((-1)\vec{w}\in W\). We must show the rest, and leave this as an exercise to the reader.
            \end{proof}
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\,Is it a Subspace? 1}{issub1}

            Let \(V=\mathbb{R}^2\) and let 
            \begin{equation*}
                W=\{[x,0]:x\in\mathbb{R}\}\subseteq \mathbb{R}^2.
            \end{equation*}
            Is \(W\) a subspace of \(V\)?.
            \\
            \\
            We see that \(W\neq\emptyset\), as \([0,0]\in W\). Then, if \([x_1,0],[x_2,0]\in W\),
            \begin{equation*}
                [x_1,0]+[x_2,0]=[x_1+x_2,0]\in W.
            \end{equation*}
            Then, if \(c\in\mathbb{R}\) and \([x,0]\in W\),
            \begin{equation*}
                c[x,0]=[cx,0]\in W.
            \end{equation*}
            Because \(W\neq\emptyset\) is closed under vector addition and scalar multiplication, \(W\) is a subspace of \(V\).

        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Subspace? 2}{issub2}

            Let \(V=\mathcal{M}_{nn}\) and let
            \begin{equation*}
                W=\mathcal{D}_n\subseteq\mathcal{U}_n\subseteq\mathcal{M}_{nn},
            \end{equation*}
            where \(\mathcal{D}_n\) is the set of all \(n\times n\) diagonal matrices and \(\mathcal{U}_n\) is the set of all \(n\times n\) upper triangular matrices. Is \(W\) a subspace of \(V\)?.
            \\
            \\
            We see that \(W\neq\emptyset\), since \(\begin{bmatrix} 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0 \end{bmatrix}\in W\). Then,
            for \(A,B\in\mathcal{D}_n\), 
            \begin{align*}
                (A+B)_{ij}&=A_{ij}+B_{ij} \\
                &=\begin{cases}
                    A_{ii}+B_{ii} & i=j \\
                    0 & i \neq j
                \end{cases} \\
                &\in\mathcal{D}_n.
            \end{align*}
            Then, for some \(c\in\mathbb{R}\),
            \begin{align*}
                (cA)_{ij}&=cA_{ij} \\
                &=\begin{cases}
                    cA_{ii} & i=j \\
                    0 & i \neq j
                \end{cases} \\
                &\in\mathcal{D}_n.
            \end{align*}
            Therefore, \(\mathcal{D}_n\) is a subspace of \(\mathcal{M}_{nn}\).
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Subspace? 3}{issub3}

            Let \(V=\mathcal{M}_{nn}\) and let
            \begin{equation*}
                W=\mathcal{U}_n\subseteq\mathcal{M}_{nn},
            \end{equation*}
            where \(\mathcal{U}_n\) is the set of all \(n\times n\) upper triangular matrices. Is \(W\) a subspace of \(V\)?.
            \\
            \\
            We see that \(W\neq\emptyset\), since \(\begin{bmatrix} 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0 \end{bmatrix}\in W\). Then,
            for \(A,B\in\mathcal{U}_n\), 
            \begin{align*}
                (A+B)_{ij}&=A_{ij}+B_{ij} \\
                &=\begin{cases}
                    A_{ij}+B_{ij} & i\leq j \\
                    0 & i > j
                \end{cases} \\
                &\in\mathcal{U}_n.
            \end{align*}
            Then, for some \(c\in\mathbb{R}\),
            \begin{align*}
                (cA)_{ij}&=cA_{ij} \\
                &=\begin{cases}
                    cA_{ij} & i\leq j \\
                    0 & i > j
                \end{cases} \\
                &\in\mathcal{U}_n.
            \end{align*}
            Therefore, \(\mathcal{U}_n\) is a subspace of \(\mathcal{M}_{nn}\).
        \end{example}
        \vphantom
        \\
        \\
        To show that \(W\) is not a subspace of \(V\),
        \begin{enumerate}
            \item If \(\vec{0}\nin S\), \(S\) is not a subspace.
            \item Find \(\vec{v}_1,\vec{v}_2\in S\) such that \(\vec{v}_1+\vec{v}_2\nin S\).
            \item Find \(\vec{v}\in S\) such that \((-1)\vec{v}\nin S\).
            \item Find \(\vec{v}\in S\) and \(c\in\mathbb{F}\) such that \(c\vec{v}\nin S\).
        \end{enumerate}
        \vphantom
        \\
        \\
        Consider the following examples of subsets of \(\mathbb{R}^n\) that are not subspaces.
        \begin{enumerate}
            \item The set of \(n\) dimensional vectors whose first coordinate is nonnegative.
            \item The set of unit \(n\) dimensional vectors.
            \item The set of \(n\) dimensional vectors with a zero in at least one coordinate, where \(n\geq 2\).
            \item The set of \(n\) dimensional vectors having all integer coordinates.
            \item The set of all \(n\) dimensional vectors whose first two coordinates add up to \(3\).
        \end{enumerate}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples of subsets of \(\mathcal{M}_{nn}\) that are not subspaces.
        \begin{enumerate}
            \item The set of nonsingular \(n\times n\) matrices.
            \item The set of singular \(n\times n\) matrices.
            \item The set of \(n\times n\) matrices in reduced row echelon form.
        \end{enumerate}
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Eigenspaces are Subspaces}{eigsubspc}

            Let \(A\in\mathcal{M}_{nn}\) and let \(\lambda\) be an eigenvalue of \(A\) with eigenspace \(E_\lambda\). Then, \(E_\lambda\) is a subspace of \(\mathbb{R}^n\).
            \begin{proof}
                By definition,
                \begin{equation*}
                    E_\lambda=\{X:AX=\lambda X\}.
                \end{equation*}
                We see that \(E_\lambda\neq\emptyset\), as \(\vec{0}\in E_\lambda\). Let \(\vec{x}_1,\vec{x}_2\in E_\lambda\). We must show that \(\vec{x}_1+\vec{x}_2\in E_\lambda\). That is, we wish to show that
                \begin{equation*}
                    A(\vec{x}_1+\vec{x}_2)=\lambda(\vec{x}_1+\vec{x}_2).
                \end{equation*}
                We realize that
                \begin{align*}
                    A(\vec{x}_1+\vec{x}_2)&=A\vec{x}_1+A\vec{x}_2 \\
                    &=\lambda\vec{x}_1+\lambda\vec{x}_2 \\
                    &=\lambda(\vec{x}_1+\vec{x}_2),
                \end{align*}
                as desired. Then, we must show that for some scalar \(c\), \(c\vec{x}_1\in E_\lambda\). We wish to show that 
                \begin{equation*}
                    A(c\vec{x}_1)=\lambda c\vec{x}_1.    
                \end{equation*}
                We see that
                \begin{align*}
                    A(c\vec{x}_1)&=cA\vec{x}_1 \\
                    &=c\lambda\vec{x}_1 \\
                    &=\lambda c\vec{x}_1,
                \end{align*}
                as desired. Because we have showed that the closure properties hold for \(E_\lambda\), \(E_\lambda\) is a subspace of \(\mathbb{R}^n\).
            \end{proof}

        \end{theorem}
        \pagebreak

\section{Lecture 23, October 19, 2022}

    \subsection{Span}
        
        We will now revisit the notion of linear combinations. Consider the following definitions.
        \begin{definition}{\Stop\,\,Finite Linear Combinations}{finitelincomb}

            Let \(S\) be a nonempty, and possibly infinite, subset of a vector space \(V\). Then, a vector \(\vec{v}\in V\) is a finite linear combination of the vectors in \(S\) if and only if there exists some finite subset \(S'=\{\vec{v}_1,\ldots\vec{v}_n\}\) of \(S\) such that
            \begin{equation*}
                \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
            \end{equation*}
            for scalars \(c_1,\ldots,c_n\).
            
        \end{definition}
        \begin{theorem}{\Stop\,\,Linear Combinations Remain in a Subspace}{lincombsubspc}

            Let \(W\) be a subspace of a vector space \(V\), and let \(\vec{v}_1,\ldots,\vec{v}_n\in W\). For scalars \(c_1,\ldots,c_n\), we have
            \begin{equation*}
                c_1\vec{v}_1+\cdots+c_n\vec{v}_n\in W.
            \end{equation*}
            \begin{proof}
                We proceed by induction. For the proposition when \(n=1\), consider the scalar \(c_1\in\mathbb{F}\) and the vector \(\vec{v}_1\in W\). By the closure property of scalar multiplication, \(c_1\vec{v}_1\in W\). Suppose the theorem holds for all \(n=k\). That is, for scalars \(c_1,\ldots,c_k\) and vectors \(\vec{v}_1,\ldots,\vec{v}_k\), we have
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_k\vec{v}_k\in W.
                \end{equation*}
                Then, for scalar \(c_{k+1}\in\mathbb{F}\) and vector \(\vec{v}_{k+1}\in W\), \(c_{k+1}\vec{v}_{k+1}\in W\) by the closure property of scalar multiplication, and by the closure property of vector addition,
                since \( c_1\vec{v}_1+\cdots+c_k\vec{v}_k\in W\), 
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_k\vec{v}_k+c_{k+1}\vec{v}_{k+1}\in W,
                \end{equation*}
                as desired.
            \end{proof}
        \end{theorem}
        \vphantom
        \\
        \\
        Essentially, Theorem \ref{thm:lincombsubspc} provides that subspaces are closed under linear combinations. We now, introduce the notion of ``span.''
        \begin{definition}{\Stop\,\,Span}{span}

            Let \(S\) be an infinite subset of a vector space \(V\), Then, \(\Span(S)\), the span of \(S\) in \(V\), is the set of all possible finite linear combinations of the vectors in \(S\). If \(S=\emptyset\), \(\Span(S)=\{\vec{0}\}\). Similarly, \(S\) is linearly dependent if and only if there exists some finite subset of 
            \(S\) such that \(S\) is linearly dependent.
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\,Find Span 1}{findspan1}
            
            Let \(S=\{[0,1,0],[0,0,1]\}\subseteq \mathbb{R}^3\). Find \(\Span(S)\).
            \\
            \\
            We realize that
            \begin{align*}
                \Span(S)&=\{c_1[0,1,0]+c_2[0,0,1]:c_1,c_2\in\mathbb{R}\} \\
                &=\{[0,c_1,c_2]:c_1,c_2\in\mathbb{R}\}.
            \end{align*}

        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Span 2}{findspan2}
            
            Let \(S=\left\{\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix},\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix},\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}\right\}\subseteq \mathcal{M}_{22}\). Find \(\Span(S)\).
            \\
            \\
            We realize that
            \begin{align*}
                \Span(S)&=\left\{c_1\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}+c_2\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}+c_3\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}:c_1,c_2,c_3\in\mathbb{R}\right\} \\
                &=\left\{\begin{bmatrix} c_1 & 0 \\ c_2 & c_3 \end{bmatrix}:c_1,c_2,c_3\in\mathbb{R}\right\}.
            \end{align*}

        \end{example}
        \vphantom
        \\
        \\
        Sometimes, for some \(S\subseteq V\), \(\Span(S)=V\). Here, we say that \(S\) spans \(V\), or equivalently, \(V\) is spanned by \(S\). Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Span 3}{findspan3}
            
            Let \(S=\{[1,0,0],[0,1,0],[0,0,1]\}\subseteq \mathbb{R}^3\). Find \(\Span(S)\).
            \\
            \\
            We realize that
            \begin{align*}
                \Span(S)&=\{c_1[1,0,0]+c_2[0,1,0]+c_3[0,0,1]:c_1,c_2,c_3\in\mathbb{R}\} \\
                &=\{[c_1,c_2,c_3]:c_1,c_2,c_3\in\mathbb{R}\} \\
                &=\mathbb{R}^3,
            \end{align*}
            meaning that \(S\) spans \(\mathbb{R}^3\).

        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Span 4}{findspan4}
            
            Let \(S=\{1,x^2,x^4,\ldots\}\subseteq V\), where \(V\) is the set of all polynomials. Find \(\Span(S)\).
            \\
            \\
            We realize that
            \begin{align*}
                T=\Span(S)=\left\{p(x):p(x)=\sum_{i=0}^n c_ix^i, c_i=0\text{ if }i\text{ is odd},n\in\mathbb{N}\right\}.
            \end{align*}
            \begin{proof}
                We must show that \(\Span(S)\subseteq T\) and \(T\subseteq S\).
            \end{proof}

        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorems.
        \begin{theorem}{\Stop\,\,A Complete Characterization of the Span}{spanchrs}
            
            Let \(S\) be a nonempty subset of a vector space \(V\). Then,
            \begin{enumerate}
                \item \(S\subseteq \Span(S)\).
                \begin{proof}
                    Suppose that \(\vec{v}\in S\). Then, \(1\vec{v}=\vec{v}\in\Span(S)\).
                \end{proof}
                \item \(\Span(S)\) is a subspace of \(V\).
                \begin{proof}
                    We know that \(\Span(S)\neq\emptyset\), by definition. Suppose that \(\vec{w}_1,\vec{w}_2\in \Span(S)\). We know that 
                    \begin{equation*}
                        \vec{w}_1=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
                    \end{equation*} 
                    for some \(c_1,\ldots,c_n\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_n\in S\). We also know that
                    \begin{equation*}
                        \vec{v}_2=d_1\vec{u}_1+\cdots+d_k\vec{u}_k.
                    \end{equation*}
                    for some \(d_1,\ldots,d_k\in\mathbb{F}\) and \(\vec{u}_1,\ldots,\vec{u}_k\in S\). Then,
                    \begin{align*}
                        \vec{w}_1+\vec{w}_2=(c_1\vec{v}_1+\cdots+c_k\vec{v}_k)+(d_1\vec{u}_1+\cdots+d_k\vec{u}_k).
                    \end{align*}
                    The sum is a linear combination of the elements in \(S\), therefore, \(\vec{w}_1+\vec{w}_2\in\Span(S)\), meaning \(\Span(S)\) is closed under vector addition. Now, we take \(c\in\mathbb{F}\) and \(\vec{v}\in\Span(S)\) and wish to show that \(c\vec{v}\in\Span(S)\). We know that
                    \begin{equation*}
                        \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
                    \end{equation*}
                    for some \(c_1,\ldots,c_n\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_n\in S\). Then,
                    \begin{equation*}
                        c\vec{v}=c(c_1\vec{v}_1)+\cdots+c(c_n\vec{v}_n),
                    \end{equation*}
                    which is a linear combination of the elements in \(S\), meaning that it is in \(\Span(S)\).
                \end{proof}
                \item If \(W\) is a subspace of \(V\) with \(S\subseteq W\), then, \(\Span(S)\subseteq W\).
                \begin{proof}
                    We let \(\vec{v}\in\Span(S)\) and wish to show that \(\vec{v}\in W\). We know that
                    \begin{equation*}
                        \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
                    \end{equation*}
                    for some \(c_1,\ldots,c_n\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_n\in S\). Since \(S\subseteq W\), and \(\vec{v}\) is a linear combination of elements of \(S\), \(\vec{v}\) is a linear combination of elements of \(W\). It follows that \(\vec{v}\in W\), by Theorem \ref{thm:lincombsubspc}.
                \end{proof}
                \item \(\Span(S)\) is the smallest subspace of \(V\) containing \(S\). 
            \end{enumerate}

        \end{theorem}
        \begin{theorem}{\Stop\,\,Two Subsets of a Vector Space and Their Span}{twosubvcspcspan}
        
            Let \(V\) be a vector space, and let \(S_1\subseteq S_2\subseteq V\). Then,
            \begin{equation*}
                \Span(S_1)\subseteq\Span(S_2).
            \end{equation*}
            \begin{proof}
                We see that \(S_1\subseteq\Span(S_1)\) and \(S_2\subseteq\Span(S_2)\). Because \(\Span(S_2)\) is the smallest subspace of \(V\) containing \(S_2\), we write
                \begin{equation*}
                    S_1\subseteq S_2\subseteq\Span(S_2)\subseteq V.
                \end{equation*}
                Then, because \(\Span(S_2)\) is a subspace of \(V\) with \(S_1\subseteq\Span(S_2)\), we have \(\Span(S_1)\subseteq\Span(S_2)\), as desired.
            \end{proof}
        \end{theorem}
        \vphantom
        \\
        \\
        Now, we turn to the question of how we can determine which vectors lie in \(\Span(S)\) given
        \begin{equation*}
            S=\{\vec{v}_1,\ldots,\vec{v}_k\}\subseteq\mathbb{R}^n.
        \end{equation*}
        Consider the following theorems.
        \begin{theorem}{\Stop\,\,Span and Row Space}{spanrowspc}

            Let \(A\) be the matrix having \(S=\{\vec{v}_1,\ldots,\vec{v}_k\}\subseteq\mathbb{R}^n\) as its rows. Then, the span of \(S\) is the row space of \(A\).
            
        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Computing Span as a Row Space}{compspanrowspc}

            Let \(\vec{v}_1=[3,6,0]\), \(\vec{v}_2=[0,-1,1]\), and \(S=\{\vec{v}_1,\vec{v}_2\}\). Find \(\Span(S)\). 
            \\
            \\
            We have the matrix
            \begin{equation*}
                A=\begin{bmatrix}
                    3 & 6 & 0 \\
                    0 & -1 & 1
                \end{bmatrix}.
            \end{equation*}
            The row space of \(A\), or equivalently, the span of \(S\) is
            \begin{equation*}
                \Span(S)=\{c_1[1,0,2]+c_2[0,1,-1]:c_1,c_2\in\mathbb{R}\}.
            \end{equation*}
            
        \end{example}
        \begin{theorem}{\Stop\,\,Is a Vector in the Span?}{vcspan}
            
            Let \(A\) be the matrix having \(S=\{\vec{v}_1,\ldots,\vec{v}_k\}\subseteq\mathbb{R}^n\) as its rows. Then, \(B\in\Span(S)\) if and only if the linear system \([A^T|B]\) has at least one solution.

        \end{theorem}
        \vphantom
        \\
        \\
        A similar algorithm is available for general vector spaces; but we will postpone the justification of it.
        

\pagebreak

\section{Lecture 24: October 21, 2022}

    \subsection{Linear Independence}

        We have mentioned the notion of linear independence and linear dependence before, but now, we will make these definitions more precise.
        \begin{definition}{\Stop\,\,Linear Independence and Dependence}{linindepdep}
            
            Suppose \(S=\{\vec{v}_1,\ldots,\vec{v}_n\}\subseteq V\). Then,
            \begin{enumerate}
                \item \(S\) is linearly dependent if and only if there exist scalars \(c_1,\ldots,c_n\in\mathbb{F}\) not all zero, such that
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0}.
                \end{equation*}
                \item \(S\) is linearly independent if \(S\) is not linearly dependent. That is,
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0} \iff c_1=\cdots=c_n=0.
                \end{equation*}
            \end{enumerate}

        \end{definition}
        \begin{definition}{\Stop\,\,Generalization Linear Independence and Dependence to Infinite Sets}{genlinindepdep}
            
            Suppose \(S\subseteq V\) where \(V\) is a vector space. We say \(S\) is linearly independent if and only if each finite subset of \(S\) is linearly idependent.

        \end{definition}
        \vphantom
        \\
        \\
        Consider the following theorems, providing methods of showing linear independence of subsets of \(\mathbb{R}^n\). Similar algorithms exist for general vector spaces, but we will postpone the justification of them.
        \begin{theorem}{\Stop\,\,Showing Linear Independence 1}{showlinindep1}

           Let \(A\) be the matrix having \(S=\{\vec{v}_1,\ldots,\vec{v}_k\}\subseteq \mathbb{R}^n\) as its columns. Then, \(S\) is linearly independent if and only if \(\det A\neq0\).
           \begin{proof}
                If \(\det A\neq0\), \(A\) is nonsingular and only has the trivial solution for the linear system \([AX=\vec{0}]\). That is, the only solution for the linear system is \(A\vec{0}=\vec{0}\). This means that
                \begin{equation*}
                    X
                    =
                    \begin{bmatrix}
                        c_1 \\ \vdots \\ c_n 
                    \end{bmatrix} 
                    =
                    \begin{bmatrix}
                        0\\ \vdots \\ 0
                    \end{bmatrix},
                \end{equation*}
                meaning \(c_1=\cdots=c_n=0\), which is the definition of linear independence.
           \end{proof}
            
        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,Showing Linear Independence 2}{showlinindep2}
            
            Suppose \(S\) is a finite set of vectors having at least two elements. Then, \(S\) is linearly independent if and only if no vector in \(S\) can be expressed as a linear combination of the other vectors in \(S\). Equivalently, \(S\) is linearly dependent if and only if some vector in \(S\) can be expressed as a linear combination of the other vectors in \(S\).
            \begin{proof}
                Suppose \(S\) is linearly dependent. That is, we have \(c_1,\ldots,c_n\in\mathbb{F}\) such that
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_{i-1}\vec{v}_{i-1}+c_i\vec{v}_i+c_{i+1}\vec{v}_{i+1}+\cdots+c_n\vec{v}_n=\vec{0}
                \end{equation*}
                with \(c_i\neq0\) for some \(i\). Then,
                \begin{equation*}
                    \vec{v}_i=\left(-\frac{c_1}{c_i}\right)\vec{v}_1+\cdots+\left(-\frac{c_{i-1}}{c_i}\right)\vec{v}_{i-1}+\left(-\frac{c_{i+1}}{c_i}\right)\vec{v}_{i+1}+\cdots+\left(-\frac{c_n}{c_i}\right)\vec{v}_n.
                \end{equation*}
                We have constructed a vector in \(S\) as a linear combination of the other vectors of \(S\). Now, we will assume that there is a vector \(\vec{v}_i\in S\) that is a linear combination of the other vectors in \(S\). Without loss of generality, suppose \(\vec{v}_i=\vec{v}_1\), meaning \(i=1\). Then, there exist \(c_2,\ldots,c_n\in\mathbb{F}\) such that
                \begin{equation*}
                    \vec{v}_1=c_2\vec{v}_2+\cdots+c_n\vec{v}_n.
                \end{equation*}
                Letting \(a_1=-1\), we have
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0}.
                \end{equation*}
                We constructed \(c_1\neq0\), meaning \(S\) is linearly dependent.
            \end{proof}
        \end{theorem}
        \begin{theorem}{\Stop\,\,Showing Linear Independence 3}{showlinindep3}
            
            Suppose \(S\subseteq V\) where \(V\) is a vector space. Then, \(S\) is linearly independent if and only if
            \begin{equation*}
                \forall\vec{v}\in S,\vec{v}\nin\Span(S-\{\vec{v}\}).
            \end{equation*}
            Similarly, \(S\) is linearly dependent if and only if
            \begin{equation*}
                \exists\vec{v}\in S,\vec{v}\in\Span(S-\{\vec{v}\}).
            \end{equation*}

        \end{theorem}