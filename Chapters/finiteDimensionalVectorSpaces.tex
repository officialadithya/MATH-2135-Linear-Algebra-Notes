\section{Lecture 20: October 7, 2022}

    \subsection{The Process of Abstraction}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Vector Spaces}{vcspc}

            Let \(\mathbb{F}\) be a field of scalars. For now, \(\mathbb{F}=\mathbb{R}\vee\mathbb{C}\). A vector space \(V\) over \(\mathbb{F}\) is a set with two operations: 
            \begin{enumerate}
                \item Vector Addition: \(V\times V\to V, (\vec{v},\vec{w})\mapsto \vec{v}+\vec{w}\).
                \item Scalar Multiplication: \(\mathbb{F}\times V\to V, (c,\vec{v})\mapsto c\vec{v}\).
            \end{enumerate}
            \vphantom
            \\
            \\
            The following axioms must hold for each \(\vec{u},\vec{v},\vec{w}\in V\) and \(c_1,c_2\in\mathbb{F}\).
            \begin{enumerate}
                \item \(\vec{u}+\vec{v}=\vec{v}+\vec{u}\)
                \item \(\vec{u}+(\vec{v}+\vec{w})=(\vec{u}+\vec{v})+\vec{w}\).
                \item \(\exists \vec{0}\in V, \forall \vec{v}\in V, \vec{0}+\vec{v}=\vec{v}\)
                \item \(\forall \vec{v}\in V, \exists! (-\vec{v})\in V, \vec{v}+(-\vec{v})=\vec{0}\).
                \item \(c_1(\vec{u}+\vec{v})=c_1\vec{u}+c_1\vec{v}\).
                \item \((c_1+c_2)\vec{u}=c_1\vec{u}+c_2\vec{u}\).
                \item \((c_1c_2)\vec{u}=c_1(c_2\vec{u})\).
                \item \(1\vec{u}=\vec{u}\).
            \end{enumerate}
            
        \end{definition}
        \vphantom
        \\
        \\
        We remark that \(0\vec{v}=\vec{0}\) is \textit{not} an axiom of a vector space; we must \textit{prove} that it holds. Now, we will justify our use of abstraction.
        \begin{enumerate}
            \item The set \(\mathbb{R}^n\) is a vector space.
            \item The set \(\mathbb{C}^n\) is a vector space.
            \item The set \(\{\vec{0}\}\), with \(\vec{0}+\vec{0}+\vec{0}\) and \(c\vec{0}=\vec{0}\), is a vector space.
            \item The set \(\mathcal{M}_{mn}\) is a vector space.
            \item Let \(S\) be a nonempty set and \(F(S)=\{f:S\to\mathbb{R}\}\) with \((f+g)(s)=f(s)+g(s)\) and \((cf)(s)=cf(s)\). The set \(F(S)\) is a vector space.
            \item The set \(\{a_nx^n+\cdots+a_0x^0:a_0,\ldots a_n\in\mathbb{R}\}\) is a vector space.
            \item The set \(\{a_nx^n+\cdots+a_0x^0:a_0,\ldots a_n\in\mathbb{R}, \text{degree is less than or equal to \(n\)}\}\) is a vector space.
        \end{enumerate}
        \vphantom
        \\
        \\
        We note that all the above examples need \textit{proof}. We remark that, for now, we will primarily consider vector spaces over \(\mathbb{R}\). We will emphasize when we consider vector spaces over \(\mathbb{C}\).  Consider the following examples of sets that are not vector spaces.
        \begin{enumerate}
            \item The set \(\mathbb{R}^+=\{x\in\mathbb{R}:x\geq0\}\), with usual addition and usual multiplication in \(\mathbb{R}\), is not a vector space. The set does not satisfy the closure axiom for scalar multiplication; \((-1)\cdot1=-1\nin\mathbb{R}^+\).
            \item The set \(S=\{f:\mathbb{R}\to\mathbb{R}:f(0)=1\}\), with \((f+g)(x)=f(x)+g(x)\) and \((cf)(x)=cf(x)\), is not a vector space. The set does not satisfy the closure axiom for vector addition; \((f+g)(0)=f(0)+g(0)=2\), so \((f+g)(x)\nin S\).
        \end{enumerate}

\pagebreak

\section{Lecture 21: October 14, 2022}

    \subsection{Derived Properties of Vector Spaces}

        Consider the following theorems. While they may seem obvious, they require proof by the vector space axioms, provided in Definition \ref{def:vcspc}.
        \begin{theorem}{\Stop\,\,Derived Property of Vector Space 1}{derpropvcspc1}

            Suppose \(V\) is a vector space, \(\vec{v}\in V\), and \(c\in\mathbb{F}\). Then,
            \begin{equation*}
                c\vec{0}=\vec{0}.
            \end{equation*}
            \begin{proof}
                We have that
                \begin{align*}
                    c\vec{0}&=c\vec{0}+\vec{0} &\text{ by axiom \(3\),} \\
                    &=c\vec{0}+(c\vec{0}+(-(c\vec{0}))) &\text{ by axiom \(4\),} \\
                    &=(c\vec{0}+c\vec{0})+(-(c\vec{0})) &\text{ by axiom \(2\),} \\
                    &=c(\vec{0}+\vec{0})+(-(c\vec{0})) &\text{ by axiom \(5\),} \\
                    &=c\vec{0}+(-(c\vec{0})) &\text{ by axiom \(3\),} \\
                    &=\vec{0} &\text{ by axiom \(4\),} 
                \end{align*}
                as desired.
            \end{proof}
        \end{theorem}
        \begin{theorem}{\Stop\,\,Derived Property of Vector Space 2}{derpropvcspc2}

            Suppose \(V\) is a vector space, \(\vec{v}\in V\), and \(c\in\mathbb{F}\). Then,
            \begin{equation*}
                0\vec{v}=\vec{0}.
            \end{equation*}
            \begin{proof}
                We have that
                \begin{align*}
                    0\vec{v}&=0\vec{v}+\vec{0} &\text{ by axiom \(3\),} \\
                    &=0\vec{v}+(0\vec{v}+(-(0\vec{v}))) &\text{ by axiom \(4\),} \\
                    &=(0\vec{v}+0\vec{v})+(-(0\vec{v})) &\text{ by axiom \(2\),} \\
                    &=(0+0)\vec{v}+(-(0\vec{v})) &\text{ by axiom \(6\),} \\
                    &=0\vec{v}+(-(0\vec{v})) &\text{ by properties of \(\mathbb{R}\),} \\
                    &=\vec{0} &\text{ by axiom \(4\),}
                \end{align*}
                as desired.
            \end{proof}
        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,Derived Property of Vector Space 3}{derpropvcspc3}

            Suppose \(V\) is a vector space, \(\vec{v}\in V\), and \(c\in\mathbb{F}\). Then,
            \begin{equation*}
                (-1)\vec{v}=-\vec{v}.
            \end{equation*}
            \begin{proof}
                We have that
                \begin{align*}
                    \vec{v}+(-1)\vec{v}&=1\vec{v}+(-1)\vec{v} &\text{ by axiom \(8\),} \\
                    &=(1+(-1))\vec{v} &\text{ by axiom \(6\),} \\
                    &=0\vec{v} &\text{ by properties of \(\mathbb{R}\),} \\
                    &=\vec{0} &\text{ by Theorem \ref{thm:derpropvcspc2},}
                \end{align*}
                which implies \((-1)\vec{v}=-\vec{v}\), the additive inverse of \(\vec{v}\).
            \end{proof}
        \end{theorem}
        \begin{theorem}{\Stop\,\,Derived Property of Vector Space 4}{derpropvcspc4}

            Suppose \(V\) is a vector space, \(\vec{v}\in V\), and \(c\in\mathbb{F}\). Then,
            \begin{equation*}
                c\vec{v}=\vec{0}\iff c=0\vee \vec{v}=\vec{0}
            \end{equation*}
            \begin{proof}
                The statement \(c=0\vee\vec{v}=\vec{0}\implies c\vec{v}=\vec{0}\) is governed by the first two derived results. Then, to show that \(c\vec{v}=\vec{0}\implies c=0\vee\vec{v}=\vec{0}\), we suppose that \(c\neq0\) and wish to show that \(\vec{v}=\vec{0}\). We have
                \begin{equation*}
                    c\vec{v}=\vec{0}
                \end{equation*}
                with \(c\neq0\). Then,
                \begin{equation*}
                    \left(\frac{1}{c}\right)(c\vec{v})=\frac{1}{c}\vec{0}=\vec{0},
                \end{equation*}
                by Theorem \ref{thm:derpropvcspc1}. But, by axiom \(7\), 
                \begin{equation*}
                    \frac{1}{c}(c\vec{v})=\left(\frac{1}{c}c\vec{v}\right)=\vec{v},
                \end{equation*}
                which implies \(\vec{v}=\vec{0}\).
            \end{proof}
        \end{theorem}

\pagebreak

\section{Lecture 22: October 17, 2022}

    \subsection{Subspaces}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Subspaces}{subspc}

            Suppose \(V\) is a vector space and the set \(W\subseteq V\). Then, \(W\) is a subspace of \(V\) if and only if \(W\) is a vector space with the same operations as \(V\).
            
        \end{definition}
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Showing a Vector Space is a Subspace}{showsubspc}

            Suppose \(V\) is a vector space and the set \(W\) is a subset of \(V\). Then, \(W\) is a subspace of \(V\) if and only if
            \begin{enumerate}
                \item \(W\neq\emptyset\).
                \item \(\vec{w}_1,\vec{w}_2\in W\implies \vec{w}_1+\vec{w}_2\in W\).
                \item \(\vec{w}\in W\implies c\vec{w}\in W, c\in\mathbb{F}\).
            \end{enumerate}
            \begin{proof}
                We know that \(W\neq\emptyset\), because \(\vec{0}\in W\). Next, \(\vec{w}_1,\vec{w}_2\in W\implies \vec{w}_1+\vec{w}_2\in W\) by the closure property of addition of a vector space \(W\). Then, \(\vec{w}\in W\implies c\vec{w}\in W, c\in\mathbb{F}\) by the closure property of scalar multiplication of a vector space \(W\). Now, we must show that all vector space axioms hold for \(W\). The closure properties are given by conditions \(2\) and \(3\). For vector space axiom \(1\), we take \(\vec{w}_1,\vec{w}_2\in W\). Then,
                \begin{align*}
                    \vec{w}_1+_W\vec{w}_2&=\vec{w}_1+_V\vec{w}_2 \\
                    &=\vec{w}_2+_V\vec{w}_1 \\
                    &=\vec{w}_2+_W\vec{w}_1.
                \end{align*}
                For vector space axiom \(3\), we take \(\vec{w}\in W\) and by the closure property of scalar multiplication, \((-1)\vec{w}\in W\). By the closure property of vector addition \(\vec{w}+(-1)\vec{w}=\vec{0}\), meaning \(\vec{0}\in W\). Moreover, for any \(\vec{w}\in W\), \(\vec{w}+\vec{0}=\vec{0}+\vec{w}=\vec{w}\). For vector space axiom \(4\), \(-1\vec{w}\) is the additive inverse of \(\vec{w}\), and by the closure property, \((-1)\vec{w}\in W\). We must show the rest, and leave this as an exercise to the reader.
            \end{proof}
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\,Is it a Subspace? 1}{issub1}

            Let \(V=\mathbb{R}^2\) and let 
            \begin{equation*}
                W=\{[x,0]:x\in\mathbb{R}\}\subseteq \mathbb{R}^2.
            \end{equation*}
            Is \(W\) a subspace of \(V\)?.
            \\
            \\
            We see that \(W\neq\emptyset\), as \([0,0]\in W\). Then, if \([x_1,0],[x_2,0]\in W\),
            \begin{equation*}
                [x_1,0]+[x_2,0]=[x_1+x_2,0]\in W.
            \end{equation*}
            Then, if \(c\in\mathbb{R}\) and \([x,0]\in W\),
            \begin{equation*}
                c[x,0]=[cx,0]\in W.
            \end{equation*}
            Because \(W\neq\emptyset\) is closed under vector addition and scalar multiplication, \(W\) is a subspace of \(V\).

        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Subspace? 2}{issub2}

            Let \(V=\mathcal{M}_{nn}\) and let
            \begin{equation*}
                W=\mathcal{D}_n\subseteq\mathcal{U}_n\subseteq\mathcal{M}_{nn},
            \end{equation*}
            where \(\mathcal{D}_n\) is the set of all \(n\times n\) diagonal matrices and \(\mathcal{U}_n\) is the set of all \(n\times n\) upper triangular matrices. Is \(W\) a subspace of \(V\)?.
            \\
            \\
            We see that \(W\neq\emptyset\), since \(\begin{bmatrix} 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0 \end{bmatrix}\in W\). Then,
            for \(A,B\in\mathcal{D}_n\), 
            \begin{align*}
                (A+B)_{ij}&=A_{ij}+B_{ij} \\
                &=\begin{cases}
                    A_{ii}+B_{ii} & i=j \\
                    0 & i \neq j
                \end{cases} \\
                &\in\mathcal{D}_n.
            \end{align*}
            Then, for some \(c\in\mathbb{R}\),
            \begin{align*}
                (cA)_{ij}&=cA_{ij} \\
                &=\begin{cases}
                    cA_{ii} & i=j \\
                    0 & i \neq j
                \end{cases} \\
                &\in\mathcal{D}_n.
            \end{align*}
            Therefore, \(\mathcal{D}_n\) is a subspace of \(\mathcal{M}_{nn}\).
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Is it a Subspace? 3}{issub3}

            Let \(V=\mathcal{M}_{nn}\) and let
            \begin{equation*}
                W=\mathcal{U}_n\subseteq\mathcal{M}_{nn},
            \end{equation*}
            where \(\mathcal{U}_n\) is the set of all \(n\times n\) upper triangular matrices. Is \(W\) a subspace of \(V\)?.
            \\
            \\
            We see that \(W\neq\emptyset\), since \(\begin{bmatrix} 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0 \end{bmatrix}\in W\). Then,
            for \(A,B\in\mathcal{U}_n\), 
            \begin{align*}
                (A+B)_{ij}&=A_{ij}+B_{ij} \\
                &=\begin{cases}
                    A_{ij}+B_{ij} & i\leq j \\
                    0 & i > j
                \end{cases} \\
                &\in\mathcal{U}_n.
            \end{align*}
            Then, for some \(c\in\mathbb{R}\),
            \begin{align*}
                (cA)_{ij}&=cA_{ij} \\
                &=\begin{cases}
                    cA_{ij} & i\leq j \\
                    0 & i > j
                \end{cases} \\
                &\in\mathcal{U}_n.
            \end{align*}
            Therefore, \(\mathcal{U}_n\) is a subspace of \(\mathcal{M}_{nn}\).
        \end{example}
        \vphantom
        \\
        \\
        To show that \(W\) is not a subspace of \(V\),
        \begin{enumerate}
            \item If \(\vec{0}\nin S\), \(S\) is not a subspace.
            \item Find \(\vec{v}_1,\vec{v}_2\in S\) such that \(\vec{v}_1+\vec{v}_2\nin S\).
            \item Find \(\vec{v}\in S\) such that \((-1)\vec{v}\nin S\).
            \item Find \(\vec{v}\in S\) and \(c\in\mathbb{F}\) such that \(c\vec{v}\nin S\).
        \end{enumerate}
        \vphantom
        \\
        \\
        Consider the following examples of subsets of \(\mathbb{R}^n\) that are not subspaces.
        \begin{enumerate}
            \item The set of \(n\) dimensional vectors whose first coordinate is nonnegative.
            \item The set of unit \(n\) dimensional vectors.
            \item The set of \(n\) dimensional vectors with a zero in at least one coordinate, where \(n\geq 2\).
            \item The set of \(n\) dimensional vectors having all integer coordinates.
            \item The set of all \(n\) dimensional vectors whose first two coordinates add up to \(3\).
        \end{enumerate}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples of subsets of \(\mathcal{M}_{nn}\) that are not subspaces.
        \begin{enumerate}
            \item The set of nonsingular \(n\times n\) matrices.
            \item The set of singular \(n\times n\) matrices.
            \item The set of \(n\times n\) matrices in reduced row echelon form.
        \end{enumerate}
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Eigenspaces are Subspaces}{eigsubspc}

            Let \(A\in\mathcal{M}_{nn}\) and let \(\lambda\) be an eigenvalue of \(A\) with eigenspace \(E_\lambda\). Then, \(E_\lambda\) is a subspace of \(\mathbb{R}^n\).
            \begin{proof}
                By definition,
                \begin{equation*}
                    E_\lambda=\{X:AX=\lambda X\}.
                \end{equation*}
                We see that \(E_\lambda\neq\emptyset\), as \(\vec{0}\in E_\lambda\). Let \(\vec{x}_1,\vec{x}_2\in E_\lambda\). We must show that \(\vec{x}_1+\vec{x}_2\in E_\lambda\). That is, we wish to show that
                \begin{equation*}
                    A(\vec{x}_1+\vec{x}_2)=\lambda(\vec{x}_1+\vec{x}_2).
                \end{equation*}
                We realize that
                \begin{align*}
                    A(\vec{x}_1+\vec{x}_2)&=A\vec{x}_1+A\vec{x}_2 \\
                    &=\lambda\vec{x}_1+\lambda\vec{x}_2 \\
                    &=\lambda(\vec{x}_1+\vec{x}_2),
                \end{align*}
                as desired. Then, we must show that for some scalar \(c\), \(c\vec{x}_1\in E_\lambda\). We wish to show that 
                \begin{equation*}
                    A(c\vec{x}_1)=\lambda c\vec{x}_1.    
                \end{equation*}
                We see that
                \begin{align*}
                    A(c\vec{x}_1)&=cA\vec{x}_1 \\
                    &=c\lambda\vec{x}_1 \\
                    &=\lambda c\vec{x}_1,
                \end{align*}
                as desired. Because we have showed that the closure properties hold for \(E_\lambda\), \(E_\lambda\) is a subspace of \(\mathbb{R}^n\).
            \end{proof}

        \end{theorem}
        \pagebreak

\section{Lecture 23, October 19, 2022}

    \subsection{Span}
        
        We will now revisit the notion of linear combinations. Consider the following definitions.
        \begin{definition}{\Stop\,\,Finite Linear Combinations}{finitelincomb}

            Let \(S\) be a nonempty, and possibly infinite, subset of a vector space \(V\). Then, a vector \(\vec{v}\in V\) is a finite linear combination of the vectors in \(S\) if and only if there exists some finite subset \(S'=\{\vec{v}_1,\ldots\vec{v}_n\}\) of \(S\) such that
            \begin{equation*}
                \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
            \end{equation*}
            for scalars \(c_1,\ldots,c_n\).
            
        \end{definition}
        \begin{theorem}{\Stop\,\,Linear Combinations Remain in a Subspace}{lincombsubspc}

            Let \(W\) be a subspace of a vector space \(V\), and let \(\vec{v}_1,\ldots,\vec{v}_n\in W\). For scalars \(c_1,\ldots,c_n\), we have
            \begin{equation*}
                c_1\vec{v}_1+\cdots+c_n\vec{v}_n\in W.
            \end{equation*}
            \begin{proof}
                We proceed by induction. For the proposition when \(n=1\), consider the scalar \(c_1\in\mathbb{F}\) and the vector \(\vec{v}_1\in W\). By the closure property of scalar multiplication, \(c_1\vec{v}_1\in W\). Suppose the theorem holds for all \(n=k\). That is, for scalars \(c_1,\ldots,c_k\) and vectors \(\vec{v}_1,\ldots,\vec{v}_k\), we have
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_k\vec{v}_k\in W.
                \end{equation*}
                Then, for scalar \(c_{k+1}\in\mathbb{F}\) and vector \(\vec{v}_{k+1}\in W\), \(c_{k+1}\vec{v}_{k+1}\in W\) by the closure property of scalar multiplication, and by the closure property of vector addition,
                since \( c_1\vec{v}_1+\cdots+c_k\vec{v}_k\in W\), 
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_k\vec{v}_k+c_{k+1}\vec{v}_{k+1}\in W,
                \end{equation*}
                as desired.
            \end{proof}
        \end{theorem}
        \vphantom
        \\
        \\
        Essentially, Theorem \ref{thm:lincombsubspc} provides that subspaces are closed under linear combinations. We now, introduce the notion of ``span.''
        \begin{definition}{\Stop\,\,Span}{span}

            Let \(S\) be an infinite subset of a vector space \(V\), Then, \(\Span(S)\), the span of \(S\) in \(V\), is the set of all possible finite linear combinations of the vectors in \(S\). If \(S=\emptyset\), \(\Span(S)=\{\vec{0}\}\). Similarly, \(S\) is linearly dependent if and only if there exists some finite subset of 
            \(S\) such that \(S\) is linearly dependent.
            
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\,Find Span 1}{findspan1}
            
            Let \(S=\{[0,1,0],[0,0,1]\}\subseteq \mathbb{R}^3\). Find \(\Span(S)\).
            \\
            \\
            We realize that
            \begin{align*}
                \Span(S)&=\{c_1[0,1,0]+c_2[0,0,1]:c_1,c_2\in\mathbb{R}\} \\
                &=\{[0,c_1,c_2]:c_1,c_2\in\mathbb{R}\}.
            \end{align*}

        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Span 2}{findspan2}
            
            Let \(S=\left\{\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix},\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix},\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}\right\}\subseteq \mathcal{M}_{22}\). Find \(\Span(S)\).
            \\
            \\
            We realize that
            \begin{align*}
                \Span(S)&=\left\{c_1\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}+c_2\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}+c_3\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}:c_1,c_2,c_3\in\mathbb{R}\right\} \\
                &=\left\{\begin{bmatrix} c_1 & 0 \\ c_2 & c_3 \end{bmatrix}:c_1,c_2,c_3\in\mathbb{R}\right\}.
            \end{align*}

        \end{example}
        \vphantom
        \\
        \\
        Sometimes, for some \(S\subseteq V\), \(\Span(S)=V\). Here, we say that \(S\) spans \(V\), or equivalently, \(V\) is spanned by \(S\). Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Span 3}{findspan3}
            
            Let \(S=\{[1,0,0],[0,1,0],[0,0,1]\}\subseteq \mathbb{R}^3\). Find \(\Span(S)\).
            \\
            \\
            We realize that
            \begin{align*}
                \Span(S)&=\{c_1[1,0,0]+c_2[0,1,0]+c_3[0,0,1]:c_1,c_2,c_3\in\mathbb{R}\} \\
                &=\{[c_1,c_2,c_3]:c_1,c_2,c_3\in\mathbb{R}\} \\
                &=\mathbb{R}^3,
            \end{align*}
            meaning that \(S\) spans \(\mathbb{R}^3\).

        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Find Span 4}{findspan4}
            
            Let \(S=\{1,x^2,x^4,\ldots\}\subseteq V\), where \(V\) is the set of all polynomials. Find \(\Span(S)\).
            \\
            \\
            We realize that
            \begin{align*}
                T=\Span(S)=\left\{p(x):p(x)=\sum_{i=0}^n c_ix^i, c_i=0\text{ if }i\text{ is odd},n\in\mathbb{N}\right\}.
            \end{align*}
            \begin{proof}
                We must show that \(\Span(S)\subseteq T\) and \(T\subseteq S\).
            \end{proof}

        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following theorems.
        \begin{theorem}{\Stop\,\,A Complete Characterization of the Span}{spanchrs}
            
            Let \(S\) be a nonempty subset of a vector space \(V\). Then,
            \begin{enumerate}
                \item \(S\subseteq \Span(S)\).
                \begin{proof}
                    Suppose that \(\vec{v}\in S\). Then, \(1\vec{v}=\vec{v}\in\Span(S)\).
                \end{proof}
                \item \(\Span(S)\) is a subspace of \(V\).
                \begin{proof}
                    We know that \(\Span(S)\neq\emptyset\), by definition. Suppose that \(\vec{w}_1,\vec{w}_2\in \Span(S)\). We know that 
                    \begin{equation*}
                        \vec{w}_1=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
                    \end{equation*} 
                    for some \(c_1,\ldots,c_n\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_n\in S\). We also know that
                    \begin{equation*}
                        \vec{v}_2=d_1\vec{u}_1+\cdots+d_k\vec{u}_k.
                    \end{equation*}
                    for some \(d_1,\ldots,d_k\in\mathbb{F}\) and \(\vec{u}_1,\ldots,\vec{u}_k\in S\). Then,
                    \begin{align*}
                        \vec{w}_1+\vec{w}_2=(c_1\vec{v}_1+\cdots+c_k\vec{v}_k)+(d_1\vec{u}_1+\cdots+d_k\vec{u}_k).
                    \end{align*}
                    The sum is a linear combination of the elements in \(S\), therefore, \(\vec{w}_1+\vec{w}_2\in\Span(S)\), meaning \(\Span(S)\) is closed under vector addition. Now, we take \(c\in\mathbb{F}\) and \(\vec{v}\in\Span(S)\) and wish to show that \(c\vec{v}\in\Span(S)\). We know that
                    \begin{equation*}
                        \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
                    \end{equation*}
                    for some \(c_1,\ldots,c_n\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_n\in S\). Then,
                    \begin{equation*}
                        c\vec{v}=c(c_1\vec{v}_1)+\cdots+c(c_n\vec{v}_n),
                    \end{equation*}
                    which is a linear combination of the elements in \(S\), meaning that it is in \(\Span(S)\).
                \end{proof}
                \item If \(W\) is a subspace of \(V\) with \(S\subseteq W\), then, \(\Span(S)\subseteq W\).
                \begin{proof}
                    We let \(\vec{v}\in\Span(S)\) and wish to show that \(\vec{v}\in W\). We know that
                    \begin{equation*}
                        \vec{v}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
                    \end{equation*}
                    for some \(c_1,\ldots,c_n\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_n\in S\). Since \(S\subseteq W\), and \(\vec{v}\) is a linear combination of elements of \(S\), \(\vec{v}\) is a linear combination of elements of \(W\). It follows that \(\vec{v}\in W\), by Theorem \ref{thm:lincombsubspc}.
                \end{proof}
                \item \(\Span(S)\) is the smallest subspace of \(V\) containing \(S\). 
            \end{enumerate}

        \end{theorem}
        \begin{theorem}{\Stop\,\,Two Subsets of a Vector Space and Their Span}{twosubvcspcspan}
        
            Let \(V\) be a vector space, and let \(S_1\subseteq S_2\subseteq V\). Then,
            \begin{equation*}
                \Span(S_1)\subseteq\Span(S_2).
            \end{equation*}
            \begin{proof}
                We see that \(S_1\subseteq\Span(S_1)\) and \(S_2\subseteq\Span(S_2)\). Because \(\Span(S_2)\) is the smallest subspace of \(V\) containing \(S_2\), we write
                \begin{equation*}
                    S_1\subseteq S_2\subseteq\Span(S_2)\subseteq V.
                \end{equation*}
                Then, because \(\Span(S_2)\) is a subspace of \(V\) with \(S_1\subseteq\Span(S_2)\), we have \(\Span(S_1)\subseteq\Span(S_2)\), as desired.
            \end{proof}
        \end{theorem}
        \vphantom
        \\
        \\
        Now, we turn to the question of how we can determine which vectors lie in \(\Span(S)\) given
        \begin{equation*}
            S=\{\vec{v}_1,\ldots,\vec{v}_k\}\subseteq\mathbb{R}^n.
        \end{equation*}
        Consider the following theorems.
        \begin{theorem}{\Stop\,\,Span and Row Space}{spanrowspc}

            Let \(A\) be the matrix having \(S=\{\vec{v}_1,\ldots,\vec{v}_k\}\subseteq\mathbb{R}^n\) as its rows. Then, the span of \(S\) is the row space of \(A\).
            
        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Computing Span as a Row Space}{compspanrowspc}

            Let \(\vec{v}_1=[3,6,0]\), \(\vec{v}_2=[0,-1,1]\), and \(S=\{\vec{v}_1,\vec{v}_2\}\). Find \(\Span(S)\). 
            \\
            \\
            We have the matrix
            \begin{equation*}
                A=\begin{bmatrix}
                    3 & 6 & 0 \\
                    0 & -1 & 1
                \end{bmatrix}.
            \end{equation*}
            The row space of \(A\), or equivalently, the span of \(S\) is
            \begin{equation*}
                \Span(S)=\{c_1[1,0,2]+c_2[0,1,-1]:c_1,c_2\in\mathbb{R}\}.
            \end{equation*}
            
        \end{example}
        \begin{theorem}{\Stop\,\,Is a Vector in the Span?}{vcspan}
            
            Let \(A\) be the matrix having \(S=\{\vec{v}_1,\ldots,\vec{v}_k\}\subseteq\mathbb{R}^n\) as its rows. Then, \(B\in\Span(S)\) if and only if the linear system \([A^T|B]\) has at least one solution.

        \end{theorem}
        \vphantom
        \\
        \\
        A similar algorithm is available for general vector spaces; but we will postpone the justification of it.
        

\pagebreak

\section{Lecture 24: October 21, 2022}

    \subsection{Linear Independence}

        We have mentioned the notion of linear independence and linear dependence before, but now, we will make these definitions more precise. As such, this section will be heavy on theory.
        \begin{definition}{\Stop\,\,Linear Independence and Dependence}{linindepdep}
            
            Suppose \(S=\{\vec{v}_1,\ldots,\vec{v}_n\}\subseteq V\). Then,
            \begin{enumerate}
                \item \(S\) is linearly dependent if and only if there exist scalars \(c_1,\ldots,c_n\in\mathbb{F}\) not all zero, such that
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0}.
                \end{equation*}
                \item \(S\) is linearly independent if \(S\) is not linearly dependent. That is,
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0} \iff c_1=\cdots=c_n=0.
                \end{equation*}
                If \(S=\emptyset\), \(S\) is linearly independent.
            \end{enumerate}

        \end{definition}
        \begin{definition}{\Stop\,\,Generalizing Linear Independence and Dependence to Infinite Sets}{genlinindepdep}
            
            Suppose \(S\subseteq V\) where \(V\) is a vector space. We say \(S\) is linearly independent if and only if each finite subset of \(S\) is linearly idependent.

        \end{definition}
        \vphantom
        \\
        \\
        Consider the following theorems, providing methods of showing linear independence of subsets of \(\mathbb{R}^n\). Similar algorithms exist for general vector spaces, but we will postpone the justification of them.
        \begin{theorem}{\Stop\,\,Showing Linear Independence 1}{showlinindep1}

            Let \(A\in\mathcal{M}_{nn}\). If \(\det A \neq 0\), the columns of \(A\) are linearly independent. If \(\det A = 0\), the columns of \(A\) are linearly dependent.
            \begin{proof}
                If \(\det A\neq 0\), \(A\) is nonsingular and has only the trivial solution for \([A|\vec{0}]\). This linear system is precisely
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0}
                \end{equation*}
                for \(c_1,\ldots,c_n\in\mathbb{R}\). Since the system has only the trivial solution, \(X=[c_1,\ldots,c_n]^T=[0,\ldots,0]^T\) and \(c_1=\cdots=c_n=0\), which by definition, means that the set \(\{\vec{v}_1,\ldots,\vec{v}_n\}\), or equivalently the columns of \(A\), are linearly independent. If \(\det A =0\), \(A\) is singular and has infinitely many nontrivial solutions for \([A|\vec{0}]\). Thus, \(c_1,\cdots,c_n\) will not all be zero, meaning that the columns of \(A\) are linearly dependent.
            \end{proof}

        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,Showing Linear Independence 2}{showlinindep2}
            
            Suppose \(S\) is a finite set of vectors having at least two elements. Then, \(S\) is linearly independent if and only if no vector in \(S\) can be expressed as a linear combination of the other vectors in \(S\). Equivalently, \(S\) is linearly dependent if and only if some vector in \(S\) can be expressed as a linear combination of the other vectors in \(S\).
            \begin{proof}
                Suppose \(S\) is linearly dependent. That is, we have \(c_1,\ldots,c_n\in\mathbb{F}\) such that
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_{i-1}\vec{v}_{i-1}+c_i\vec{v}_i+c_{i+1}\vec{v}_{i+1}+\cdots+c_n\vec{v}_n=\vec{0}
                \end{equation*}
                with \(c_i\neq0\) for some \(i\). Then,
                \begin{equation*}
                    \vec{v}_i=\left(-\frac{c_1}{c_i}\right)\vec{v}_1+\cdots+\left(-\frac{c_{i-1}}{c_i}\right)\vec{v}_{i-1}+\left(-\frac{c_{i+1}}{c_i}\right)\vec{v}_{i+1}+\cdots+\left(-\frac{c_n}{c_i}\right)\vec{v}_n.
                \end{equation*}
                We have constructed a vector in \(S\) as a linear combination of the other vectors of \(S\). Now, we will assume that there is a vector \(\vec{v}_i\in S\) that is a linear combination of the other vectors in \(S\). Without loss of generality, suppose \(\vec{v}_i=\vec{v}_1\), meaning \(i=1\). Then, there exist \(c_2,\ldots,c_n\in\mathbb{F}\) such that
                \begin{equation*}
                    \vec{v}_1=c_2\vec{v}_2+\cdots+c_n\vec{v}_n.
                \end{equation*}
                Letting \(a_1=-1\), we have
                \begin{equation*}
                    c_1\vec{v}_1+\cdots+c_n\vec{v}_n=\vec{0}.
                \end{equation*}
                We constructed \(c_1\neq0\), meaning \(S\) is linearly dependent.
            \end{proof}
        \end{theorem}
        \begin{theorem}{\Stop\,\,Showing Linear Independence 3}{showlinindep3}
            
            Suppose \(S\subseteq V\) where \(V\) is a vector space. Then, \(S\) is linearly independent if and only if
            \begin{equation*}
                \forall\vec{v}\in S,\vec{v}\nin\Span(S-\{\vec{v}\}).
            \end{equation*}
            Similarly, \(S\) is linearly dependent if and only if
            \begin{equation*}
                \exists\vec{v}\in S,\vec{v}\in\Span(S-\{\vec{v}\}).
            \end{equation*}

        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        We will now consider the linear dependence of sets with one or two elements.
        \begin{theorem}{\Stop\,\,Linear Dependence of Sets With One or Two Elements}{onetwosetsdep}

            Let \(S=\{\vec{v}\}\), and we have that if \(c\vec{v}=\vec{0}\) for some \(c\neq0\), \(S\) is linearly dependent. If \(S\) is linearly dependent, we know that \(\vec{v}=\vec{0}\) by Theorem \ref{thm:derpropvcspc4}. If \(\vec{v}=\vec{0}\), then \(1\vec{v}=\vec{0}\) and \(S\) is linearly dependent. Therefore \(S\) is linearly dependent if and only if \(\vec{v}=\vec{0}\). Now, suppose that \(S=\{\vec{v}_1,\vec{v}_2\}\). If \(\vec{v}_1=\vec{0}\), \(S\) is linearly dependent because
            \begin{equation*}
                1\vec{v}_1+0\vec{v}_2=\vec{0}.
            \end{equation*}
            Then, if \(\vec{v}_2=\vec{0}\), \(S\) is linearly dependent because
            \begin{equation*}
                0\vec{v}_1+1\vec{v}_2=\vec{0}.
            \end{equation*}
            If both \(\vec{v}_1\) and \(\vec{v}_2\) are nonzero, and there exist \(c_1,c_2\in\mathbb{F}\), not both zero, such that
            \begin{equation*}
                c_1\vec{v}_1+c_2\vec{v}_2=\vec{0}.
            \end{equation*}
            If \(c_1\neq0\), \(\vec{v}_1=-\frac{c_2}{c_1}\vec{v}_2\). That is, \(\vec{v}_1\) is a scalar multiple of \(\vec{v}_2\). If \(c_2\neq0\), \(\vec{v}_1=-\frac{c_1}{c_2}\vec{v}_2\). That is, \(\vec{v}_1\) is a scalar multiple of \(\vec{v}_2\). That means, a set of two nonzero vectors is linearly dependent if and only if the vectors are scalar multiples of each other.
        
        \end{theorem}
        \vphantom
        \\
        \\
        Now, we will consider the linear dependence of a finite subset of a vector space \(V\) that contains the zero vector.
        \begin{theorem}{\Stop\,\,Linear Dependence of Finite Subsets of a Vector Space Containing \(\vec{0}\)}{finitesubsetcontain0}
            
            Any finite subset of a vector space that contains the zero vector \(\vec{0}\) is linearly dependent.
            \begin{proof}
                Recall that if \(S\) is a set with \(|S|=1\) or \(|S|=2\), \(S\) is linearly dependent. Now, let \(S=\{\vec{v}_1,\ldots,\vec{v}_n\}\), a finite subset of a vector space \(V\) and \(\vec{0}\in S\). If \(\vec{v}_k=\vec{0}\), we have
                \begin{equation*}
                    0\vec{v}_1+\cdots+1\vec{v}_k+\cdots+0\vec{v}_n,
                \end{equation*}
                meaning that \(S\) is linearly dependent.
            \end{proof}

        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider another characterization of linear independence.
        \begin{theorem}{\Stop\,\,Linear Independence of Nonempty Sets}{linindepnonemptsets}
        
            A nonempty set \(S=\{\vec{v}_1,\ldots,\vec{v}_n\}\) is linearly independent if and only if both the following conditions hold:
            \begin{enumerate}
                \item \(\vec{v}_1\neq\vec{0}\).
                \item \(\forall k,2\leq k \leq n, \vec{v}_k\nin\Span(\{\vec{v}_1,\ldots,\vec{v}_{k-1}\})\).
            \end{enumerate}
        
        \end{theorem}

\pagebreak

    \section{Lecture 25: October 24, 2022}

        \subsection{Determining Linear Indpendence in \(\mathbb{R}^n\)}

            We now consider a method to test for linear independence using row reduction in \(\mathbb{R}^n\), and present a related result on linear dependence in \(\mathbb{R}^n\).
            \begin{theorem}{\Stop\,\,A Test for Linear Independence in \(\mathbb{R}^n\)}{lindeprn}
                
                Let \(S\) be a finite nonempty set of vectors in \(\mathbb{R}^n\). To determine whether \(S\) is linearly independent, perform the following steps:
                \begin{enumerate}
                    \item Create the matrix \(A\) whose columns are the vectors in \(S\).
                    \item Solve the system \([A|\vec{0}]\) by row reduction.
                \end{enumerate}
                Then, \(S\) is linearly independent if the system has only the trivial solution, and linearly dependent otherwise.
            \end{theorem}
            \begin{theorem}{\Stop\,\,A Test for Linear Dependence in \(\mathbb{R}^n\)}{indlindep}

                Suppose \(S\subseteq\mathbb{R}^n\) and \(S\) contains distinct elements \(\vec{v}_1,\ldots,\vec{v}_k\) where \(k>n\). Then, \(S\) is linearly dependent.
                
            \end{theorem}
            \vphantom
            \\
            \\
            Consider the following examples.
            \begin{example}{\Difficulty\,\Difficulty\,\,Determining Linear Dependence in \(\mathbb{R}^n\) 1}{detlindeprn1}
                
                Let \(S=\{[1,-1,0,2],[0,-2,1,0],[2,0,-1,1]\}\subseteq\mathbb{R}^4\). We build the system
                \begin{equation*}
                    \begin{bmatrix}
                        1 & 0 & 2 & | & 0 \\
                        -1 & -2 & 0 & | & 0 \\
                        0 & 1 & -1 & | & 0 \\
                        2 & 0 & 1 & | & 0
                    \end{bmatrix},
                \end{equation*}
                which, by row reduction, becomes
                \begin{equation*}
                    \begin{bmatrix}
                        1 & 0 & 0 & | & 0 \\
                        0 & 1 & 0 & | & 0 \\
                        0 & 0 & 1 & | & 0 \\
                        0 & 0 & 0 & | & 0
                    \end{bmatrix}.
                \end{equation*}
                We see that the system only has the trivial solution, so \(S\) is linearly independent.

            \end{example}
            \begin{example}{\Difficulty\,\Difficulty\,\,Determining Linear Dependence in \(\mathbb{R}^n\) 2}{detlindeprn2}
                
                Let \(S=\{[3,1,-1],[-5,-2,2],[2,2,-1]\}\subseteq\mathbb{R}^3\). We build the system
                \begin{equation*}
                    \begin{bmatrix}
                        3 & -5 & 2 & | & 0 \\
                        1 & -2 & 2 & | & 0 \\
                        -1 & 2 & -1 & | & 0 \\
                    \end{bmatrix},
                \end{equation*}
                which, by row reduction, becomes
                \begin{equation*}
                    \begin{bmatrix}
                        1 & 0 & 0 & | & 0 \\
                        0 & 1 & 0 & | & 0 \\
                        0 & 0 & 1 & | & 0
                    \end{bmatrix}.
                \end{equation*}
                We see that the system only has the trivial solution, so \(S\) is linearly independent.

            \end{example}
            \begin{example}{\Difficulty\,\Difficulty\,\,Determining Linear Dependence in \(\mathbb{R}^n\) 3}{detlindeprn3}
                
                Let \(S=\{[2,1],[-1,3],[1,4]\}\subseteq\mathbb{R}^2\). We build the system
                \begin{equation*}
                    \begin{bmatrix}
                        2 & -1 & 1 & | & 0 \\
                        1 & 3 & 4 & | & 0
                    \end{bmatrix},
                \end{equation*}
                which, by row reduction, becomes
                \begin{equation*}
                    \begin{bmatrix}
                        1 & -\frac{1}{2} & \frac{1}{2} & | & 0 \\
                        0 & 1 & 1 & | & 0
                    \end{bmatrix}.
                \end{equation*}
                We see that the system has infinitely many solutions, since the third column gives a free variable. Thus, \(S\) is linearly dependent. Note that the same conclusion is reached by realizing that \(k=|S|=3\), \(n=2\), and \(k>n\).

            \end{example}

        \pagebreak

        \subsection{Bases: Part I}

            We now define the basis of a vector space.
            \begin{definition}{\Stop\,\,Basis}{basis}

                Suppose \(V\) is a vector space. Then, \(B\subseteq V\) is a basis of \(V\) if and only if
                \begin{enumerate}
                    \item \(\Span(B)=V\).
                    \item \(B\) is linearly independent.
                \end{enumerate}

            \end{definition}
            \vphantom
            \\
            \\
            Consider the following examples.
            \begin{example}{\Difficulty\,\Difficulty\,\,The Standard Basis of \(\mathbb{R}^3\)}{stanbasisr3}

                Verify that \(B=\{[1,0,0],[0,1,0],[0,0,1]\}\) is a basis of \(\mathbb{R}^3\).
                \\
                \\
                Let \(A\) be the matrix with the elements of \(B\) as its columns. We form the linear system \([A|\vec{0}]\), or
                \begin{equation*}
                    \begin{bmatrix}
                        1 & 0 & 0 & | & 0 \\
                        0 & 1 & 0 & | & 0 \\
                        0 & 0 & 1 & | & 0
                    \end{bmatrix}
                \end{equation*}
                and realize that it is already in reduced row echelon form. The system has only the trivial solution, and so \(B\) is linearly independent. Now, we consider the span of \(B\), or the row space of \(A^T\). We have
                \begin{equation*}
                    \Span(B)=\{[c_1,c_2,c_3]:c_1,c_2,c_3\in\mathbb{R}^3\}=\mathbb{R}^3.
                \end{equation*}
                Thus, \(B\) is a basis for \(\mathbb{R}^3\).
                
            \end{example}
            \pagebreak
            \begin{example}{\Difficulty\,\Difficulty\,\,Another Basis of \(\mathbb{R}^3\)}{otherbasisr3}

                Verify that \(B=\{[2,2,2],[5,0,0],[0,-3,1]\}\) is a basis of \(\mathbb{R}^3\).
                \\
                \\
                Let \(A\) be the matrix with the elements of \(B\) as its columns. We form the linear system \([A|\vec{0}]\), or
                \begin{equation*}
                    \begin{bmatrix}
                        2 & 5 & 0 & | & 0 \\
                        2 & 0 & -3 & | & 0 \\
                        2 & 0 & 1 & | & 0
                    \end{bmatrix}.
                \end{equation*}
                By row reduction, we obtain
                \begin{equation*}
                    \begin{bmatrix}
                        1 & 0 & 0 & | & 0 \\
                        0 & 1 & 0 & | & 0 \\
                        0 & 0 & 1 & | & 0
                    \end{bmatrix}
                \end{equation*}
                The system has only the trivial solution, and so \(B\) is linearly independent. Now, we consider the span of \(B\), or the row space of \(A^T\). We don't need to compute the row reduction in this case as \(A\) is square. If \(A\) can be row reduced to \(I_n\), \(\det A=\det A^T \neq 0\), meaning that \(A^T\) can be row reduced to \(I_n\). This means
                \begin{equation*}
                    \Span(B)=\{[c_1,c_2,c_3]:c_1,c_2,c_3\in\mathbb{R}^n\}.
                \end{equation*}
                Thus, \(B\) is a basis for \(\mathbb{R}^3\).
            \end{example}
            \begin{example}{\Difficulty\,\Difficulty\,\Difficulty\,\,A Basis of \(\mathcal{M}_{22}\)}{basism22}

                Verify that 
                \begin{equation*}
                    B=\left\{
                    \begin{bmatrix}
                        1 & 0 \\
                        0 & 0
                    \end{bmatrix},
                    \begin{bmatrix}
                        0 & 1 \\
                        0 & 0
                    \end{bmatrix},
                    \begin{bmatrix}
                        0 & 0 \\
                        1 & 0
                    \end{bmatrix},
                    \begin{bmatrix}
                        0 & 0 \\
                        0 & 1
                    \end{bmatrix}
                    \right\}
                \end{equation*}
                is a basis of \(\mathbb{R}^3\).
                \\
                \\
                We start by computing \(\Span(B)\), to produce
                \begin{align*}
                    \Span(B)&=\left\{c_1\begin{bmatrix}
                        1 & 0 \\
                        0 & 0
                    \end{bmatrix}+c_2\begin{bmatrix}
                        0 & 1 \\
                        0 & 0
                    \end{bmatrix}+c_3\begin{bmatrix}
                        0 & 0 \\
                        1 & 0
                    \end{bmatrix}+c_4\begin{bmatrix}
                        0 & 0 \\
                        0 & 1
                    \end{bmatrix}:c_1,c_2,c_3,c_4\in\mathbb{R}\right\} \\
                    &=\left\{\begin{bmatrix}
                        c_1 & c_2 \\
                        c_3 & c_4
                    \end{bmatrix}:c_1,c_2,c_3,c_4\in\mathbb{R}\right\} \\
                    &=\mathcal{M}_{22}.
                \end{align*}
                Then, to verify linear independence, we see that if 
                \begin{equation*}
                    c_1\begin{bmatrix}
                        1 & 0 \\
                        0 & 0
                    \end{bmatrix}+c_2\begin{bmatrix}
                        0 & 1 \\
                        0 & 0
                    \end{bmatrix}+c_3\begin{bmatrix}
                        0 & 0 \\
                        1 & 0
                    \end{bmatrix}+c_4\begin{bmatrix}
                        0 & 0 \\
                        0 & 1
                    \end{bmatrix}=
                    \begin{bmatrix}
                        c_1 & c_2 \\
                        c_3 & c_4
                    \end{bmatrix}=
                    \begin{bmatrix}
                        0 & 0 \\
                        0 & 0
                    \end{bmatrix},
                \end{equation*}
                \(c_1=c_2=c_3=c_4=0\), meaning \(B\) is linearly independent. Thus, \(B\) is a basis for \(\mathcal{M}_{22}\).

            \end{example}
            \begin{example}{\Difficulty\,\Difficulty\,\Difficulty\,\,A Basis of \(\mathcal{P}_n\)}{basispn}

                Verify that \(B=\{x^0,\ldots,x^n\}\) is a basis of \(\mathcal{P}_n\).
                \\
                \\
                We start by computing \(\Span(B)\), to produce
                \begin{align*}
                    \Span(B)&=\{c_0x_0+\cdots+c_nx^n:c_0,\ldots,c_n\in\mathbb{R}^n\} \\
                    &=\mathcal{P}_n.
                \end{align*}
                Then, to verify linear independence, we see that if
                \begin{equation*}
                    c_0x_0+\cdots+c_nx^n=0,
                \end{equation*}
                \(c_0=\cdots=c_n=0\), meaning \(B\) is linearly independent. Thus, \(B\) is a basis for \(\mathcal{P}_n\).
            \end{example}
            \begin{example}{\Difficulty\,\Difficulty\,\Difficulty\,\,A Basis of \(\{\vec{0}\}\)}{basiszero}

                Verify that \(B=\emptyset\) is a basis of \(\{\vec{0}\}\).
                \\
                \\
                We note that \(\Span(B)=\vec{0}\), by Definition \ref{def:span}. Then, we recall that \(B\) is linearly independent, by Definition \ref{def:linindepdep}. Thus, \(B\) is a basis for \(\{\vec{0}\}\).
            
            \end{example}
            \pagebreak
            \begin{example}{\Difficulty\,\Difficulty\,\Difficulty\,\,A Basis of \(\mathcal{D}_3\)}{basisdiag3}

                Find a basis of \(\mathcal{D}_3\), the set of \(3\times 3\) diagonal matrices.
                \\
                \\
                Consider the set
                \begin{equation*}
                    B=\left\{\begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix},\begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix},\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}\right\}.
                \end{equation*}
                To see that \(\Span(B)=\mathcal{D}_3\), consider
                \begin{equation*}
                    A_1=\begin{bmatrix}
                        1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                        0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
                        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1
                    \end{bmatrix}.
                \end{equation*}
                The row space of \(A_1\) is \(\{[c_1,0,0,0,c_2,0,0,0,c_3]:c_1,c_2,c_3\in\mathbb{R}\}\). Thus, \(\Span(B)=\mathcal{D}_3\). Then, to verify that \(B\) is linearly independent, consider
                \begin{equation*}
                    A_2=\begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 0 & 0 \\
                        0 & 0 & 0 \\
                        0 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 0 \\
                        0 & 0 & 0 \\
                        0 & 0 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                \end{equation*}
                which is easily row reduced to a matrix with pivots in each column, so \(B\) is linearly independent and is a basis for \(\mathcal{D}_3\).
                
            \end{example}

\pagebreak

\section{Lecture 26: October 26, 2022}

    \subsection{Bases: Part II}

        Consider the following nonexample of a basis.
        \begin{example}{\Difficulty\,\Difficulty\,\,Not a Basis of \(\mathcal{P}_2\)}{notabasis}

            Verify that \(B=\{1,x^2\}\) is not a basis for \(\mathcal{P}_2\).
            \\
            \\
           We see that \(x\in\mathcal{P}_2\) but \(x\nin\Span(B)\), so \(B\) is not a basis for \(\mathcal{P}_2\).
            \begin{proof}
                Suppose \(x\in\Span(B)\). Then,
                \begin{equation*}
                    x=c_1(1)+c_2(x^2)
                \end{equation*}
                for some \(c_1,c_2\in\mathbb{R}\). If \(x=0\), we have
                \begin{equation*}
                    0=a+b(0)^2\implies a=0.
                \end{equation*} 
                Therefore, we have \(x=bx^2\). After we take the derivative of both sides, we get
                \begin{equation*}
                    1=2bx,
                \end{equation*}
                and when \(x=0\), that implies \(1=0\), and we arrive at a contradiction. Hence, \(x\nin\Span(B)\).
            \end{proof}
            
        \end{example}
        \vphantom
        \\
        \\
        Consider the following theorems.
        \begin{theorem}{\Stop\,\,A Useful Lemma Regarding Bases}{basislemma}

            Let \(S\) and \(T\) be subsets of a vector space \(V\) such that \(\Span(S)=V\) where \(S\) is finite and \(T\) is linearly independent. Then, \(T\) is finite and \(|T|\leq|S|\).
    
        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,Bases Have Equivalent Cardinality}{basisequivcard}

            Suppose \(V\) is a vector space and \(B_1\) is a basis for \(V\) with finitely many elements. If \(B_2\) is a basis for \(V\) with finitely many elements,
            \begin{equation*}
                |B_1|=|B_2|.
            \end{equation*}
            \begin{proof}
                Since \(\Span(B_1)=V\) and \(B_1\) is finite, \(|B_2|\leq|B_1|\). Since \(\Span(B_2)=V\) and \(B_2\) is finite, \(|B_1|\leq|B_2|\). Thus, \(|B_1|=|B_2|\).
            \end{proof}
            
        \end{theorem}
        \vphantom
        \\
        \\
        Now, we come to the definition of the dimension of a vector space.
        \begin{definition}{\Stop\,\,Dimension}{dimension}

            If \(V\) is a vector space with a finite basis \(B\),
            \begin{equation*}
                \dim V = |B|.
            \end{equation*}
            If \(V=\{\vec{0}\}\), \(\dim V=0\). Otherwise, \(\dim V\) is infinite.
            
        \end{definition}
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,General Statements About Span and Dimension 1}{spandim1}
            
            Suppose \(V\) is a vector space. If \(S\subseteq V\) is finite and \(\Span(S)=V\), then, \(\dim V\leq|S|\).
            \begin{proof}
                See Theorem \ref{thm:basislemma} and Definition \ref{def:dimension}.
            \end{proof}

        \end{theorem}
        \begin{theorem}{\Stop\,\,General Statements About Span and Dimension 2}{spandim2}
            
            Suppose \(V\) is a vector space. If \(S\subseteq V\) is finite and \(\Span(S)=V\), then, \(S\) is a basis if and only if \(\dim V = |S|\).
            \begin{proof}
                Suppose \(S\) is a basis. Then, by Definition \ref{def:dimension}, \(\dim V = |S|\). If \(\dim V = |S|\), let \(n=|S|=\dim V\) so \(S=\{\vec{v}_1,\ldots,\vec{v}_n\}\). Since \(\Span(S)=V\), we need only show that \(S\) is linearly independent. Suppose \(S\) is linearly dependent, then, there exists \(\vec{v}_i\) such that
                \begin{equation*}
                    \Span(\{\vec{v}_1,\ldots,\vec{v}_{i-1},\vec{v}_{i+1},\vec{v}_n\})=V.
                \end{equation*}
                This contradicts \(\dim V = n\). 
            \end{proof}

        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,General Statements About Linear Independence and Dimension 1}{lindim1}
            
            Suppose \(V\) is a vector space. If \(T\subseteq V\) is linearly independent, then, \(|T|\leq\dim V\).
            \begin{proof}
                Let \(S\) be a basis of \(V\). Thus, \(\Span(S)=V\) and \(\dim V=|S|\). By Theorem \ref{thm:basislemma}, if \(T\) is linearly independent, \(|T|\leq|S|\), so \(|T|\leq\dim V\).
            \end{proof}
        
        \end{theorem}
        \begin{theorem}{\Stop\,\,General Statements About Linear Independence and Dimension 2}{lindim2}
            
            Suppose \(V\) is a vector space. If \(T\subseteq V\) is linearly independent and finite, \(T\) is a basis for \(V\) if and only if \(\dim V = |T|\).
            \begin{proof}
                Suppose \(T\) is a basis. Then, by Definition \ref{def:dimension}, \(\dim V = |T|\). If \(\dim V = |T|\), Let \(n=|T|=\dim V\) so \(T=\{\vec{v}_1,\ldots,\vec{v}_n\}\). Since \(T\) is linearly independent, we need only show that \(\Span(T)=V\). Suppose not, then, there exists some \(\vec{v}\in V\) where \(\vec{v}\nin \Span(T)\). Consider the set \(T\cup\{\vec{v}\}\). We see that
                \begin{equation*}
                    |T\cup\{\vec{v}\}|=|T|+1=\dim V+1
                \end{equation*}
                since \(\vec{v}\nin\Span(T)\), and because \(T\subseteq\Span(T)\), \(\vec{v}\nin T\). We also see that \(T\cup\{\vec{v}\}\) is linearly independent by considering
                \begin{equation*}
                    \vec{0}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n+c_{n+1}\vec{v}
                \end{equation*}
                for \(c_1,\ldots,c_{n+1}\in\mathbb{F}\) and \(\vec{v}_1,\ldots,\vec{v}_k\in T\). We have that \(c_{n+1}=0\) since, otherwise, 
                \begin{equation*}
                    \vec{v}=-\frac{1}{c_{n+1}}(c_1\vec{v}_1+\cdots+c_n\vec{v}_n).
                \end{equation*}
                Since we've written \(\vec{v}\) as a linear combination of the elements of \(T\), \(\vec{v}\in\Span(T)\), which is contradictory. We also have
                \begin{equation*}
                    \vec{0}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n
                \end{equation*}
                since \(T\) is linearly independent and \(\vec{v}_1,\ldots,\vec{v}_n\in T\). Thus, \(c_1=\cdots=c_n=c_{n+1}=0\). By the previous part, we have \(|T\cup\{\vec{v}\}|\leq\dim V\). However, the left hand side is \(\dim V+1\), which is contradictory. Thus, \(\Span(T)=V\), so \(T\) is a basis for \(V\).
            \end{proof}
        
        \end{theorem}

\pagebreak

\section{Lecture 27: October 28, 2022}

    \subsection{Bases: Part III}

        Consider the following theorems.
        \begin{theorem}{\Stop\,\,Dimensions of Subspaces}{dimsubspc}

            If \(W\) is a subspace of a vector space \(V\),
            \begin{equation*}
                \dim W\leq \dim V.
            \end{equation*}
            Moreover, if \(\dim V\) is finite and \(\dim W=\dim V\), then, \(W=V\).
            \begin{proof}
                Let \(W\) be a subspace of \(V\). If \(W=\{\vec{0}\}\), \(\dim W=0\leq\dim V\). Otherwise, we have that \(\vec{w}_1\in W\) where \(\vec{w}_1\neq\vec{0}\). Then, \(\{\vec{w}_1\}\) is linearly independent. If \(\Span(\{\vec{w}_1\})\neq W\), we can find \(\vec{w}_2\in (W-\Span(\{\vec{w}_1\}))\). Then, \(\{\vec{w}_1,\vec{w}_2\}\) is linearly independent. Then, if our set still doesn't span \(W\), we keep adding \(\vec{w}_{k+1}\in (W-\Span(\{\vec{w}_1,\ldots,\vec{w}_k\}))\) to eventually get a linearly independent set
                \begin{equation*}
                    \{\vec{w}_1,\ldots,\vec{w}_{k+1}\}\in W.
                \end{equation*}
                Since this set is also linearly independent in \(V\) since \(W\subseteq V\), \(\dim W=k+1\leq \dim V\). For the second part of the theorem, suppose \(\dim W=\dim V\). Consider the basis \(B=\{\vec{w}_1,\ldots,\vec{w}_{k+1}\}\) of \(W\). Suppose, for the sake of contradiction that \(W\neq V\). Then, we have \(\vec{v}\in V\) where \(\vec{v}\nin W\). Then, we know that \(B\cup\{\vec{v}\}\) is linearly independent in \(V\). But, this linearly independent set has more elements than \(\dim V\), where it should have at most \(\dim V\) elements. Thus, \(W=V\).
            \end{proof}
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,Diagonalizability, Revisited: Part I}{diagrev1}

            Suppose \(A\in\mathcal{M}_{nn}\). Then, \(A\) is diagonalizable if and only if there exists a basis of \(\mathbb{R}^n\) that consists of eigenvectors of \(A\).
            \begin{proof}
                Recall that by Theorem \ref{thm:diagonalizability}, \(A\in\mathcal{M}_{nn}\) is diagonalizable if and only if there exists a set of \(n\) linearly indpendent eigenvectors \(\vec{v}_1,\ldots,\vec{v}_n\). Let \(V=\mathbb{R}^n\) and \(W=\{\vec{v}_1,\ldots,\vec{v}_n\}\). Then, \(\dim V=n\). We also see that \(W\subseteq V\) is linearly independent and finite with \(|W|=\dim V\). By Theorem \ref{thm:lindim2}, \(W\) is a basis for \(V\). Our proof is reversible, as all implications are bidirectional.
            \end{proof}

        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        We now posit two questions. Given \(S\subseteq V\), where \(V\) is a vector space, how would one construct a basis for \(\Span(S)\)? Similarly, given a linearly independent set \(S\) where \(S\subseteq V\) and \(V\) is a vector space, how can we expand \(S\) to form a basis of \(V\)? Consider the following theorems.
        \begin{theorem}{\Stop\,\,Finding a Basis for \(\Span(S)\) by Contraction}{basisforspan}
            
            If \(\Span(S)=V\), where \(V\) is a finite dimensional vector space, there exists some \(B\subseteq S\) where \(B\) is a basis for \(V\). Given \(S=\{\vec{v}_1,\ldots,\vec{v}_k\}\subseteq\mathbb{R}^n\). Consider the following steps.
            \begin{enumerate}
                \item We form the matrix \(A\) with the elements of \(S\) as columns.
                \item We row reduce \(A\) into reduced row echelon form to obtain a matrix \(C\).
                \item The basis of \(\Span(S)\), \(B\), is formed by removing the vectors in \(A\) associated with the columns containing free variables.
            \end{enumerate}
            \vphantom
            \\
            \\
            Note that the vectors in \(B\) are in \(S\).
            \\
            \\
            Note that we can also construct a basis for \(\Span(S)\) by forming a matrix with the elements of \(S\) as rows and finding the row space. In general though, the basis will not contain vectors in \(S\).

        \end{theorem}
        \begin{theorem}{\Stop\,\,Finding a Basis by Expansion}{basisexpand}

            Let \(T\) be a linearly independent subset of a finite dimensional vector space \(V\). Then, \(V\) has a basis \(B\) with \(T\subseteq B\). If \(T=\{\vec{t}_1,\ldots,\vec{t}_k\}\subseteq V\), perform the following steps to find \(B\):
            \begin{enumerate}
                \item Find a spanning set \(A=\{\vec{a}_1,\ldots,\vec{a}_n\}\). For \(\mathbb{R}^n\), this will often be \(\{\vec{e}_1,\ldots,\vec{e}_n\}\).
                \item Form the ordered spanning set \(S=\{\vec{t}_1,\ldots,\vec{t}_k,\vec{a}_1,\ldots,\vec{a}_n\}\) for \(V\).
                \item Use Theorem \ref{thm:basisforspan} to find a basis \(B\) for \(V\), containing \(T\).
            \end{enumerate}

        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Basis 1}{basis1}

            Let \(S=\{[1,2,0,1],[3,1,5,-7],[-2,4,-8,14]\}\). We form the matrix \(A\) with the elements of \(S\) as columns, giving
            \begin{equation*}
                A=\begin{bmatrix}
                    1 & 3 & -2 \\ 
                    2 & 1 & 4 \\
                    0 & 5 & -8 \\
                    1 & -7 & 14
                \end{bmatrix}.
            \end{equation*}
            By row reduction, we get
            \begin{equation*}
                C=\begin{bmatrix}
                    1 & 0 & \frac{14}{5} \\
                    0 & 1 & -\frac{8}{5} \\
                    0 & 0 & 0 \\
                    0 & 0 & 0 
                \end{bmatrix}.
            \end{equation*} 
            Now, we remove the vectors associated with the columns containing free variables to get the basis. Our basis is then
            \begin{equation*}
                B=\{[1,2,0,1],[3,1,5,-7]\}.
            \end{equation*}
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Basis 2}{basis2}

            Let \(S=\{x^3-3x^2+2,2x^3-7x^2+x-3,4x^3-13x^2+x+5\}\). We instead equivalently consider \(S'=\{[2,0,-3,1],[-3,1,-7,2],[5,1,-13,4]\}\). We form the matrix \(A\) with the elements of \(S'\) as columns, giving
            \begin{equation*}
                A=\begin{bmatrix}
                    1 & -3 & 5 \\
                    0 & 1 & 1 \\
                    -3 & -7 & -13 \\
                    1 & 2 & 4
                \end{bmatrix}.
            \end{equation*}
            By row reduction, we get
            \begin{equation*}
                C=\begin{bmatrix}
                    1 & 0 & 0 \\
                    0 & 1 & 0 \\
                    0 & 0 & 1 \\
                    0 & 0 & 0 
                \end{bmatrix}.
            \end{equation*} 
            There are no vectors to remove and we have \(B=S\).
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Basis 3}{basis3}

            Let \(S=\{x^3-8x^2+1,3x^3-2x^2+x,4x^3+2x-10,x^3-20x^2-x+12,x^3+24x^2+2x-13\}\). We instead equivalently consider \(S'=\{[1,0,-8,1],[0,1,-2,3],[-10,2,0,4],[12,-1,-20,1],[-13,2,24,1]\}\). We form the matrix \(A\) with the elements of \(S'\) as columns, giving
            \begin{equation*}
                A=\begin{bmatrix}
                   1 & 0 & -10 & 12 & -13 \\
                   0 & 1 & 2 & -1 & 2 \\
                   -8 & -2 & 0 & -20 & 24 \\
                   1 & 3 & 4 & 1 & 1
                \end{bmatrix}.
            \end{equation*}
            By row reduction, we get
            \begin{equation*}
                C=\begin{bmatrix}
                    1 & 0 & 0 & 0 & -3 \\
                    0 & 1 & 0 & 0 & 0 \\
                    0 & 0 & 1 & 0 & 1 \\
                    0 & 0 & 0 & 1 & 0
                \end{bmatrix}.
            \end{equation*} 
            We remove the last vector to get \(B=\{x^3-8x^2+1,3x^3-2x^2+x,4x^3+2x-10,x^3-20x^2-x+12\}\).
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Basis 4}{basis4}

            Let \(T=\{[1,2,0,1],[3,1,5,-7]\}\). We consider 
            \begin{equation*}
                S=\{[1,2,0,1],[3,1,5,-7],[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]\}
            \end{equation*}
            Note that \(S\) is the union of the standard basis of \(\mathbb{R}^4\) and \(T\). Since
            \begin{equation*}
                \{[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]\}
            \end{equation*}
            is a basis of \(\mathbb{R}^4\), \(\Span(S)=\mathbb{R}^4\). Now, we form
            \begin{equation*}
                A=\begin{bmatrix}
                    1 & 3 & 1 & 0 & 0 & 0 \\
                    2 & 3 & 0 & 1 & 0 & 0 \\
                    0 & 5 & 0 & 0 & 1 & 0 \\
                    1 & -7 & 0 & 0 & 0 & 1
                \end{bmatrix}
            \end{equation*}
            and row reduce to
            \begin{equation*}
                C=\begin{bmatrix}
                    1 & 0 & 0 & 0 & \frac{7}{5} & 1 \\
                    0 & 1 & 0 & 0 & \frac{1}{5} & 0 \\
                    0 & 0 & 1 & 0 & -2 & -1 \\
                    0 & 0 & 0 & 1 & -3 & 2
                \end{bmatrix}.
            \end{equation*}
            Hence, our basis for \(\mathbb{R}^4\) is
            \begin{equation*}
                B=\{[1,2,0,1],[3,1,5,-7],[1,0,0,0],[0,1,0,0]\}.
            \end{equation*}
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Basis 5}{basis5}

            Let \(T=\{[2,0,4,-12],[0,-1,-3,9]\}\). We consider 
            \begin{equation*}
                S=\{[2,0,4,-12],[0,-1,-3,9],[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]\}
            \end{equation*}
            Note that \(S\) is the union of the standard basis of \(\mathbb{R}^4\) and \(T\). Since
            \begin{equation*}
                \{[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]\}
            \end{equation*}
            is a basis of \(\mathbb{R}^4\), \(\Span(T)=\mathbb{R}^4\). Now, we form
            \begin{equation*}
                A=\begin{bmatrix}
                    2 & 0 & 1 & 0 & 0 & 0 \\
                    0 & -1 & 0 & 1 & 0 & 0 \\
                    4 & -3 & 0 & 0 & 1 & 0 \\
                    -12 & 9 & 0 & 0 & 0 & 1
                \end{bmatrix}
            \end{equation*}
            and row reduce to
            \begin{equation*}
                C=\begin{bmatrix}
                    1 & 0 & 0 & -\frac{3}{4} & 0 & -\frac{1}{12} \\
                    0 & 1 & 0 & -1 & 0 & 0 \\
                    0 & 0 & 1 & \frac{3}{2} & 0 & \frac{1}{6} \\
                    0 & 0 & 0 & 0 & 1 & \frac{1}{3}
                \end{bmatrix}.
            \end{equation*}
            Hence, our basis for \(\mathbb{R}^4\) is
            \begin{equation*}
                B=\{[2,0,4,-12],[0,-1,-3,9],[1,0,0,0],[0,0,1,0]\}.
            \end{equation*}
        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        We end this section with two examples on finding bases of ``weird'' sets.
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Weird Basis 1}{weirdbasis1}
            
            Find the basis and the dimension of \(V=\{[x,y,z,w]:x,y,z,w\in\mathbb{R}, x-y-w+z=0\}\).
            \\
            \\
            The membership condition of the set indicates that \(w=x-y+z\). We then have
            \begin{equation*}
                V=\{[x,y,z,x-y+z]:x,y,z\in\mathbb{R}\}.
            \end{equation*}
            We hypothesize that the dimension of \(V\) will be \(3\), because there are only three variables making up the set; however, we will confirm this after we have found the basis. We form, somewhat arbitrarily, the set
            \begin{equation*}
                S=\{[1,1,1,1],[1,1,2,2],[1,2,1,0]\}.
            \end{equation*}
            To find \(\Span(S)\), we consider 
            \begin{equation*}
                \begin{bmatrix}
                    1 & 1 & 1 & 1 \\
                    1 & 1 & 2 & 2 \\
                    1 & 2 & 1 & 0
                \end{bmatrix}\underbrace{\to}_{\text{RREF}}
                \begin{bmatrix}
                    1 & 0 & 0 & 1 \\
                    0 & 1 & 0 & -1 \\
                    0 & 0 & 1 & 1
                \end{bmatrix}.
            \end{equation*}
            Thus,
            \begin{equation*}
                \Span(S)=\{[c_1,c_2,c_3,c_1-c_2+c_3]:c_1,c_2,c_3\in\mathbb{R}\}=V.
            \end{equation*}
            Since \(\Span(S)=V\), we check if \(S\) is linearly independent by considering
            \begin{equation*}
                \begin{bmatrix}
                    1 & 1 & 1 \\
                    1 & 1 & 2 \\
                    1 & 2 & 1 \\
                    1 & 2 & 0
                \end{bmatrix}\underbrace{\to}_{\text{RREF}}
                \begin{bmatrix}
                    1 & 0 & 0 \\
                    0 & 1 & 0 \\
                    0 & 0 & 1 \\
                    0 & 0 & 0
                \end{bmatrix}.
            \end{equation*}
            All columns have pivots, so \(S\) is linearly independent, spans \(V\), and is therefore a basis for \(V\). Because there are \(3\) elements in \(S\), \(\dim V=3\).
        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Weird Basis 2}{weirdbasis1}
            
            Find the basis and the dimension of 
            \begin{equation*}
                V=\{a_0+a_1x+a_2x^2+a_3x^3:a_0,a_1,a_2,a_3\in\mathbb{R},2a_0+a_1=0,a_2-2a_3=0\}
            \end{equation*}
            The membership condition of the set indicates that \(a_1=-2a_0\) and \(a_3=\frac{1}{2}a_2\). We then have
            \begin{equation*}
                V=\{a_0-2a_0x+a_2x^2+\frac{1}{2}a_2x^3:a_0,a_2\in\mathbb{R}\}.
            \end{equation*}
            We hypothesize that the dimension of \(V\) will be \(2\), because there are only two variables making up the set; however, we will confirm this after we have found the basis. We form, somewhat arbitrarily, the set
            \begin{equation*}
                S=\{1-2x+2x^2+x^3,1-2x+4x^2+2x^3\}.
            \end{equation*}
            Consider \(S'=\{[1,-2,2,1],[1,-2,4,2]\}\). To find \(\Span(S')\), we consider
            \begin{equation*}
                \begin{bmatrix}
                    1 & -2 & 2 & 1 \\
                    1 & -2 & 4 & 2
                \end{bmatrix}\underbrace{\to}_{\text{RREF}}
                \begin{bmatrix}
                    1 & -2 & 0 & 0 \\
                    0 & 0 & 1 & \frac{1}{2}
                \end{bmatrix}
            \end{equation*} 
            so \(\Span(S')=\{[c_1,-2c_1,c_2,\frac{1}{2}c_2]:c_1,c_2\in\mathbb{R}\}\) and 
            \begin{equation*}
                \Span(S)=\left\{c_1-2c_1x+c_2x^2+\frac{1}{2}c_2x^3\right\}=V.
            \end{equation*}
            Since \(\Span(S)=V\), we check if \(S\) is linearly independent by considering
            \begin{equation*}
                \begin{bmatrix}
                    1 & 1 \\
                    -2 & -2 \\
                    2 & 4 \\
                    1 & 2
                \end{bmatrix}\underbrace{\to}_{\text{RREF}}
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1 \\
                    0 & 0 \\
                    0 & 0
                \end{bmatrix}.
            \end{equation*}
            All columns have pivots, so \(S\) is linearly independent, spans \(V\), and is therefore a basis for \(V\). Since there are \(2\) elements in \(S\), \(\dim V=2\).

        \end{example}

\pagebreak

\section{Lecture 28: October 31, 2022}

    \subsection{Coordinatization: Part I}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Ordered Bases}{orderedbases}
            
            An ordered basis of an \(n\) dimensional vector space \(V\) is an \(n\)-tuple of vectors \((\vec{v}_1,\ldots,\vec{v}_n)\) such that the set \(\{\vec{v}_1,\ldots,\vec{v}_n\}\) is a basis for \(V\).

        \end{definition}
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Ordered Bases of \(\mathbb{R}^3\)}{ordbasesr3}

            The tuples
            \begin{equation*}
                B_1=([1,0,0],[0,1,0],[0,0,1])
            \end{equation*}
            and
            \begin{equation*}
                B_2=([0,1,0],[1,0,0],[0,0,1])
            \end{equation*}
            are two distinct ordered bases of \(\mathbb{R}^3\).

        \end{example}
        \vphantom
        \\
        \\
        The concept of ordering allows us to define coordinates with respect to a basis.
        \begin{definition}{\Stop\,\,Coordinates With Respect to a Basis}{coordswrtbasis}
            
            Let \(B=(\vec{v}_1,\ldots,\vec{v}_n)\) be an ordered basis for a vector space \(V\). Suppose that
            \begin{equation*}
                \vec{w}=c_1\vec{v}_1+\cdots+c_n\vec{v}_n\in V
            \end{equation*}
            for \(c_1,\ldots,c_n\in\mathbb{F}\). Then, 
            \begin{equation*}
                [\vec{w}]_B=[c_1,\ldots,c_n].
            \end{equation*}
            The quantity \([\vec{w}]_B\) is the coordinatization of \(\vec{w}\) with respect to \(B\), or equivalently, ``\(\vec{w}\) expressed in \(B\) coordinates.''

        \end{definition}
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\,Coordinatization 1}{coords1}

            Let \(\vec{w}=[1,2,3]\in\mathbb{R}^3\). Then,
            \begin{equation*}
                [\vec{w}]_{B_1=([1,0,0],[0,1,0],[0,0,1])}=[1,2,3].
            \end{equation*}
            
        \end{example}
        \begin{example}{\Difficulty\,\,Coordinatization 2}{coords2}

            Let \(\vec{w}=[1,2,3]\in\mathbb{R}^3\). Then,
            \begin{equation*}
                [\vec{w}]_{B_2=([0,1,0],[1,0,0],[0,0,1])}=[2,1,3].
            \end{equation*}
            
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Coordinatization 3}{coords3}

            Find the matrix associated to
            \begin{equation*}
                [-2,6,-1,-11]_B
            \end{equation*}
            where
            \begin{equation*}
                B=\left(\begin{bmatrix} 1 & -1 \\ 0 & 0 \end{bmatrix},\begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix},\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix},\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}\right).
            \end{equation*}
            \\
            \\
            We have the matrix
            \begin{equation*}
                -2\begin{bmatrix} 1 & -1 \\ 0 & 0 \end{bmatrix}+6\begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix}-1\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}-11\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}=\begin{bmatrix} 4 & -9 \\ -5 & -1 \end{bmatrix}.
            \end{equation*}

        \end{example}
        \vphantom
        \\
        \\
        We now present the general algorithm for find the coordinatization of a vector with respect to an ordered basis.
        \begin{theorem}{\Stop\,\,Coordinatization}{coords}

            Let \(V\) be a nontrivial subspace of \(\mathbb{R}^n\) and let \(B=(\vec{v}_1,\ldots,\vec{v}_k)\) be an ordered basis for \(V\), and let \(\vec{v}\in\mathbb{R}^n\). The following steps will find \([\vec{v}]_B\), if it exists:
            \begin{enumerate}
                \item Form the system \([A|\vec{v}]\) by using the vectors in \(B\) as the columns for \(A\), in order. Use \(\vec{v}\) as a column as well.
                \item Row reduce to obtain the system \([C|\vec{w}]\) in reduced row echelon form.
                \item If the system has no solutions, \(\vec{v}\nin\Span(B)\) and coordinatization is not possible. We note that \(B\) is not a basis if this happens.
                \item Otherwise, \(\vec{v}\in\Span(B)=V\). Eliminate all zeroes consisting solely of zeroes in \([C|\vec{w}]\) to obtain \([I_k|\vec{y}]\). Then, \([\vec{v}]_B=\vec{y}\).
            \end{enumerate}

        \end{theorem}
        \begin{theorem}{\Stop\,\,Properties of Coordinatization}{propcoords}

            Let \(B=(\vec{v}_1,\ldots,\vec{v}_n)\) be an ordered basis of \(V\) and \(\vec{w}_1,\ldots,\vec{w}_k\in V\) and \(c_1,\ldots,c_k\in\mathbb{F}\). Then,
            \begin{enumerate}
                \item \([\vec{w}_1+\vec{w}_2]_B=[\vec{w}_1]_B+[\vec{w}_2]_B\).
                \item \([c_1\vec{w}_1]_B=c_1[\vec{w}_1]_B\).
                \item \([c_1\vec{w}_1+\cdots+c_k\vec{w}_k]_B=c_1[\vec{w}_1]_B+\cdots+c_k[\vec{w}_k]_B\).
            \end{enumerate}
            
        \end{theorem}

\section{Lecture 29: November 2, 2022}

    \subsection{Coordinatization: Part II}
        
        We now posit a question: Given two ordered bases \(B_1\) and \(B_2\), how are \([\vec{v}]_{B_1}\) and \([\vec{v}]_{B_2}\) related? Consider the following definition and theorem.
        \begin{definition}{\Stop\,\,Transition Matrices}{transitionmatrix}

            Suppose that \(V\) is a nontrivial vector space with \(\dim V=n\). Let \(B_1\) and \(B_2\) be ordered bases of \(V\). Let \(P\) be the \(n\times n\) matrix whose \(i\)th column, for \(1\leq i\leq n\), equals \([\vec{b}_{1i}]_{B_2}\), where \(\vec{b}_{1i}\) is the \(i\)th basis vector in \(B_1\). Then, \(P\) is called the transition matrix from \(B_1\) to \(B_2\).
            
        \end{definition}
        \begin{theorem}{\Stop\,\,Finding a Transition Matrix}{transitionmatrix}

            Given \(B_1=(\vec{v}_1,\ldots,\vec{v}_n)\) and \(B_2=(\vec{w}_1,\ldots,\vec{w}_n)\). To go from \(B_1\) to \(B_2\), we form
            \begin{equation*}
                \begin{bmatrix}
                    \vec{w}_1 & \cdots & \vec{w}_n & | & \vec{v}_1 & \cdots & \vec{v}_n
                \end{bmatrix}
            \end{equation*}
            and row reduce to get \([I_n|P]\). The matrix \(P\) is the transition matrix from the coordinatization with respect to \(B_1\) to the coordinatization with respect to \(B_2\). Then,
            \begin{equation*}
                P[\vec{v}]_{B_1}=[\vec{v}]_{B_2}.
            \end{equation*}
            
        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Finding a Transition Matrix}{transitionmatrix}

            Let
            \begin{equation*}
                B_1=([1,1,0,1],[1,0,0,1],[0,1,0,1],[1,1,1,0]),\quad B_2=([1,0,0,0],[1,1,0,0],[1,1,0,1],[1,1,1,1]).
            \end{equation*}
            Find the associated transition matrix \(P\).
            \\
            \\
            We form
            \begin{equation*}
                \begin{bmatrix}
                    1 & 1 & 1 & 1 & | & 1 & 1 & 0 & 1 \\
                    0 & 1 & 1 & 1 & | & 1 & 0 & 1 & 1 \\
                    0 & 0 & 0 & 1 & | & 0 & 0 & 0 & 1 \\
                    0 & 0 & 1 & 1 & | & 1 & 1 & 1 & 0
                \end{bmatrix}
            \end{equation*}
            and row reduce to get
            \begin{equation*}
                \begin{bmatrix}
                    1 & 0 & 0 & 0 & | & 0 & 1 & -1 & 0 \\
                    0 & 1 & 0 & 0 & | & 0 & -1 & 0 & 1 \\
                    0 & 0 & 1 & 0 & | & 1 & 1 & 1 & -1 \\
                    0 & 0 & 0 & 1 & | & 0 & 0 & 0 & 1
                \end{bmatrix}.
            \end{equation*}
            Thus, 
            \begin{equation*}
                P=\begin{bmatrix}
                    0 & 1 & -1 & 0 \\
                    0 & -1 & 0 & 1 \\
                    1 & 1 & 1 & -1 \\
                    0 & 0 & 0 & 1
                \end{bmatrix}.
            \end{equation*}
            
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Checking a Transition Matrix}{check}

            Check that the \(P\) found in Example \ref{exa:transitionmatrix} yields the correct result for 
            \begin{equation*}
                \vec{v}_1=[1,1,0,1].
            \end{equation*}
            We see that \([\vec{v}_1]_{B_1}=[1,0,0,0]\). Then, we have
            \begin{align*}
                P[\vec{v}_1]_{B_1}&=\begin{bmatrix}
                    0 & 1 & -1 & 0 \\
                    0 & -1 & 0 & 1 \\
                    1 & 1 & 1 & -1 \\
                    0 & 0 & 0 & 1
                \end{bmatrix}\begin{bmatrix}
                    1 \\ 0 \\ 0 \\ 0
                \end{bmatrix} \\
                &=\begin{bmatrix}
                    0 \\ 0 \\ 1 \\ 0
                \end{bmatrix} \\
                &=[\vec{v}_1]_{B_2},
            \end{align*}
            as desired.
            
        \end{example}
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Properties of Transition Matrices}{proptransmat}

            Suppose \(B_1\), \(B_2\), and \(B_3\) are ordered bases of \(\mathbb{R}^n\). Then,
            \begin{enumerate}
                \item The transition matrix from \(B_1\) to \(B_2\) is unique and nonsingular.
                \item If \(P\) is the transition matrix from \(B_1\) to \(B_2\) and \(Q\) is the transition matrix from \(B_2\) to \(B_3\), \(QP\) is the transition matrix from \(B_1\) to \(B_3\).
                \begin{proof}
                    We consider 
                    \begin{align*}
                        QP([\vec{v}_{B_1}])&=Q(P[\vec{v}]_{B_1}) \\
                        &=Q[\vec{v}]_{B_2} \\
                        &=[\vec{v}]_{B_3}.
                    \end{align*}
                    Then, uniqueness implies we have the correct transition matrix \(QP\).
                \end{proof}
                \item If \(P\) is the transition matrix from \(B_1\) to \(B_2\), \(P^{-1}\) is the transition matrix from \(B_2\) to \(B_1\).
                \begin{proof}
                    Uniqueness implies that the transition matrix from \(B_1\) to \(B_1\) is \(I_n\). By existence, we have \(P\), the transition matrix from \(B_1\) to \(B_2\) and \(Q\), the transition matrix from \(B_2\) to \(B_1\). We know that
                    \begin{equation*}
                        QP=I_n,
                    \end{equation*}
                    which only occurs if \(Q=P^{-1}\).
                \end{proof}
            \end{enumerate}
            
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        We now revisit diagonalization, yet again.
        \begin{theorem}{\Stop\,\,Diagonalizability, Revisited: Part II}{diagrev2}

            Suppose \(A\in\mathcal{M}_{nn}\) and we have found nonsingular \(P\) with
            \begin{equation*}
                D=P^{-1}AP.
            \end{equation*}
            To find \(P\), we have
            \begin{equation*}
                P=\begin{bmatrix}
                    \vec{v}_1 & \cdots & \vec{v}_n
                \end{bmatrix}.
            \end{equation*}
            By Theorem \ref{thm:diagrev1}, \(B=(\vec{v}_1,\ldots,\vec{v}_n)\) is an ordered basis of \(\mathbb{R}^n\). Then, \(P\) is the transition matrix from \(B\) to the standard basis.
            
        \end{theorem}