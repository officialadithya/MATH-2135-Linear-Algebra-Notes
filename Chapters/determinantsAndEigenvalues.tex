\section{Lecture 13: September 21, 2022}

    \subsection{Defining the Determinant}

        Consider the following theorems and definitions.
        \begin{theorem}{\Stop\,\,The Determinant Determines the Area in \(\mathbb{R}^2\)}{areadet}

            Consider \(\vec{x}=[x_1,x_2]\) and \(\vec{y}=[y_1,y_2]\). If we form
            \begin{equation*}
                A=\begin{bmatrix}
                    x_1 & x_2 \\
                    y_1 & y_2 
                \end{bmatrix},
            \end{equation*}
            \begin{equation*}
                |\det A| = ||\vec{x}\times\vec{y}||.
            \end{equation*}
            That is, \(|\det A|\) provides the area of the parallelogram determined by \(\vec{x}\) and \(\vec{y}\).
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,The Determinant Determines the Volume in \(\mathbb{R}^3\)}{voldet}

            Consider \(\vec{x}=[x_1,x_2,x_3]\), \(\vec{y}=[y_1,y_2,y_3]\), and \(\vec{z}=[z_1,z_2,z_3]\).
            If we form
            \begin{equation*}
                A=\begin{bmatrix}
                    x_1 & x_2 & x_3 \\
                    y_1 & y_2 & y_3 \\
                    z_1 & z_2 & z_3
                \end{bmatrix},
            \end{equation*}
            \begin{equation*}
                |\det A| = \vec{x}\cdot(\vec{y}\times\vec{z}).
            \end{equation*}
            That is, \(|\det A|\) provides the volume of the parallelepiped determined by \(\vec{x}\), \(\vec{y}\), and \(\vec{z}\).
            
        \end{theorem}
        \begin{definition}{\Stop\,\,The \((i,j)\) Submatrix}{submatrix}

            Suppose \(A\in\mathcal{M}_{nn}\). The \((i,j)\) submatrix of \(A\) is the \((n-1)\times(n-1)\) matrix obtained by removing the \(i\)th row and the \(j\)th column. We denote this by \(A_{(i,j)}\) 
            
        \end{definition}
        \begin{definition}{\Stop\,\,The \((i,j)\) Minor}{minor}

            Suppose \(A\in\mathcal{M}_{nn}\). The \((i,j)\) minor of \(A\) is the determinant of the \((i,j)\) submatrix of \(A\).
            
        \end{definition}
        \begin{definition}{\Stop\,\,The \((i,j)\) Cofactor}{cofactor}

            Suppose \(A\in\mathcal{M}_{nn}\). The \((i,j)\) cofactor of \(A\) is
            \begin{equation*}
                A_{ij}=(-1)^{i+j}\det (A_{(i,j)}).
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,The Determinant}{det}
            Suppose \(A\in\mathcal{M}_{nn}\). Then,
            \begin{enumerate}
                \item If \(n=1\), and \(A=\begin{bmatrix} a_{11} \end{bmatrix}\), \(\det A = a_{11}\).
                \item If \(n=2\), and \(A=\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}\), \(\det A = a_{11}a_{22}-a_{12}a_{21}\).
                \item If \(n>2\), and \(A=\begin{bmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn} \end{bmatrix}\), \(\det A = (a_{11}A_{11}+\cdots+a_{1n}A_{1n})+\cdots+(a_{n1}A_{n1}+\cdots+a_{nn}A_{nn})\).
            \end{enumerate}
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        For fun, consider the following Python 3 implementation of computing the determinant of any \(n\times n\) matrix.
        \lstinputlisting[language=Python]{Graphics/matrixdet.py}
        
        \pagebreak

\section{Lecture 14: September 23, 2022}

    \subsection{Determinants of Upper Triangular Matrices}

        Consider the following theorems.
        \begin{theorem}{\Stop\,\,The Determinant of an Upper Triangular Matrix}{uppertriangulardet}

            If \(A\in\mathcal{M}_{nn}\) is upper triangular, 
            \begin{equation*}
                \det A = a_{11} a_{22} \cdots a_{nn}.
            \end{equation*}
            Recall that \(A\) is upper triangular if and only if all elements below the main diagonal are zero.
            \begin{proof}
                We proceed by induction on \(n\). For \(n=1\), \(A=\begin{bmatrix} a_{11} \end{bmatrix}\), so \(\det A = a_{11}\). Suppose for all \(k\in\mathbb{N}\), and some upper triangular \(A\in\mathcal{M}_{kk}\),
                \begin{equation*}
                    a_{11} a_{22} \cdots a_{kk}.
                \end{equation*}
                Consider 
                \begin{equation*}
                    B=\begin{bmatrix}
                        b_{11} & \cdots & b_{1(k+1)} \\
                        \vdots & \ddots & \vdots \\
                        0 & \cdots & b_{(k+1)(k+1)}
                    \end{bmatrix}.
                \end{equation*}
                Then, we compute \(\det B\) using the last row as our ``first row.''
                \begin{align*}
                    \det B &= (-1)^{1+1}b_{11}\det\begin{bmatrix} b_{22} & b_{23} & \cdots & a_{2(k+1)} \\ 0 & b_{33} & \cdots & b_{3(k+1)} \\ \vdots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & b_{(k+1)(k+1)} \end{bmatrix} \\
                    &= b_{11}\underbrace{b_{22}\cdots b_{(k+1)(k+1)}}_{\text{By the inductive hypothesis.}}.
                    %\begin{comment}\det B&= 0+\cdots+0+a_{(k+1)(k+1)}(-1)^{(k+1)+(k+1)}\det B_{(k+1),(k+!)} \\ &=a_{(k+1)(k+1)}(-1)^{2(k+1)}\det A.\end{comment}
                \end{align*}
                This is precisely the stipulation of the theorem when \(n=k+1\).
            \end{proof}

        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,Determinants and Row Operations}{detrowops}

            Suppose \(A\in\mathcal{M}_{nn}\) and let \(R\) be a row operation. Then,
            \begin{enumerate}
                \item If \(R\) is \(c\langle i\rangle\to\langle i \rangle\) for some \(c\in\mathbb{R}\), 
                \begin{equation*}
                    \det R(A) = c\det A.
                \end{equation*}
                Note that if \(c=0\), \(R\) is not a valid row operation.
                \item If \(R\) is \(c\langle i\rangle+\langle j \rangle\to\langle j \rangle\) for some \(c\in\mathbb{R}\),
                \begin{equation*}
                    \det R(A) = \det A.
                \end{equation*}
                Note that if \(c=0\), \(R\) is not a valid row operation.
                \item If \(R\) is \(\langle i \rangle\leftrightarrow\langle j \rangle\),
                \begin{equation*}
                    \det R(A) = -\det A.
                \end{equation*}
            \end{enumerate}
            \vphantom
            \\
            \\
            Note that if \(\det A\neq 0\) and \(R\) is a row operation, \(\det R(A)\neq 0\).
        \end{theorem}
        \vphantom
        \\
        \\
        We can use Theorem \ref{thm:detrowops} in conjunction with row operations to compute the determinant of a matrix easily. We simply use row operations to create an upper triangular matrix, while keeping track of how the determinant changes. We then apply Theorem \ref{thm:uppertriangulardet}. Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Computing a Determinant by Row Reduction}{compdetrowred}
            
            Compute \(\det\begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & -2 \\ 4 & 9 & 4 \end{bmatrix}\).
            \\
            \\
            Let 
            \begin{equation*}
                A_0=\begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & -2 \\ 4 & 9 & 4 \end{bmatrix}.
            \end{equation*}
            Consider the following table.
            \begin{center}
                \begin{tabular}{||c|c|c||}
                    \hline
                    Row Operation & Resultant Matrix & Effect on the Determinant \\
                    \hline
                    \hline
                    \(-2\langle1\rangle+\langle2\rangle\to\langle2\rangle\) & \(A_1=\begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & -4 \\ 4 & 9 & 4 \end{bmatrix}\) & \(\det A_1=\det A_0\) \\
                    \hline
                    \(-4\langle1\rangle+\langle3\rangle\to\langle3\rangle\) & \(A_2=\begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & -4 \\ 0 & 5 & 0 \end{bmatrix}\) & \(\det A_2=\det A_1\) \\
                    \hline
                    \(-5\langle2\rangle+\langle3\rangle\to\langle3\rangle\) & \(A_3=\begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & -4 \\ 0 & 0 & 20 \end{bmatrix}\) & \(\det A_3=\det A_2\) \\
                    \hline
                \end{tabular}
            \end{center}
            \vphantom
            \\
            \\
            Thus, \(\det A_0=20\).

        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Computing a Determinant by Row Reduction}{compdetrowred}
            
            Compute \(\det\begin{bmatrix} 0 & -14 & -8 \\ 1 & 3 & 2 \\ -2 & 0 & 6 \end{bmatrix}\).
            \\
            \\
            Let 
            \begin{equation*}
                A_0=\begin{bmatrix} 0 & -14 & -8 \\ 1 & 3 & 2 \\ -2 & 0 & 6 \end{bmatrix}.
            \end{equation*}
            Consider the following table.
            \begin{center}
                \begin{tabular}{||c|c|c||}
                    \hline
                    Row Operation & Resultant Matrix & Effect on the Determinant \\
                    \hline
                    \hline
                    \(\langle2\rangle\leftrightarrow\langle1\rangle\) & \(A_1=\begin{bmatrix} 1 & 3 & 2 \\ 0 & -14 & -8 \\ -2 & 0 & 6 \end{bmatrix}\) & \(\det A_1=-\det A_0\) \\
                    \hline
                    \(2\langle1\rangle+\langle3\rangle\to\langle3\rangle\) & \(A_2=\begin{bmatrix} 1 & 3 & 2 \\ 0 & -14 & -8 \\ 0 & 6 & 10 \end{bmatrix}\) & \(\det A_2=\det A_1\) \\
                    \hline
                    \(-\frac{1}{14}\langle2\rangle\to\langle2\rangle\) & \(A_3=\begin{bmatrix} 1 & 3 & 2 \\ 0 & 1 & \frac{4}{7} \\ 0 & 6 & 10 \end{bmatrix}\) & \(\det A_3=-\frac{1}{14}\det A_2\) \\
                    \hline
                    \(-6\langle2\rangle+\langle3\rangle\to\langle3\rangle\) & \(A_3=\begin{bmatrix} 1 & 3 & 2 \\ 0 & 1 & \frac{4}{7} \\ 0 & 0 & \frac{46}{7} \end{bmatrix}\) & \(\det A_4=\det A_3\) \\
                    \hline
                \end{tabular}
            \end{center}
            \vphantom
            \\
            \\
            Thus, \(\det A_0=\frac{46}{7}(-14)(-1)=92\).

        \end{example}
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Inverses and Determinants}{invdet}
            
            Suppose that \(A\in\mathcal{M}_{nn}\). Then, \(A\) is nonsingular if and only if \(\det A \neq 0\).
            \begin{proof}
                If \(A\) is nonsingular, we can row reduce \(A\) to produce \(I_n\). Since \(\det I_n\neq 0\), by Theorem \ref{thm:detrowops}, \(\det A\neq 0\). If \(\det A\neq 0\), we form the system \([A|B]\) and reduce it to \([C|D]\). We know that \(\det C\neq 0\). Because \(C\) is in reduced row echelon form, and square since \(A\) is square, it is upper triangular. That means all main diagonal entries are nonzero, meaning they are all \(1\). Meaning \(C=I_n\). This means we were able to row reduce \(A\) to \(I_n\), so \(A\) is nonsingular.
            \end{proof}
            
        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following table summarizing various results. Statements in each column are equivalent.
        \begin{center}
            \begin{tabular}{||c|c||}
                \hline
                \hline
                \(A\in\mathcal{M}_{nn}\) is Nonsingular & \(A\in\mathcal{M}_{nn}\) is Singular \\
                \hline
                \hline
                \(\rank A = n\) & \(\rank A < n\) \\
                \(\det A \neq 0\) & \(\det A = 0\) \\
                \(A\) is row equivalent to \(I_n\). & \(A\) is not row equivalent to \(I_n\). \\
                \(AX=0\) has only the trivial solution for \(X\). & \(AX=0\) has a nontrivial solution for \(X\). \\
                \(AX=B\) has a unique solution for \(X\), and \(X=A^{-1}B\). & \(AX=B\) does not have a unique solution. \\
                \hline
            \end{tabular}
        \end{center}

\pagebreak

\section{Lecture 15: September 26, 2022}

    \subsection{Further Properties of Determinants}

        \begin{theorem}{\Stop\,\,Properties of Determinants}{detprop}

            Suppose \(A,B\in\mathcal{M}_{nn}\). Then,
            \begin{enumerate}
                \item \(\det (AB) = \det A\det B\).
                \begin{proof}
                    If \(A\) or \(B\) is singular, \(\det A\det B=0\). For now, let \(B\) be singular. We don't make any assumptions about \(A\), for now. Since \(B\) is singular, there exists some \(X\neq0\) such that \(BX=0\). We consider \((AB)X=A(BX)=A(0)=0\). Thus, \(X\) is a nontrivial solution to the homogeneous equation associated with \(AB\). Thus \(AB\) is singular and \(\det (AB) = 0 = \det A\det B\). Now, suppose that \(A\) is singular and \(B\) is nonsingular. There exists a nontrivial solution for \(Y\) in the system \(AY=0\). Since \(B\) is nonsingular, \(B^{-1}\) exists. We define \(X=B^{-1}Y\) where \(X\neq0\). Then \(ABX=AB(B^{-1}Y)=AY=0\). Thus, \(ABX\) and \(X\neq 0\) implies that \(AB\) is singular, so \(\det (AB)=0\). Now, we consider the case where \(A\) and \(B\) are both nonsingular. There exists row operations \(R_1,\ldots,R_k\) such that \(A=R_1(\ldots(R_k(I_n)\ldots))\) and 
                    \begin{align*}
                        \det (AB)&=\det(R_1(\ldots(R_k(I_n)\ldots))B) \\
                        &=c_1\cdots c_k\det (I_nB) \\
                        &=c_1\cdots c_k\det B \\
                        &=c_1\cdots c_k \det I_n\det B \\
                        &=\det(R_1(\ldots(R_k(I_n)\ldots)))\det B \\
                        &=\det A\det B,
                    \end{align*}
                    hence proving the proposition.
                \end{proof}
                \item \(\det (A^T) = \det A\).
                \begin{proof}
                    Suppose that \(A\) is singular, meaning \(\det A=0\). We seek to show that then, \(\det (A^T)=0\). Suppose, for the sake of contradiction, \(\det (A^T)\neq0\), meaning \(A^T\) is nonsingular. Then, \((A^T)^T=A\) is also nonsingular, which contradicts our assumption. Suppose that \(A\) is nonsingular, meaning \(A\) is row equivalent to \(I_n\). That means
                    \begin{equation*}
                        \det A = \det(R_k(\ldots R_2(R_1(I_n))\ldots))=\det((R_k(\ldots R_2(R_1(I_n))\ldots))^T)=\det (A^T),
                    \end{equation*}
                    proving the proposition.
                \end{proof}
                \item \(\underbrace{\det (A^{-1}) = \frac{1}{\det A}}_{\text{Suppose that \(A\) is nonsingular.}}\).
                \begin{proof}
                    We know that \(A\) is nonsingular. We have \(\det I_n=\det(AA^{-1})=\det A\det (A^{-1})=1\). By simple algebra, \(\det (A^{-1})=\frac{1}{\det A}\).
                \end{proof}
            \end{enumerate}
            
        \end{theorem}

    \pagebreak

    \subsection{Eigenvectors, Eigenvalues, and Diagonalization}

        Consider the following definitions and theorems.

        \begin{definition}{\Stop\,\,Similarity}{similarity}

            Suppose \(A,B\in\mathcal{M}_{nn}\). The matrix \(B\) is similar to a matrix \(A\) if and only if there exists some nonsingular matrix \(P\) such that
            \begin{equation*}
                B=P^{-1}AP.
            \end{equation*}

        \end{definition}
        \begin{definition}{\Stop\,\,Diagonalizability}{diagonalizability}

            The matrix \(A\in\mathcal{M}_{nn}\) is diagonalizable if and only if a diagonal matrix \(D\) is similar to \(A\). That is, \(A\) is diagonalizable if and only if, for nonsingular matrix \(P\),
            \begin{equation*}
                D=P^{-1}AP.
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,Eigenvalues and Eigenvectors}{eigenvaluesandvectors}

            For \(A\in\mathcal{M}_{nn}\), \(\lambda\in\mathbb{R}\) is an eigenvalue of \(A\) if and only if there exists \(X\neq\vec{0}\) such that
            \begin{equation*}
                AX=\lambda X.
            \end{equation*}
            If \(\lambda\) is an eigenvalue of \(A\), \(X\) is an eigenvector of \(A\) with eigenvalue \(\lambda\).

        \end{definition}

        \pagebreak

\section{Lecture 16: September 28, 2022}

    \subsection{The Process of Diagonalization: Part I}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Eigenspace}{eigenspace}

            Given \(A\in\mathcal{M}_{nn}\), the eigenspace of a given eigenvalue \(\lambda\) is
            \begin{equation*}
                E_\lambda=\{X:AX=\lambda X\}\cup\{\vec{0}\}.
            \end{equation*}
            Note that ``\(\cup \{\vec{0}\}\)'' is somewhat redundant, as it will always satisfy the equation. However, the zero vector is never an eigenvector.

        \end{definition}
        \vphantom
        \\
        \\
        Our goal is to find all the eigenvalues and eigenspaces of \(A\). Consider the following theorem.
        \begin{theorem}{\Stop\,\,Finding Eigenvectors and Eigenvalues}{findeigenvs}
    
            Consider a matrix \(A\in\mathcal{M}_{nn}\). By definition, \(X\) is an eigenvector of \(A\) with eigenvalue \(\lambda\in\mathbb{R}\) when
            \begin{equation*}
                AX=\lambda X=\lambda I_nX.
            \end{equation*}
            That is, when
            \begin{equation*}
                (A-\lambda I_n)X=\vec{0}.
            \end{equation*}
            The above equation has a nontrivial solution for \(X\) if and only if \((A-\lambda I_n)\) is singular, that is, when
            \begin{equation*}
                \det(A-\lambda I_n)=0.
            \end{equation*}
            Therefore, the scalar \(\lambda\) is an eigenvalue of \(A\) if and only if \(\lambda\) satisfies the above equation.
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Note that, for now, we will only consider real eigenvalues; however, complex eigenvalues are incredibly useful and have numerous applications. Consider the following example.
        \begin{example}{\Difficulty\,\,Eigenspaces of A Diagonal Matrix}{eigdiag}

            Consider \(A=\begin{bmatrix} 5 & 0 & 0 \\ 0 & 7 & 0 \\ 0 & 0 & 7 \end{bmatrix}\). Find the eigenvalues and eigenspaces of \(A\).
            \\
            \\
            We see that the eigenvalues are \(\lambda_1=5\) and \(\lambda_2=7\). The eigenspaces are
            \begin{equation*}
                E_{\lambda_1}=\left\{c\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}:c\in\mathbb{R}\right\}
            \end{equation*}
            and
            \begin{equation*}
                E_{\lambda_2}=\left\{c_1\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}+c_2\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}:c_1,c_2\in\mathbb{R}\right\}.
            \end{equation*}

        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        We note that for a diagonal matrix, we can simply read off the eigenvalues and eigenspaces. Generally though, we use Theorem \ref{thm:findeigenvs}, as seen in the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Eigenspaces of a \(2\times 2\) Matrix}{eig22}
            
            Consider \(A=\begin{bmatrix} 7 & 1 \\ -3 & 3 \end{bmatrix}\). Find the eigenvalues and eigenspaces of \(A\).
            \\
            \\
            To find the eigenvalues, we consider
            \begin{align*}
                \det(A-\lambda I_2)&=\det \begin{bmatrix} 7-\lambda & 1 \\ -3 & 3-\lambda \end{bmatrix} \\
                &=(7-\lambda)(3-\lambda)+3 \\
                &=21-10\lambda+\lambda^2+3 \\
                &=\lambda^2-10\lambda+24 \\
                &=0.
            \end{align*}
            By factoring, we have \((\lambda-6)(\lambda-4)=0\), meaning \(\lambda_1=4\) and \(\lambda_2=6\). Now, we seek to find the eigenspace. We substitute in \(\lambda_1\) and \(\lambda_2\) into \([(A-\lambda I_2)|\vec{0}]\) and solve for \(\lambda\). For \(\lambda_1\), we have
            \begin{equation*}
                \begin{bmatrix} 7-4 & 1 & | & 0 \\ -3 & 3-4 & | & 0 \end{bmatrix}=\begin{bmatrix} 3 & 1 & | & 0 \\ -3 & -1 & | & 0 \end{bmatrix}.
            \end{equation*}
            By row reduction, we have
            \begin{equation*}
                \begin{bmatrix} 1 & \frac{1}{3} & | & 0 \\ 0 & 0 & | & 0 \end{bmatrix}
            \end{equation*}
            and
            \begin{equation*}
                E_{\lambda_1}=\left\{\begin{bmatrix} -\frac{1}{3}c \\ c \end{bmatrix}: c\in\mathbb{R}\right\}.
            \end{equation*}
            By a similar process, we have
            \begin{equation*}
                E_{\lambda_2}=\left\{\begin{bmatrix} -c \\ c \end{bmatrix}: c\in\mathbb{R}\right\}.
            \end{equation*}

        \end{example}
        \pagebreak
        \vphantom
        \\
        \\
        Now, we revisit Definition \ref{def:diagonalizability}. How do we find \(D\) and \(P\)? We take \(D\) to be the diagonal matrix with all nonzero elements being the eigenvalues of \(A\). Then, we take \(P\) to be the matrix with each column being the eigenvector associated with the eigenvalue in the corresponding column of \(D\). Finally, we check that \(P^{-1}\) exists. Why does this work? In the general \(2\times 2\) case, we have
        \begin{equation*}
            D=\begin{bmatrix}
                \lambda_1 & 0 \\ 0 & \lambda_2
            \end{bmatrix}
        \end{equation*}
        and \(P=\begin{bmatrix} \vec{v}_1 & \vec{v}_2 \end{bmatrix}\) where \(\vec{v}_1\) and \(\vec{v}_2\) are eigenvectors of \(A\), associated with eigenvalues \(\lambda_1\) and \(\lambda_2\), respectively. We have checked that \(P^{-1}\) exists and has the form \(P^{-1}=\begin{bmatrix} \vec{w}_1 \\ \vec{w}_2 \end{bmatrix}\). Then, 
        \begin{equation*}
            P^{-1}P=\begin{bmatrix} \vec{w}_1\cdot\vec{v}_1 & \vec{w}_1\cdot\vec{v}_2 \\ \vec{w}_2\cdot\vec{v}_1 & \vec{w}_2\cdot\vec{v}_2 \end{bmatrix}=I_2.
        \end{equation*}
        Then, we have
        \begin{align*}
            P^{-1}AP&=P^{-1}(AP) \\
            &=P^{-1}\begin{bmatrix} A\vec{v}_1 & A\vec{v}_2 \end{bmatrix} \\
            &=P^{-1}\begin{bmatrix} \lambda_1\vec{v}_1 & \lambda_2\vec{v}_2 \end{bmatrix} \\
            &=\begin{bmatrix} \vec{w}_1 \\ \vec{w}_2 \end{bmatrix}\begin{bmatrix} \lambda_1\vec{v}_1 & \lambda_2\vec{v}_2 \end{bmatrix} \\
            &=\begin{bmatrix} \lambda_1\vec{w}_1\cdot\vec{v}_1 & \lambda_2\vec{w}_1\cdot\vec{v}_2 \\ \lambda_1\vec{w}_2\cdot\vec{v}_1 & \lambda_2\vec{w}_2\cdot\vec{v}_2 \end{bmatrix} \\
            &=\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix} \\
            &=D.
        \end{align*}

\pagebreak

\section{Lecture 17: September 30, 2022}

    \subsection{The Process of Diagonalization: Part II}

        We summarize the process of diagonalization in a more general sense.
        \begin{theorem}{\Stop\,\,The Process of Diagonalization}{diagonalization}
            Let \(A\in\mathcal{M}_{nn}\), consider the following steps.
            \begin{enumerate}
                \item Find the solutions of \(\det(A-\lambda I_n)=0\). The solutions \(\lambda_1,\ldots,\lambda_k\) are the eigenvalues of \(A\).
                \item For each eigenvalue \(\lambda_m\), solve the system \([A-\lambda_mI_n|0]\) by row reduction.
                \item If there are less than \(n\) fundamental eigenvectors, \(A\) cannot be diagonalized.
                \item Form \(P=\begin{bmatrix} \vec{v}_1 & \cdots & \vec{v}_n \end{bmatrix}\). Note that \(P\) is nonsingular.
                \item Verify that \(D=P^{-1}AP\) is a diagonal matrix where each entry \(d_{ii}\) is the eigenvalue for the fundamental eigenvector forming the \(i\)th column of \(P\).
            \end{enumerate}
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Diagonalization 1}{diag1}

            Given
            \begin{equation*}
                A=\begin{bmatrix}
                    9 & 5 \\ 
                    -25 & -21
                \end{bmatrix},
            \end{equation*}
            construct the diagonal matrix \(D\) and form the nonsingular matrix \(P\) such that
            \begin{equation*}
                D=P^{-1}AP.
            \end{equation*}
            To find the eigenvalues, we have
            \begin{align*}
                0&=\det\begin{bmatrix}
                    9-\lambda & 5 \\
                    -25 & -21-\lambda
                \end{bmatrix} \\
                &=(9-\lambda)(-21-\lambda)+125 \\
                &=(\lambda+16)(\lambda-4).
            \end{align*}
            Therefore, our eigenvalues are \(\lambda_1=-16\) and \(\lambda_2=4\). To find the associated eigenvectors, for \(\lambda_1\), we have
            \begin{equation*}
                \begin{bmatrix}
                    25 & 5 & | & 0 \\
                    -25 & -5 & | & 0
                \end{bmatrix},
            \end{equation*}
            which, by \(\langle1\rangle\to\langle2\rangle\to\langle2\rangle\), becomes
            \begin{equation*}
                \begin{bmatrix}
                    25 & 5 & | & 0 \\
                    0 & 0 & | & 0
                \end{bmatrix}.
            \end{equation*}
            We have the eigenspace \(E_{-16}=\left\{c\begin{bmatrix} -1 \\ 5 \end{bmatrix}:c\in\mathbb{R}\right\}\). For \(\lambda_2\), we have
            \begin{equation*}
                \begin{bmatrix}
                    5 & 5 & | & 0 \\
                    -25 & -25 & | & 0
                \end{bmatrix}
            \end{equation*} 
            which can be row reduced to
            \begin{equation*}
                \begin{bmatrix}
                    -25 & -25 & | & 0 \\
                    0 & 0 & | & 0 \\
                \end{bmatrix}.
            \end{equation*} 
            This produces the eigenspace \(E_4=\left\{c\begin{bmatrix} -1 \\ 1 \end{bmatrix}:c\in\mathbb{R}\right\}\). We form 
            \begin{equation*}
                D=\begin{bmatrix} -16 & 0 \\ 0 & 4 \end{bmatrix},\quad P=\begin{bmatrix} -1 & -1 \\ 5 & 1 \end{bmatrix},\quad P^{-1}=\begin{bmatrix} \frac{1}{4} & \frac{1}{4} \\ -\frac{5}{4} & -\frac{1}{4} \end{bmatrix}.
            \end{equation*}
            To check our work, we have
            \begin{align*}
                P^{-1}AP&=\begin{bmatrix} \frac{1}{4} & \frac{1}{4} \\ -\frac{5}{4} & -\frac{1}{4} \end{bmatrix} \begin{bmatrix} 9 & 5 \\ -25 & 21 \end{bmatrix} \begin{bmatrix} -1 & -1 \\ 5 & 1 \end{bmatrix} \\
                &=\begin{bmatrix} -16 & 0 \\ 0 & 4 \end{bmatrix} \\
                &=D,
            \end{align*}
            thus verifying our answer.
        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,Diagonalization 2}{diag2}

            Given
            \begin{equation*}
                A=\begin{bmatrix}
                    0 & -6 & 0 \\
                    3 & 9 & 0 \\
                    0 & 0 & 6
                \end{bmatrix},
            \end{equation*}
            construct the diagonal matrix \(D\) and form the nonsingular matrix \(P\) such that
            \begin{equation*}
                D=P^{-1}AP.
            \end{equation*}
            To find the eigenvalues, we have
            \begin{align*}
                0&=\det\begin{bmatrix}
                    -\lambda & -6 & 0 \\
                    3 & 9-\lambda & 0 \\
                    0 & 0 & 6-\lambda
                \end{bmatrix} \\
                &=-\lambda((9-\lambda)(6-\lambda))+6(3(6-\lambda)) \\
                &=-(\lambda-3)(\lambda-6)^2.
            \end{align*}
            Therefore, our eigenvalues are \(\lambda_1=3\) and \(\lambda_2=6\). For \(\lambda_1\), we have the linear system
            \begin{equation*}
                \begin{bmatrix}
                    -3 & -6 & 0 & | & 0 \\
                    0 & 0 & 0 & | & 0 \\
                    0 & 0 & 3 & | & 0 \\
                \end{bmatrix}
            \end{equation*}
            which can be row reduced to
            \begin{equation*}
                \begin{bmatrix}
                    1 & 2 & 0 & | & 0 \\
                    0 & 0 & 1 & | & 0 \\
                    0 & 0 & 0 & | & 0 
                \end{bmatrix}.
            \end{equation*}
            This gives us \(E_3=\left\{c\begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix}:c\in\mathbb{R}\right\}\). For \(\lambda_2\), we have
            \begin{equation*}
                \begin{bmatrix}
                    -6 & -6 & 0 & | & 0 \\
                    3 & 3 & 0 & | & 0 \\
                    0 & 0 & 0 & | & 0
                \end{bmatrix},
            \end{equation*}
            which can be row reduced to
            \begin{equation*}
                \begin{bmatrix}
                    1 & 1 & 0 & | & 0 \\
                    0 & 0 & 0 & | & 0 \\
                    0 & 0 & 0 & | & 0
                \end{bmatrix},
            \end{equation*}
            giving
            \begin{align*}
                E_6&=\left\{\begin{bmatrix} -c_1 \\ c_1 \\ c_2 \end{bmatrix}:c_1,c_2\in\mathbb{R}\right\} \\ 
                &=\left\{c_1\begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}+c_2\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}:c_1,c_2\in\mathbb{R}\right\}
            \end{align*}   
            We can then form
            \begin{equation*}
                D=\begin{bmatrix} 3 & 0 & 0 \\ 0 & 6 & 0 \\ 0 & 0 & 6 \end{bmatrix},\quad P=\begin{bmatrix} -2 & -1 & 0 \\ 1 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix},\quad P^{-1}=\begin{bmatrix} -1 & -1 & 0 \\ 1 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix}.
            \end{equation*}    
        \end{example}

\pagebreak

\section{Lecture 18: October 3, 2022}

    \subsection{The Process of Diagonalization: Part III}

        We start with a question; what can go wrong in the diagonalization process? We will trace each step of Theorem \ref{thm:diagonalization}. 
        \begin{center}
            \begin{tabular}{||c|c||}
                \hline
                \hline
                What Could go Wrong? & Example \\
                \hline
                \hline
                There exist complex roots to \(\det(A-\lambda I_n)=0\). & \(A=\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\) \\
                \hline
                There are less than \(n\) fundamental eigenvectors. & \(A=\begin{bmatrix} 7 & 1 \\ 0 & 7 \end{bmatrix}\) \\
                \hline
            \end{tabular}
        \end{center}
        \vphantom
        \\
        \\
        We remark that if there exist complex roots, we can resolve it by considering complex eigenvalues in the diagonalization process, as seen in Example \ref{exa:compdiag}; however, we cannot resolve the issue of having less than \(n\) fundamental eigenvectors. Naturally, the question of what constitutes a fundamental eigenvector is brought up. Consider the following definitions and theorems.
        \begin{definition}{\Stop\,\,Linear Independence}{linindep}
            
            Suppose \(\vec{v}_1,\ldots,\vec{v}_k\) are vectors in \(\mathbb{R}^n\). The set \(\{\vec{v}_1,\ldots,\vec{v}_k\}\) is linearly independent if and only if all scalars \(c_1,\ldots,c_k\) that form
            \begin{equation*}
                c_1\vec{v}_1+\cdots+c_k\vec{v}_k=\vec{0}
            \end{equation*}
            are zero.

        \end{definition}
        \begin{theorem}{\Stop\,\,Diagonalizability}{diagonalizability}

            The matrix \(A\in\mathcal{M}_{nn}\) is diagonalizable if and only if there exists a set \(S=\{\vec{v}_1,\ldots,\vec{v}_n\}\) such that
            \begin{enumerate}
                \item Each \(\vec{v}_i\) is an eigenvector of \(A\).
                \item The set \(S\) is linearly independent.
            \end{enumerate}

        \end{theorem}
        \vphantom
        \\
        \\
        The statement ``There are less than \(n\) fundamental eigenvectors'' means that the set \(S=\{\vec{v}_1,\ldots,\vec{v}_n\}\) is not linearly independent, or equivalently, linearly dependent. To conclude if we will have a sufficient number of fundamental eigenvectors, we solve the linear homogeneous system
        \begin{equation*}
            \begin{bmatrix}
                \vec{v}_1 & \cdots & \vec{v}_n | \vec{0}
            \end{bmatrix}
        \end{equation*}
        for \(c_1,\ldots,c_n\). Consider the following theorem.
        \begin{theorem}{\Stop\,\,Diagonalizability and Rank}{diagandrank}
            
            Suppose \(A\in\mathcal{M}_{nn}\) with \(\det(A-\lambda I_n)=(x-\lambda_1)^{n_1}\cdots(x-\lambda_\ell)^{n_\ell}\) for 
            \(\lambda_1,\ldots,\lambda_\ell\in\mathbb{R}\). Then, there exists P=\(\begin{bmatrix} \vec{v}_1 & \cdots & \vec{v}_n \end{bmatrix}\) where \(P\) is nonsingular and each \(\vec{v}_i\) is an eigenvector if and only if, for each \(\lambda_i\),
            \begin{equation*}
                \rank(A-\lambda_iI_n)=n-n_i.
            \end{equation*}
            
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Consider the following example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Is \(A\) Diagonalizable?}{isitdiag}
            
            Suppose \(A\in\mathcal{M}_{nn}\) and \(\det(A-\lambda I_n)=(x-\lambda_1)(x-\lambda_2)\cdots(x-\lambda_n)\), where \(\lambda_1,\ldots,\lambda_n\in\mathbb{R}\) and \(\lambda_1<\lambda_2\cdots<\lambda_n\). Is \(A\) diagonalizable?
            \\
            \\
            The matrix \(A\) is diagonalizable and
            \begin{equation*}
                D=\begin{bmatrix} 
                    \lambda_1 & \cdots & 0 \\
                    \vdots & \ddots & \vdots \\
                    0 & \cdots & \lambda_n
                \end{bmatrix}.
            \end{equation*}
            The matrix is diagonalizable if all roots \(\lambda_1,\ldots,\lambda_n\) are distinct.

        \end{example}
        \begin{example}{\Difficulty\,\Difficulty\,\,A Non-Diagonalizable Matrix}{nondiagmat}

            Given
            \begin{equation*}
                A=\begin{bmatrix}
                    7 & 1 & -1 \\
                    -11 & -3 & 2 \\
                    18 & 2 & -4
                \end{bmatrix},
            \end{equation*}
            construct the diagonal matrix \(D\) and form the nonsingular matrix \(P\) such that
            \begin{equation*}
                D=P^{-1}AP.
            \end{equation*}
            We will skip over the determinant calculation and will, instead, assert that the characteristic polynomial is \((\lambda+2)^2(\lambda-4)\), meaning the eigenvalues are \(\lambda_1=-2\) and \(\lambda_2=4\). The first eigenvalue, \(\lambda_1\), gives the linear system
            \begin{equation*}
                \begin{bmatrix}
                    9 & 1 & -1 & | & 0 \\
                    -11 & -1 & 2 & | & 0 \\
                    18 & 2 & -2 & | & 0
                \end{bmatrix}
            \end{equation*}
            which reduces to
            \begin{equation*}
                \begin{bmatrix}
                    1 & 0 & -\frac{1}{2} & | & 0 \\
                    0 & 1 & \frac{7}{2} & | & 0 \\
                    0 & 0 & 0 & | & 0
                \end{bmatrix},
            \end{equation*}
            giving us the eigenspace \(E_{-2}=\left\{c\begin{bmatrix} 1 \\ -7 \\ 2 \end{bmatrix}:c\in\mathbb{R}\right\}\). Notice that for \(\lambda_1\),
            \begin{equation*}
                \rank(A-\lambda_1I_3)=2\neq3-2=1.
            \end{equation*}
            Therefore, \(A\) cannot be diagonalized; we need not find the eigenspace of \(\lambda_2\), but for the sake of a curious student, we have \(E_4=\left\{c\begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix}:c\in\mathbb{R}\right\}\).

        \end{example}
    
    \subsection{Using Diagonalization to Raise Matrices to Powers}

        One useful application of the diagonalization process is raising matrices to powers. Consider the following theorem.
        \begin{theorem}{\Stop\,\,Raising Matrices to Powers}{raisematpow}

            Given a diagonalizable matrix \(A\),
            \begin{equation*}
                A^k=PD^kP^{-1}.
            \end{equation*}
            \begin{proof}
                We proceed by induction. For \(k=2\),
                \begin{equation*}
                    A^2=AA=(PDP^{-1})(PDP^{-1})=PD(P^{-1}P)DP^{-1}=PDI_nDP^{-1}=PD^2P^{-1}.
                \end{equation*}
                Suppose that for all \(m\in\mathbb{N}\), \(A^m=PD^mP^{-1}\). Then,
                \begin{equation*}
                    A^{m+1}=A^mA=PD^mP^{-1}(PDP^{-1})=PD^m(P^{-1}P)DP^{-1}=PD^mI_nDP^{-1}=PD^{m+1}P^{-1},
                \end{equation*}
                proving the theorem.
            \end{proof}
            
        \end{theorem} 

\pagebreak

\section{Lecture 19: October 5, 2022}

    \subsection{Complex Numbers}

        Consider the following definition.
        \begin{definition}{\Stop\,\,The Complex Numbers}{compnum}

            The set of complex numbers, \(\mathbb{C}\), is given by
            \begin{equation*}
                \mathbb{C}=\{a+bi:a,b\in\mathbb{R},i^2=-1\}.
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,The Set \(\mathbb{C}^n\)}{compvcspace}

            We define \(\mathbb{C}^n\) as
            \begin{equation*}
                \mathbb{C}^n=\{[z_1,\ldots,z_n]:z_i=\mathbb{C}\}.
            \end{equation*}
            
        \end{definition}
        \vphantom
        \\
        \\
        As a set, \(\mathbb{C}\) can be visualized as \(\mathbb{R}^2\). Consider the following definitions and properties.
        \begin{definition}{\Stop\,\,Definitions of Operations With Complex Numbers}{depcompops}
            
            Let \(a,b,c,d\in\mathbb{R}\). Then, the following operations are defined as follows.
            \begin{enumerate}
                \item \((a+bi)+(c+di):=(a+c)+(b+d)i\).
                \item \(\Re (a+bi):=a,\quad \Im (a+bi):=b\).
                \item \(z=a+bi\in\mathbb{C}\implies||z||:=\sqrt{a^2+b^2}\).
                \item \((a+bi)(c+di):=ac+adi+bci+bdi^2=(ac-bd)+(ad+bc)i\).
                \item \(z=a+bi\in\mathbb{C}\implies\bar{z}:=a-bi\).
            \end{enumerate}

        \end{definition}
        \begin{theorem}{\Stop\,\,Properties of Complex Numbers}{propcomp}
            
            Let \(a,b,c,d\in\mathbb{R}\). Then,
            \begin{enumerate}
                \item \(z\bar{z}=(a+bi)(a-bi)=a^2+b^2=||z||^2\).
                \item \(z\in\mathbb{C}- \{0\}\implies\frac{1}{z}=\frac{a}{a^2+b^2}-\left(\frac{b}{a^2+b^2}\right)\in\mathbb{C}\).
            \end{enumerate}

        \end{theorem}
        \begin{theorem}{\Stop\,\,The Fundamental Theorem of Algebra}{fundthmalg}
            
            If \(p(z)=a_nz^n+\cdots+a_0z^0\) is an \(n\)th degree polynomial with coefficients \(a_0,\ldots,a_n\in\mathbb{C}\), \(p(z)\) has \(n\) complex roots. Note that some roots may be repeated.

        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        We now consider vectors in \(\mathbb{C}^n\) and define their operations.
        \begin{definition}{\Stop\,\,Vector Addition in \(\mathbb{C}^n\)}{compvcadd}

            Let \(\vec{v}=[v_1,\ldots,v_n]\in\mathbb{C}^n\) and \(\vec{w}=[w_1,\ldots,w_n]\in\mathbb{C}^n\). Then,
            \begin{equation*}
                \vec{v}+\vec{w}=[v_1+w_1,\ldots,v_n+w_n].
            \end{equation*}

        \end{definition}
        \begin{definition}{\Stop\,\,Scalar Multiplication in \(\mathbb{C}^n\)}{compscalmult}

            Let \(\vec{v}=[v_1,\ldots,v_n]\in\mathbb{C}^n\) and \(c\in\mathbb{C}\). Then,
            \begin{equation*}
                c\vec{v}=[cv_1,\ldots,cv_n].
            \end{equation*}

        \end{definition}
        \begin{definition}{\Stop\,\,The Dot Product in \(\mathbb{C}^n\)}{compdotprod}

            Let \(\vec{v}=[v_1,\ldots,v_n]\in\mathbb{C}^n\) and \(\vec{w}=[w_1,\ldots,w_n]\in\mathbb{C}^n\). Then,
            \begin{equation*}
                \vec{v}\cdot\vec{w}=v_1\bar{w_1}+\cdots+v_n\bar{w_n}.
            \end{equation*}

        \end{definition}
        \begin{definition}{\Stop\,\,The Magnitude in \(\mathbb{C}^n\)}{compmagn}

            Let \(\vec{v}=[v_1,\ldots,v_n]\in\mathbb{C}^n\). Then,
            \begin{equation*}
                ||\vec{v}||=\sqrt{\vec{v}\cdot\vec{v}}.
            \end{equation*}

        \end{definition}
        \vphantom
        \\
        \\
        Definition \ref{def:compdotprod} provides a good question: do we consider complex conjugates when performing matrix multiplication with matrices of the sort \(\mathcal{M}_{mn}^\mathbb{C}\)? We do not.
        \begin{definition}{\Stop\,\,The Adjoint}{adjoint}

            Given \(A\in\mathcal{M}_{mn}^\mathbb{C}\), 
            \begin{equation*}
                A=\begin{bmatrix}
                    z_{11} & \cdots & z_{1n} \\
                    \vdots & \ddots & \vdots \\
                    z_{m1} & \cdots & z_{mn}
                \end{bmatrix}
                \implies 
                A^*=\begin{bmatrix}
                    \bar{z_{11}} & \cdots & \bar{z_{m1}} \\
                    \vdots & \ddots & \vdots \\
                    \bar{z_{1n}} & \cdots & \bar{z_{mn}}
                \end{bmatrix}
            \end{equation*}

        \end{definition}
        \vphantom
        \\
        \\
        We will now provide a theorem motivating the transpose operation and apply it to our definition of the adjoint.
        \begin{theorem}{\Stop\,\,The Transpose and The Dot Product}{transposedot}
            If \(\vec{v},\vec{w}\in\mathbb{R}^n\), and \(A\in\mathcal{M}_{nn}^\mathbb{R}\), 
            \begin{equation*}
                (A\vec{v})\cdot\vec{w}=\vec{v}\cdot(A^T\vec{w})
            \end{equation*}
        \end{theorem}
        \begin{theorem}{\Stop\,\,The Adjoint and The Dot Product}{transposedot}
            If \(\vec{v},\vec{w}\in\mathbb{C}^n\), and \(A\in\mathcal{M}_{nn}^\mathbb{C}\), 
            \begin{equation*}
                (A\vec{v})\cdot\vec{w}=\vec{v}\cdot(A^*\vec{w})
            \end{equation*}
        \end{theorem}
        \vphantom
        \\
        \\
        The analog of symmetric and skew-symmetric matrices is the notion of hermitian and skew-hermitian matrices. Consider the following definition. It may also be useful to recall Definition \ref{def:symmetricmatrices}.
        \begin{definition}{\Stop\,\,Hermitian and Skew-Hermitian Matrices}{hermitskewhermit}

            Suppose \(A\in\mathcal{M}_{nn}^\mathbb{C}\). Then,
            \begin{enumerate}
                \item \(A\) is hermitian if and only if \(A=A^*\).
                \item \(A\) is skew-hermitian if and only if \(A=-A^*\).
                \item \(A\) is normal if and only if \(AA^*=A^*A\).
            \end{enumerate}
            
        \end{definition}

\pagebreak

\section{Lecture 20: October 7, 2022}

    \subsection{Diagonalization Using Complex Eigenvalues}

        We now consider the diagonalization process for matrices with complex eigenvalues. Consider the following motivating example.
        \begin{example}{\Difficulty\,\Difficulty\,\,Diagonalization With Complex Eigenvalues}{compdiag}

            Diagonalize the matrix
            \begin{equation*}
                A=\begin{bmatrix}
                    0 & 1 \\
                     -1 & 0
                \end{bmatrix}.
            \end{equation*}
            The roots of the characteristic polynomial, \(\det(A-\lambda I_2)\) are \(\lambda_1=i\) and \(\lambda_2=-i\). For \(\lambda_1\), we have
            \begin{equation*}
                \begin{bmatrix}
                    i & -1 & | & 0 \\
                    1 & i & | & 0
                \end{bmatrix}.
            \end{equation*}
            We perform \(\langle1\rangle\leftrightarrow\langle2\rangle\) to obtain
            \begin{equation*}
                \begin{bmatrix}
                    1 & i & | & 0 \\
                    i & -1 & | & 0
                \end{bmatrix}.
            \end{equation*}
            Then, we have \(-i\langle1\rangle+\langle2\rangle\to\langle2\rangle\), producing
            \begin{equation*}
                \begin{bmatrix}
                    1 & i & | & 0 \\
                    0 & 0 & | & 0
                \end{bmatrix}.
            \end{equation*}
            Thus, \(E_i=\left\{c\begin{bmatrix} -i \\ 1 \end{bmatrix}:c\in\mathbb{C}\right\}\). By a similar process \(E_{-i}=\left\{c\begin{bmatrix} i \\ 1\end{bmatrix}:c\in\mathbb{C}\right\}\). Now, we form
            \begin{equation*}
                D=\begin{bmatrix}
                    i & 0 \\
                    0 & -i
                \end{bmatrix},\quad
                P=\begin{bmatrix}
                    -i & i \\
                    1 & 1
                \end{bmatrix}.
            \end{equation*}
            
        \end{example}
        \vphantom
        \\
        \\
        Note that we still may not have ``enough'' eigenvectors. This is when the set of fundamental eigenvectors is linearly dependent. Recall Theorem \ref{thm:diagandrank}.