\section{Lecture 13: September 21, 2022}

    \subsection{Defining the Determinant}

        Consider the following theorems and definitions.
        \begin{theorem}{\Stop\,\,The Determinant Determines the Area in \(\mathbb{R}^2\)}{areadet}

            Consider \(\vec{x}=[x_1,x_2]\) and \(\vec{y}=[y_1,y_2]\). If we form
            \begin{equation*}
                A=\begin{bmatrix}
                    x_1 & x_2 \\
                    y_1 & y_2 
                \end{bmatrix},
            \end{equation*}
            \begin{equation*}
                |\det A| = ||\vec{x}\times\vec{y}||.
            \end{equation*}
            That is, \(|\det A|\) provides the area of the parallelogram determined by \(\vec{x}\) and \(\vec{y}\).
            
        \end{theorem}
        \begin{theorem}{\Stop\,\,The Determinant Determines the Volume in \(\mathbb{R}^3\)}{voldet}

            Consider \(\vec{x}=[x_1,x_2,x_3]\), \(\vec{y}=[y_1,y_2,y_3]\), and \(\vec{z}=[z_1,z_2,z_3]\).
            If we form
            \begin{equation*}
                A=\begin{bmatrix}
                    x_1 & x_2 & x_3 \\
                    y_1 & y_2 & y_3 \\
                    z_1 & z_2 & z_3
                \end{bmatrix},
            \end{equation*}
            \begin{equation*}
                |\det A| = \vec{x}\cdot(\vec{y}\times\vec{z}).
            \end{equation*}
            That is, \(|\det A|\) provides the volume of the parallelepiped determined by \(\vec{x}\), \(\vec{y}\), and \(\vec{z}\).
            
        \end{theorem}
        \begin{definition}{\Stop\,\,The \((i,j)\) Submatrix}{submatrix}

            Suppose \(A\in\mathcal{M}_{nn}\). The \((i,j)\) submatrix of \(A\) is the \((n-1)\times(n-1)\) matrix obtained by removing the \(i\)th row and the \(j\)th column. We denote this by \(A_{(i,j)}\) 
            
        \end{definition}
        \begin{definition}{\Stop\,\,The \((i,j)\) Minor}{minor}

            Suppose \(A\in\mathcal{M}_{nn}\). The \((i,j)\) minor of \(A\) is the determinant of the \((i,j)\) submatrix of \(A\).
            
        \end{definition}
        \begin{definition}{\Stop\,\,The \((i,j)\) Cofactor}{cofactor}

            Suppose \(A\in\mathcal{M}_{nn}\). The \((i,j)\) cofactor of \(A\) is
            \begin{equation*}
                A_{ij}=(-1)^{i+j}\det (A_{(i,j)}).
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,The Determinant}{det}
            Suppose \(A\in\mathcal{M}_{nn}\). Then,
            \begin{enumerate}
                \item If \(n=1\), and \(A=\begin{bmatrix} a_{11} \end{bmatrix}\), \(\det A = a_{11}\).
                \item If \(n=2\), and \(A=\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}\), \(\det A = a_{11}a_{22}-a_{12}a_{21}\).
                \item If \(n>2\), and \(A=\begin{bmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn} \end{bmatrix}\), \(\det A = (a_{11}A_{11}+\cdots+a_{1n}A_{1n})+\cdots+(a_{n1}A_{n1}+\cdots+a_{nn}A_{nn})\).
            \end{enumerate}
        \end{definition}
        \pagebreak
        \vphantom
        \\
        \\
        For fun, consider the following Python 3 implementation of computing the determinant of any \(n\times n\) matrix.
        \lstinputlisting[language=Python]{Graphics/matrixdet.py}
        
        \pagebreak

\section{Lecture 14: September 23, 2022}

    \subsection{Determinants of Upper Triangular Matrices}

        Consider the following theorems.
        \begin{theorem}{\Stop\,\,The Determinant of an Upper Triangular Matrix}{uppertriangulardet}

            If \(A\in\mathcal{M}_{nn}\) is upper triangular, 
            \begin{equation*}
                \det A = a_{11} a_{22} \cdots a_{nn}.
            \end{equation*}
            Recall that \(A\) is upper triangular if and only if all elements below the main diagonal are zero.
            \begin{proof}
                We proceed by induction on \(n\). For \(n=1\), \(A=\begin{bmatrix} a_{11} \end{bmatrix}\), so \(\det A = a_{11}\). Suppose for all \(k\in\mathbb{N}\), and some upper triangular \(A\in\mathcal{M}_{kk}\),
                \begin{equation*}
                    a_{11} a_{22} \cdots a_{kk}.
                \end{equation*}
                Consider 
                \begin{equation*}
                    B=\begin{bmatrix}
                        b_{11} & \cdots & b_{1(k+1)} \\
                        \vdots & \ddots & \vdots \\
                        0 & \cdots & b_{(k+1)(k+1)}
                    \end{bmatrix}.
                \end{equation*}
                Then, we compute \(\det B\) using the last row as our ``first row.''
                \begin{align*}
                    \det B &= (-1)^{1+1}b_{11}\det\begin{bmatrix} b_{22} & b_{23} & \cdots & a_{2(k+1)} \\ 0 & b_{33} & \cdots & b_{3(k+1)} \\ \vdots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & b_{(k+1)(k+1)} \end{bmatrix} \\
                    &= b_{11}\underbrace{b_{22}\cdots b_{(k+1)(k+1)}}_{\text{By the inductive hypothesis.}}.
                    %\begin{comment}\det B&= 0+\cdots+0+a_{(k+1)(k+1)}(-1)^{(k+1)+(k+1)}\det B_{(k+1),(k+!)} \\ &=a_{(k+1)(k+1)}(-1)^{2(k+1)}\det A.\end{comment}
                \end{align*}
                This is precisely the stipulation of the theorem when \(n=k+1\).
            \end{proof}

        \end{theorem}
        \pagebreak
        \begin{theorem}{\Stop\,\,Determinants and Row Operations}{detrowops}

            Suppose \(A\in\mathcal{M}_{nn}\) and let \(R\) be a row operation. Then,
            \begin{enumerate}
                \item If \(R\) is \(c\langle i\rangle\to\langle i \rangle\) for some \(c\in\mathbb{R}\), 
                \begin{equation*}
                    \det R(A) = c\det A.
                \end{equation*}
                Note that if \(c=0\), \(R\) is not a valid row operation.
                \item If \(R\) is \(c\langle i\rangle+\langle j \rangle\to\langle j \rangle\) for some \(c\in\mathbb{R}\),
                \begin{equation*}
                    \det R(A) = \det A.
                \end{equation*}
                Note that if \(c=0\), \(R\) is not a valid row operation.
                \item If \(R\) is \(\langle i \rangle\leftrightarrow\langle j \rangle\),
                \begin{equation*}
                    \det R(A) = -\det A.
                \end{equation*}
            \end{enumerate}
            \vphantom
            \\
            \\
            Note that if \(\det A\neq 0\) and \(R\) is a row operation, \(\det R(A)\neq 0\).
        \end{theorem}
        \vphantom
        \\
        \\
        We can use Theorem \ref{thm:detrowops} in conjunction with row operations to compute the determinant of a matrix easily. We simply use row operations to create an upper triangular matrix, while keeping track of how the determinant changes. We then apply Theorem \ref{thm:uppertriangulardet}. Consider the following examples.
        \begin{example}{\Difficulty\,\Difficulty\,\,Computing a Determinant by Row Reduction}{compdetrowred}
            
            Compute \(\det\begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & -2 \\ 4 & 9 & 4 \end{bmatrix}\).
            \\
            \\
            Let 
            \begin{equation*}
                A_0=\begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & -2 \\ 4 & 9 & 4 \end{bmatrix}.
            \end{equation*}
            Consider the following table.
            \begin{center}
                \begin{tabular}{||c|c|c||}
                    \hline
                    Row Operation & Resultant Matrix & Effect on the Determinant \\
                    \hline
                    \hline
                    \(-2\langle1\rangle+\langle2\rangle\to\langle2\rangle\) & \(A_1=\begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & -4 \\ 4 & 9 & 4 \end{bmatrix}\) & \(\det A_1=\det A_0\) \\
                    \hline
                    \(-4\langle1\rangle+\langle3\rangle\to\langle3\rangle\) & \(A_2=\begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & -4 \\ 0 & 5 & 0 \end{bmatrix}\) & \(\det A_2=\det A_1\) \\
                    \hline
                    \(-5\langle2\rangle+\langle3\rangle\to\langle3\rangle\) & \(A_3=\begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & -4 \\ 0 & 0 & 20 \end{bmatrix}\) & \(\det A_3=\det A_2\) \\
                    \hline
                \end{tabular}
            \end{center}
            \vphantom
            \\
            \\
            Thus, \(\det A_0=20\).

        \end{example}
        \pagebreak
        \begin{example}{\Difficulty\,\Difficulty\,\,Computing a Determinant by Row Reduction}{compdetrowred}
            
            Compute \(\det\begin{bmatrix} 0 & -14 & -8 \\ 1 & 3 & 2 \\ -2 & 0 & 6 \end{bmatrix}\).
            \\
            \\
            Let 
            \begin{equation*}
                A_0=\begin{bmatrix} 0 & -14 & -8 \\ 1 & 3 & 2 \\ -2 & 0 & 6 \end{bmatrix}.
            \end{equation*}
            Consider the following table.
            \begin{center}
                \begin{tabular}{||c|c|c||}
                    \hline
                    Row Operation & Resultant Matrix & Effect on the Determinant \\
                    \hline
                    \hline
                    \(\langle2\rangle\leftrightarrow\langle1\rangle\) & \(A_1=\begin{bmatrix} 1 & 3 & 2 \\ 0 & -14 & -8 \\ -2 & 0 & 6 \end{bmatrix}\) & \(\det A_1=-\det A_0\) \\
                    \hline
                    \(2\langle1\rangle+\langle3\rangle\to\langle3\rangle\) & \(A_2=\begin{bmatrix} 1 & 3 & 2 \\ 0 & -14 & -8 \\ 0 & 6 & 10 \end{bmatrix}\) & \(\det A_2=\det A_1\) \\
                    \hline
                    \(-\frac{1}{14}\langle2\rangle\to\langle2\rangle\) & \(A_3=\begin{bmatrix} 1 & 3 & 2 \\ 0 & 1 & \frac{4}{7} \\ 0 & 6 & 10 \end{bmatrix}\) & \(\det A_3=-\frac{1}{14}\det A_2\) \\
                    \hline
                    \(-6\langle2\rangle+\langle3\rangle\to\langle3\rangle\) & \(A_3=\begin{bmatrix} 1 & 3 & 2 \\ 0 & 1 & \frac{4}{7} \\ 0 & 0 & \frac{46}{7} \end{bmatrix}\) & \(\det A_4=\det A_3\) \\
                    \hline
                \end{tabular}
            \end{center}
            \vphantom
            \\
            \\
            Thus, \(\det A_0=\frac{46}{7}(-14)(-1)=92\).

        \end{example}
        \vphantom
        \\
        \\
        Consider the following theorem.
        \begin{theorem}{\Stop\,\,Inverses and Determinants}{invdet}
            
            Suppose that \(A\in\mathcal{M}_{nn}\). Then, \(A\) is nonsingular if and only if \(\det A \neq 0\).
            \begin{proof}
                If \(A\) is nonsingular, we can row reduce \(A\) to produce \(I_n\). Since \(\det I_n\neq 0\), by Theorem \ref{thm:detrowops}, \(\det A\neq 0\). If \(\det A\neq 0\), we form the system \([A|B]\) and reduce it to \([C|D]\). We know that \(\det C\neq 0\). Because \(C\) is in reduced row echelon form, and square since \(A\) is square, it is upper triangular. That means all main diagonal entries are nonzero, meaning they are all \(1\). Meaning \(C=I_n\). This means we were able to row reduce \(A\) to \(I_n\), so \(A\) is nonsingular.
            \end{proof}
            
        \end{theorem}
        \vphantom
        \\
        \\
        Consider the following table summarizing various results about singularity, determinants, rank, existence and uniqueness of linear systems, and row equivalence. Statements in each column are equivalent.
        \begin{center}
            \begin{tabular}{c|c}
                \hline
                \(A\in\mathcal{M}_{nn}\) is Nonsingular & \(A\in\mathcal{M}_{nn}\) is Singular \\
                \hline
                \(\rank A = n\) & \(\rank A < n\) \\
                \(\det A \neq 0\) & \(\det A = 0\) \\
                \(A\) is row equivalent to \(I_n\). & \(A\) is not row equivalent to \(I_n\). \\
                \(AX=0\) has only the trivial solution for \(X\). & \(AX=0\) has a nontrivial solution for \(X\). \\
                \(AX=B\) has a unique solution for \(X\), and \(X=A^{-1}B\). & \(AX=B\) does not have a unique solution. \\
                \hline
            \end{tabular}
        \end{center}

\pagebreak

\section{Lecture 15: September 26, 2022}

    \subsection{Further Properties of Determinants}

        \begin{theorem}{\Stop\,\,Properties of Determinants}{detprop}

            Suppose \(A,B\in\mathcal{M}_{nn}\). Then,
            \begin{enumerate}
                \item \(\det (AB) = \det A\det B\).
                \begin{proof}
                    If \(A\) or \(B\) is singular, \(\det A\det B=0\). For now, let \(B\) be singular. We don't make any assumptions about \(A\), for now. Sine \(B\) is singular, there exists some \(X\neq0\) such that \(BX=0\). We consider \((AB)X=A(BX)=A(0)=0\). Thus, \(X\) is a nontrivial solution to the homogeneous equation associated with \(AB\). Thus \(AB\) is singular and \(\det (AB) = 0 = \det A\det B\). Now, suppose that \(A\) is singular and \(B\) is nonsingular. There exists a nontrivial solution for \(Y\) in the system \(AY=0\). Since \(B\) is nonsingular, \(B^{-1}\) exists. We define \(X=B^{-1}Y\) where \(X\neq0\). Then \(ABX=AB(B^{-1}Y)=AY=0\). Thus, \(ABX\) and \(X\neq 0\) implies that \(AB\) is singular, so \(\det (AB)=0\). Now, we consider the case where \(A\) and \(B\) are both nonsingular. There exists row operations \(R_1,\ldots,R_k\) such that \(A=R_1(\ldots(R_k(I_n)\ldots))\) and 
                    \begin{align*}
                        \det (AB)&=\det(R_1(\ldots(R_k(I_n)\ldots))B) \\
                        &=c_1\cdots c_k\det (I_nB) \\
                        &=c_1\cdots c_k\det B \\
                        &=c_1\cdots c_k \det I_n\det B \\
                        &=\det(R_1(\ldots(R_k(I_n)\ldots)))\det B \\
                        &=\det A\det B,
                    \end{align*}
                    hence proving the proposition.
                \end{proof}
                \item \(\det (A^T) = \det A\).
                \item \(\underbrace{\det (A^{-1}) = \frac{1}{\det A}}_{\text{Suppose that \(A\) is nonsingular.}}\).
                \begin{proof}
                    We know that \(A\) is nonsingular. We have \(\det I_n=\det(AA^{-1})=\det A\det (A^{-1})=1\). By simple algebra, \(\det (A^{-1})=\frac{1}{\det A}\).
                \end{proof}
            \end{enumerate}
            
        \end{theorem}

    \pagebreak

    \subsection{Eigenvectors, Eigenvalues, and Diagonalization}

        Consider the following definitions and theorems.

        \begin{definition}{\Stop\,\,Similarity}{sim}

            Suppose \(A,D\in\mathcal{M}_{nn}\). Then, \(A\) and \(D\) are similar if and only if there exists nonsingular \(P\) such that
            \begin{equation*}
                A=PDP^{-1}.
            \end{equation*}
            
        \end{definition}
        \begin{definition}{\Stop\,\,Diagonalizability}{diagonalizability}

            The matrix \(A\in\mathcal{M}_{nn}\) is diagonalizable if \(A\) is similar to a diagonal matrix.
            
        \end{definition}
        \begin{definition}{\Stop\,\,Eigenvalues and Eigenvectors}{eigenvaluesandvectors}

            For \(A\in\mathcal{M}_{nn}\), \(\lambda\in\mathbb{R}\) is an eigenvalue of \(A\) if and only if there exists \(X\neq0\) such that
            \begin{equation*}
                AX=\lambda X.
            \end{equation*}
            If \(\lambda\) is an eigenvalue of \(A\), \(X\) is an eigenvector of \(A\) with eigenvalue \(\lambda\).

        \end{definition}

        \pagebreak

\section{Lecture 16: September 28, 2022}

    \subsection{The Process of Diagonalization}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Eigenspace}{eigenspace}

            Given \(A\in\mathcal{M}_{nn}\), the eigenspace of a given eigenvalue \(\lambda\) is
            \begin{equation*}
                E_\lambda=\{X:AX=\lambda X\}\cup\{\vec{0}\}
            \end{equation*}
            Note that ``\(\cup \{\vec{0}\}\)'' is somewhat redundant, as it will always satisfy the equation. However, the zero vector is never an eigenvector.

        \end{definition}
        \vphantom
        \\
        \\
        Our goal is to find all the eigenvalues and eigenspaces of \(A\). Consider the following theorem.
        \begin{theorem}{\Stop\,\,Finding Eigenvectors and Eigenvalues}{findeigenvs}
    
            Consider a matrix \(A\in\mathcal{M}_{nn}\). By definition, \(X\) is an eigenvector of \(A\) with eigenvalue \(\lambda\in\mathbb{R}\) when
            \begin{equation*}
                AX=\lambda X=\lambda I_nX.
            \end{equation*}
            That is, when
            \begin{equation*}
                (A-\lambda I_n)X=\vec{0}.
            \end{equation*}
            The above equation has a nontrivial solution for \(X\) if and only if \((A-\lambda I_n)\) is singular, that is, when
            \begin{equation*}
                \det(A-\lambda I_n)=\vec{0}.
            \end{equation*}
            Therefore, the scalar \(\lambda\) is an eigenvalue of \(A\) if and only if \(\lambda\) satisfies the above equation.
        \end{theorem}
        \pagebreak
        \vphantom
        \\
        \\
        Note that, for now, we will only consider real eigenvalues; however, complex eigenvalues are incredibly useful and have numerous applications. Consider the following example.
        \begin{example}{\Difficulty\,\,Eigenspaces of A Diagonal Matrix}{eigdiag}

            Consider \(A=\begin{bmatrix} 5 & 0 & 0 \\ 0 & 7 & 0 \\ 0 & 0 & 7 \end{bmatrix}\). Find the eigenvalues and eigenspaces of \(A\).
            \\
            \\
            We see that the eigenvalues are \(\lambda_1=5\) and \(\lambda_2=7\). The eigenspaces are
            \begin{equation*}
                E_{\lambda_1}=\left\{c\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}:c\in\mathbb{R}\right\}
            \end{equation*}
            and
            \begin{equation*}
                E_{\lambda_2}=\left\{c_1\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}+c_2\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}:c_1,c_2\in\mathbb{R}\right\}.
            \end{equation*}

        \end{example}
        \vphantom
        \\
        \\
        We note that for a diagonal matrix, we can simply read off the eigenvalues and eigenspaces. Generally though, we use Theorem \ref{thm:findeigenvs}. Now, we revisit Definition \ref{def:sim}. How do we find \(D\) and \(P\)? We take \(D\) to be the diagonal matrix with all nonzero elements being the eigenvalues of \(A\). Then, we take \(P\) to be the matrix with each column being the eigenvector associated with the eigenvalue in the corresponding column of \(D\). Finally, we check that \(P^{-1}\) exists. Why does this work? In the general \(2\times 2\) case, we have
        \begin{equation*}
            D=\begin{bmatrix}
                \lambda_1 & 0 \\ 0 & \lambda_2
            \end{bmatrix}
        \end{equation*}
        and \(P=\begin{bmatrix} \vec{v}_1 & \vec{v}_2 \end{bmatrix}\) where \(\vec{v}_1\) and \(\vec{v}_2\) are eigenvectors of \(A\), associated with eigenvalues \(\lambda_1\) and \(\lambda_2\), respectively. We have checked that \(P^{-1}\) exists and has the form \(P^{-1}=\begin{bmatrix} \vec{w}_1 \\ \vec{w}_2 \end{bmatrix}\). Then, 
        \begin{equation*}
            P^{-1}P=\begin{bmatrix} \vec{w}_1\cdot\vec{v}_1 & \vec{w}_1\cdot\vec{v}_2 \\ \vec{w}_2\cdot\vec{v}_1 & \vec{w}_2\cdot\vec{v}_2 \end{bmatrix}=I_2.
        \end{equation*}
        Then, we have
        \begin{align*}
            P^{-1}AP&=P^{-1}(AP) \\
            &=P^{-1}\begin{bmatrix} A\vec{v}_1 & A\vec{v}_2 \end{bmatrix} \\
            &=P^{-1}\begin{bmatrix} \lambda_1\vec{v}_1 & \lambda_2\vec{v}_2 \end{bmatrix} \\
            &=\begin{bmatrix} \vec{w}_1 \\ \vec{w}_2 \end{bmatrix}\begin{bmatrix} \lambda_1\vec{v}_1 & \lambda_2\vec{v}_2 \end{bmatrix} \\
            &=\begin{bmatrix} \lambda_1\vec{w}_1\cdot\vec{v}_1 & \lambda_2\vec{w}_1\cdot\vec{v}_2 \\ \lambda_1\vec{w}_2\cdot\vec{v}_1 & \lambda_2\vec{w}_2\cdot\vec{v}_2 \end{bmatrix} \\
            &=\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix} \\
            &=D.
        \end{align*}
        \vphantom
        \\
        \\
        Consider the following example of finding the eigenspaces of a \(2\times 2\) matrix.
        \begin{example}{\Difficulty\,\Difficulty\,\,Eigenspaces of a \(2\times 2\) Matrix}{eig22}
            
            Consider \(A=\begin{bmatrix} 7 & 1 \\ -3 & 3 \end{bmatrix}\). Find the eigenvalues and eigenspaces of \(A\).
            \\
            \\
            To find the eigenvalues, we consider
            \begin{align*}
                \det(A-\lambda I_2)&=\det \begin{bmatrix} 7-\lambda & 1 \\ -3 & 3-\lambda \end{bmatrix} \\
                &=(7-\lambda)(3-\lambda)+3 \\
                &=21-10\lambda+\lambda^2+3 \\
                &=\lambda^2-10\lambda+24 \\
                &=0.
            \end{align*}
            By factoring, we have \((\lambda-6)(\lambda-4)=0\), meaning \(\lambda_1=4\) and \(\lambda_2=6\). Now, we seek to find the eigenspace. We substitute in \(\lambda_1\) and \(\lambda 2\) into \((A-\lambda I_2)=0\) and solve for \(\lambda\). For \(\lambda_1\), we have
            \begin{align*}
                \begin{bmatrix} 0 \\ 0 \end{bmatrix}&=\begin{bmatrix} 7-4 & 1 \\ -3 & 3-4 \end{bmatrix} \\
                &=\begin{bmatrix} 3 & 1 \\ -3 & -1 \end{bmatrix}.
            \end{align*}
            By row reduction, we have
            \begin{equation*}
                \begin{bmatrix} 0 \\ 0 \end{bmatrix}=\begin{bmatrix} 1 & \frac{1}{3} \\ 0 & 0 \end{bmatrix}
            \end{equation*}
            and
            \begin{equation*}
                E_{\lambda_1}=\left\{\begin{bmatrix} -\frac{1}{3}c \\ c \end{bmatrix}: c\in\mathbb{R}\right\}
            \end{equation*}
            By a similar process, we have
            \begin{equation*}
                E_{\lambda_2}=\left\{\begin{bmatrix} -c \\ c \end{bmatrix}: c\in\mathbb{R}\right\}
            \end{equation*}

        \end{example}
        